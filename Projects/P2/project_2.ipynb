{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb51a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt \n",
    "import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        #render the env\n",
    "        env.render()\n",
    "    print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e584f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self,num_states,num_actions,nodes_1 =50, nodes_2 = 50,seed =10):\n",
    "        super(Deep_Q_Network,self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(num_states,nodes_1)\n",
    "        self.fc2 = nn.Linear(nodes_1,nodes_2)\n",
    "        self.fc3 = nn.Linear(nodes_2,num_actions)\n",
    "    \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deep_Q_Network(8,4).to(device)\n",
    "print(device)\n",
    "states = env.reset()\n",
    "states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "model(states).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MemoryReplay:\n",
    "#     def __init__(self, max_size):\n",
    "#         self.states = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.next_states = []\n",
    "#         self.Dones = []\n",
    "#         self.max_size = max_size\n",
    "#         self.idx = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def append(self, state,action, reward,next_state,Done):\n",
    "#         #first in, first out \n",
    "#         if self.idx <= self.max_size:\n",
    "#             self.states.append(state)\n",
    "#             self.actions.append(action)\n",
    "#             self.rewards.append(reward)\n",
    "#             self.next_states.append(next_state)\n",
    "#             self.Dones.append(Done)\n",
    "#         else:\n",
    "#             #overwrite older values \n",
    "#             self.states[self.idx] = state\n",
    "#             self.actions[self.idx]= action\n",
    "#             self.rewards[self.idx] = reward\n",
    "#             self.next_states[self.idx] =next_state\n",
    "#             self.Dones[self.idx] = Done\n",
    "#         self.size = min(self.size + 1, self.max_size)\n",
    "#         self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         indices = sample(range(self.size), batch_size)\n",
    "#         states = np.array(self.states)[indices]\n",
    "#         actions =  np.array(self.actions)[indices]\n",
    "#         rewards = np.array(self.rewards)[indices]\n",
    "#         next_states = np.array(self.next_states)[indices]\n",
    "#         Dones = np.array(self.Dones)[indices]\n",
    "        \n",
    "#         return states,actions, rewards, next_states, Dones\n",
    "#     def __len__(self):\n",
    "#         return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4824d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReplay:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size,seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None]).squeeze(1)\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None]).squeeze(1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None]).squeeze(1)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c1f8b",
   "metadata": {},
   "source": [
    "## test Memory Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6227d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MemoryReplay(10000)\n",
    "\n",
    "num_episodes = 150\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        m1.append(state,action,reward,next_state,Done)\n",
    "        state = next_state\n",
    "    #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "len(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2 = MemoryReplay_2(10000)\n",
    "\n",
    "# num_episodes = 150\n",
    "# for i in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     Done = False\n",
    "#     total_rewards = 0\n",
    "    \n",
    "#     while not Done: \n",
    "#         #take a random action\n",
    "#         action = env.action_space.sample()\n",
    "#         #implement the action \n",
    "#         next_state, reward, Done, info = env.step(action)\n",
    "#         #sum the rewards\n",
    "#         total_rewards += reward\n",
    "#         m2.append(state,action,reward,next_state,Done)\n",
    "#         state = next_state\n",
    "#     #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "# len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states1,actions1,rewards2,next_states1,Dones1 = m1.sample(32)\n",
    "# states2,actions2,rewards2,next_states2,Dones2 = m2.sample(32)\n",
    "# print(Dones1.shape, Dones2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit m1.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c788cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL_Agent:\n",
    "    def __init__(self,env,memory_max_size =10_000,dicount= 0.99,lr_optim=1e-3,DQL_node1=50,DQL_node2=50,decay_rate = 0.996,seed =10):\n",
    "        self.env = env\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        self.dicount = dicount\n",
    "        self.seed = seed\n",
    "        self.eps = 1.0\n",
    "        self.decay_rate_eps = decay_rate\n",
    "        self.min_eps = 0.05\n",
    "        \n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.reply_memory = MemoryReplay(memory_max_size,self.seed)\n",
    "        self.Q_action = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2,self.seed).to(self.device)\n",
    "        self.Q_target = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2,self.seed).to(self.device)\n",
    "        self.Q_target.eval() #will turn off any dropout or batch norm layer \n",
    "        #make sure both network has identical weights \n",
    "        self.update_target_weights()\n",
    "        \n",
    "        self.loss_fucntion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.Q_action.parameters(),lr=lr_optim)\n",
    "        \n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        self.Q_target.load_state_dict(self.Q_action.state_dict())\n",
    "    \n",
    "    def eps_greedy(self,states):\n",
    "        if np.random.rand()<self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            #act greedy\n",
    "            \n",
    "            #make sure the state are tensor in order to feed it to the network\n",
    "            if not torch.is_tensor(states):\n",
    "                states = torch.from_numpy(states[np.newaxis,:]).float().to(self.device)\n",
    "            with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "                action = self.Q_action(states)\n",
    "            max_action = torch.argmax(action).item()\n",
    "            return max_action\n",
    "    \n",
    "    def decay_eps(self):\n",
    "        self.eps = np.maximum(self.eps*self.decay_rate_eps,self.min_eps)\n",
    "    \n",
    "    def to_tensor(self,states, actions, rewards,next_states,Dones):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        Dones = torch.from_numpy(Dones).to(self.device)\n",
    "        return states, actions, rewards, next_states, Dones\n",
    "    def learnFromExperience(self,miniBatchSize): #hallucinations\n",
    "        if miniBatchSize <2:\n",
    "            raise ValueError(\"batch size must greater than 1\")\n",
    "        #make sure we have enough experiences \n",
    "        if len(self.reply_memory) < miniBatchSize:\n",
    "            return #not enough experience, sounds familiar right?\n",
    "        #else sample and learn\n",
    "        states, actions, rewards, next_states, Dones = self.reply_memory.sample(miniBatchSize)\n",
    "        #convert the result to tensor for model input \n",
    "        states, actions, rewards, next_states, Dones = self.to_tensor(states, actions, rewards, next_states, Dones)\n",
    "        #calculate the current Q estimation \n",
    "        Q_estimate = self.Q_action(states)\n",
    "        #obtain the q value for the actioned used in the experiences \n",
    "        Q_estimate_a = Q_estimate.gather(1, actions.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        #calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\n",
    "        #the max gives both the max values and the indices \n",
    "        Q_target = self.Q_target(next_states).max(dim=1).values\n",
    "        #note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\n",
    "        #if Done is true, we should not calculate Q for the next states \n",
    "        Q_target[Dones] = 0.0 \n",
    "        #final target calculation\n",
    "        Q_target = rewards + self.dicount*Q_target\n",
    "        \n",
    "        #make sure the grad is zero \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #calculate the loss \n",
    "        loss=self.loss_fucntion(Q_target,Q_estimate_a)\n",
    "        #calcualte the gradient dL/dw\n",
    "        loss.backward()\n",
    "        #optimize using gradient decent\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_max_action(self,states):\n",
    "        self.Q_action.eval()\n",
    "        #make sure the state are tensor in order to feed it to the network\n",
    "        if not torch.is_tensor(states):\n",
    "            states = torch.from_numpy(states[np.newaxis,:]).to(self.device)\n",
    "        with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "            action = self.Q_action(states)\n",
    "        max_action = torch.argmax(action).item()\n",
    "        return max_action\n",
    "        \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.Q_action.state_dict(), path) \n",
    "    \n",
    "    def load_model(self,path):\n",
    "        self.Q_action.load_state_dict(torch.load(path))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce2bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d89db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-17 22:59:15,791]\u001b[0m A new study created in memory with name: no-name-dcfb937c-c919-450e-a055-5815315f6959\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -131.0458338132981\n",
      "The episode 11 total rewards is -147.29479223035523\n",
      "The episode 21 total rewards is -118.54849779012977\n",
      "The episode 31 total rewards is -207.44683440882324\n",
      "The episode 41 total rewards is -99.52256575601781\n",
      "The episode 51 total rewards is -53.67533116976352\n",
      "The episode 61 total rewards is -187.0833867066617\n",
      "The episode 71 total rewards is -197.51264964571953\n",
      "The episode 81 total rewards is -58.3746173645153\n",
      "The episode 91 total rewards is -63.18004994143435\n",
      "The episode 101 total rewards is -37.696703925775154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards_testing)\n\u001b[1;32m     74\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Create a new study.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[1;32m     44\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[0;34m(self, miniBatchSize)\u001b[0m\n\u001b[1;32m     69\u001b[0m Q_estimate_a \u001b[38;5;241m=\u001b[39m Q_estimate\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#the max gives both the max values and the indices \u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#if Done is true, we should not calculate Q for the next states \u001b[39;00m\n\u001b[1;32m     76\u001b[0m Q_target[Dones] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#using optuna\n",
    "\n",
    "\n",
    "def train(trial):\n",
    "    seed = trial.suggest_int('seed', 1, 300)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    memory_max_size = 10_000\n",
    "#     DQL_node1 = trial.suggest_int('DQL_nodes 1', 30, 100)\n",
    "#     DQL_node2 = trial.suggest_int('nodes_2', 30, 100)\n",
    "#     dicount = trial.suggest_float('dicount rate', 0.9, 1.0, log=True)\n",
    "    DQL_node1 = 86\n",
    "    DQL_node2 = 47\n",
    "    dicount = 0.991\n",
    "    lr_optim = trial.suggest_float('lr', 5e-4, 1e-3, log=True)\n",
    "    #batch_size = trial.suggest_int('batch size', 32, 64)\n",
    "    batch_size = 64\n",
    "    decay_rate = trial.suggest_float('decay', 0.99,0.999, log=True)\n",
    "    agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,DQL_node1,DQL_node2,decay_rate,seed = seed)\n",
    "    update_freq = 1000\n",
    "    steps = 0 \n",
    "    \n",
    "    num_episodes = 500\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    moving_average = []\n",
    "    for i in range(num_episodes):\n",
    "        state = agent.env.reset()\n",
    "        Done = False\n",
    "        total_rewards = 0\n",
    "        while not Done: \n",
    "            #take an action using the greedy policy\n",
    "            action = agent.eps_greedy(state)\n",
    "            #implement the action \n",
    "            next_state, reward, Done, info = agent.env.step(action)\n",
    "            #save the experience in the memory of the agent \n",
    "            agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "            #sum the rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            #learn from experience (if there is enough)\n",
    "            agent.learnFromExperience(batch_size)\n",
    "            #update the tarqet network per the desired frequency \n",
    "            steps +=1 \n",
    "            if (steps % update_freq) == 0:\n",
    "                agent.update_target_weights()\n",
    "            state = next_state\n",
    "        #append the rewards\n",
    "        rewards[i] = total_rewards\n",
    "        moving_average.append(np.mean(rewards[-50:]))\n",
    "        #update the eps \n",
    "        agent.decay_eps()\n",
    "        if i %10 == 0:\n",
    "            print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "            #print(len(agent.reply_memory))\n",
    "        \n",
    "    #testing \n",
    "    num_tests = 100\n",
    "    rewards_testing = np.zeros(num_tests)\n",
    "    for i in range(num_tests):\n",
    "        state = env.reset()\n",
    "        Done = False \n",
    "        total_rewards = 0\n",
    "        while not Done: \n",
    "            action = agent.get_max_action(state)\n",
    "            next_state, reward,Done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "        rewards_testing[i]= total_rewards\n",
    "\n",
    "    print(\"The testing mean rewards is \", np.mean(rewards_testing))\n",
    "        \n",
    "    return np.mean(rewards_testing)\n",
    "study = optuna.create_study(direction=\"maximize\")  # Create a new study.\n",
    "study.optimize(train, n_trials=30)  # Invoke optimization of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ede0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_agent:\n",
    "    #Agent and environment interaction\n",
    "    def __init__(self, env,memory_max_size =10_000,discount= 0.99,lr_optim=0.00065,update_freq =1000,DQL_node1=88,DQL_node2=50,decay_rate=0.991):\n",
    "        self.seed =77502\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        env.seed(self.seed)\n",
    "        self.agent = DQL_Agent(env,memory_max_size ,discount ,lr_optim,DQL_node1,DQL_node2,decay_rate,self.seed)\n",
    "        self.update_freq = update_freq\n",
    "        self.steps = 0 \n",
    "    \n",
    "    def train_agent(self, num_episodes,batch_size):\n",
    "        self.rewards = np.zeros(num_episodes)\n",
    "        self.moving_average = []\n",
    "        for i in range(num_episodes):\n",
    "            state = self.agent.env.reset()\n",
    "            Done = False\n",
    "            total_rewards = 0\n",
    "            n = 0\n",
    "            while not Done: \n",
    "                n +=1\n",
    "                #take an action using the greedy policy\n",
    "                action = self.agent.eps_greedy(state)\n",
    "                #implement the action \n",
    "                next_state, reward, Done, info = self.agent.env.step(action)\n",
    "                #save the experience in the memory of the agent \n",
    "                self.agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "                #sum the rewards\n",
    "                total_rewards += reward\n",
    "                \n",
    "                #learn from experience (if there is enough)\n",
    "                self.agent.learnFromExperience(batch_size)\n",
    "                #update the tarqet network per the desired frequency \n",
    "                self.steps +=1 \n",
    "                if (self.steps % self.update_freq) == 0:\n",
    "                    self.agent.update_target_weights()\n",
    "                state = next_state\n",
    "            #append the rewards\n",
    "            self.rewards[i] = total_rewards\n",
    "            self.moving_average.append(np.mean(self.rewards[-50:]))\n",
    "            #update the eps \n",
    "            self.agent.decay_eps()\n",
    "            if i %10 == 0:\n",
    "                print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "                print(len(self.agent.reply_memory))\n",
    "    def test_agent(self,num_run, render=False):\n",
    "        rewards = np.zeros(num_run)\n",
    "        for i in range(num_run):\n",
    "            state = self.agent.env.reset()\n",
    "            Done = False \n",
    "            total_rewards = 0\n",
    "\n",
    "            while not Done: \n",
    "                action = self.agent.get_max_action(state)\n",
    "                next_state, reward,Done, info = self.agent.env.step(action)\n",
    "                total_rewards += reward\n",
    "                if render:\n",
    "                    self.agent.env.render()\n",
    "                state = next_state\n",
    "            rewards[i]= total_rewards\n",
    "            #print(\"The episode total rewards is \", total_rewards)\n",
    "        return rewards\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7190658",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a2dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -140.99090114052726\n",
      "118\n",
      "The episode 11 total rewards is -287.2062880338501\n",
      "1025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m training_agent \u001b[38;5;241m=\u001b[39m Training_agent(env)\n\u001b[1;32m      4\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtraining_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m ((time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60.\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mTraining_agent.train_agent\u001b[0;34m(self, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[0;34m(self, miniBatchSize)\u001b[0m\n\u001b[1;32m     84\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fucntion(Q_target,Q_estimate_a)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#calcualte the gradient dL/dw\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#optimize using gradient decent\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('LunarLander-v2')\n",
    "training_agent = Training_agent(env)\n",
    "start = time.time()\n",
    "training_agent.train_agent(num_eps,64)\n",
    "print ((time.time()-start)/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "#training_agent.agent.save_model(\"model.pt\") #weights \n",
    "\n",
    "#whole model \n",
    "model_scripted = torch.jit.script(training_agent.agent.Q_action) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ac69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_agent.agent.Q_action.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "#whole model \n",
    "#model = torch.jit.load('model_scripted.pt')\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e6c30",
   "metadata": {},
   "source": [
    "# Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82422048",
   "metadata": {},
   "source": [
    "## Experiemnt 1: traininig rewards + 50 moving average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e14589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "moving_average_period = 50\n",
    "moving_avg = moving_average(training_agent.rewards,n=moving_average_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69786029",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(moving_average_period,num_eps+1)\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(training_agent.rewards)\n",
    "plt.plot(x,moving_avg,label='50 episodeing moving average')\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.title('Training DQL agent')\n",
    "plt.legend()\n",
    "plt.savefig('training_500_eps.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba259d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = training_agent.test_agent(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_rewards[test_rewards>200]))\n",
    "print(test_rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(test_rewards)\n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.axvline(test_rewards.mean(),label=\"average rewards\",color='g',linestyle='--',linewidth=4)\n",
    "plt.hist(test_rewards, bins=8, linewidth=0.5, edgecolor=\"white\")\n",
    "\n",
    "# plt.set(xlim=(-100, 300),\n",
    "#        ylim=(0, 60), yticks=np.linspace(0, 56, 9))\n",
    "plt.legend()\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('rewards')\n",
    "plt.title(\"Rewards of 100 independent episodes\")\n",
    "plt.show()\n",
    "#plt.close()\n",
    "fig.savefig('testing_100_eps.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca074d8",
   "metadata": {},
   "source": [
    "## experiment 3: change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c146833",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_eps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3,5e-3,1e-4,5e-4,6.1e-5]\n",
    "rewards_lrs = []\n",
    "test_rewards_lr = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "for lr in lrs:\n",
    "    training_agent = Training_agent(env,lr_optim=lr)\n",
    "    start = time.time()\n",
    "    training_agent.train_agent(num_eps,64)\n",
    "    print ((time.time()-start)/60.)\n",
    "    rewards_lrs.append(training_agent.rewards)\n",
    "    \n",
    "    test_rewards = training_agent.test_agent(num_test_eps)\n",
    "    test_rewards_lr.append(test_rewards)\n",
    "    print(\"Average testing rewards: \",test_rewards.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_period = 50\n",
    "x = np.arange(moving_average_period,num_eps+1)\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for i in range(len(lrs)):\n",
    "    moving_avg = moving_average(rewards_lrs[i],n=moving_average_period)\n",
    "    plt.plot(x,moving_avg,label='lr = {}'.format(lrs[i]))\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.title('50 epsidoes moving average for different learning rate')\n",
    "plt.legend()\n",
    "plt.savefig('lr_exp.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot:\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "means = np.zeros(len(test_rewards_lr))\n",
    "labels = ['1e-3','5e-3','1e-4','5e-4','1e-5']\n",
    "for i in range(len(test_rewards_lr)):\n",
    "    means[i] = test_rewards_lr[i].mean()\n",
    "\n",
    "ax.bar(labels, means, width=0.9, edgecolor=\"white\", linewidth=0.7)\n",
    "\n",
    "\n",
    "# plt.set(xlim=(-100, 300),\n",
    "#        ylim=(0, 60), yticks=np.linspace(0, 56, 9))\n",
    "plt.title('Average rewards of 100 episodes for different learning rates')\n",
    "plt.ylabel('Average rewards')\n",
    "plt.xlabel('learning rate of DQN agent')\n",
    "plt.savefig('lr_exp_testing.pdf',format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1a86c",
   "metadata": {},
   "source": [
    "## Exeperiment 4: change the discount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts = [0.8,0.9,0.99,0.999]\n",
    "rewards_discounts = []\n",
    "test_rewards_discount = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "for discount in discounts:\n",
    "    training_agent = Training_agent(env,discount=discount)\n",
    "    start = time.time()\n",
    "    training_agent.train_agent(num_eps,64)\n",
    "    print ((time.time()-start)/60.)\n",
    "    rewards_discounts.append(training_agent.rewards)\n",
    "    \n",
    "    test_rewards = training_agent.test_agent(num_test_eps)\n",
    "    test_rewards_discount.append(test_rewards)\n",
    "    print(\"Average testing rewards: \",test_rewards.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_period = 50\n",
    "x = np.arange(moving_average_period,num_eps+1)\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for i in range(len(discounts)):\n",
    "    moving_avg = moving_average(rewards_discounts[i],n=moving_average_period)\n",
    "    plt.plot(x,moving_avg,label='dicount rate = {}'.format(discounts[i]))\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.title('50 epsidoes moving average for different discount factor')\n",
    "plt.legend()\n",
    "plt.savefig('discount_exp.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b110f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "means = np.zeros(len(test_rewards_discount))\n",
    "labels = ['0.8','0.9','0.99','0.999']\n",
    "for i in range(len(test_rewards_discount)):\n",
    "    means[i] = test_rewards_discount[i].mean()\n",
    "\n",
    "ax.bar(labels, means, width=0.8, edgecolor=\"white\", linewidth=0.7)\n",
    "\n",
    "\n",
    "# plt.set(xlim=(-100, 300),\n",
    "#        ylim=(0, 60), yticks=np.linspace(0, 56, 9))\n",
    "plt.title('Average rewards of 100 episodes for different dicount rates')\n",
    "plt.ylabel('Average rewards')\n",
    "plt.xlabel('Discount rate for rewards')\n",
    "plt.savefig('discount_exp_testing.pdf',format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb21c34",
   "metadata": {},
   "source": [
    "## Exeperiment 5: change the decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0844dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = [0.9,0.95,0.99,0.999]\n",
    "rewards_decay = []\n",
    "test_rewards_decay = []\n",
    "#num_test_eps = 50\n",
    "env = gym.make('LunarLander-v2')\n",
    "for decay in decay_rates:\n",
    "    training_agent = Training_agent(env,decay_rate=decay)\n",
    "    start = time.time()\n",
    "    training_agent.train_agent(num_eps,64)\n",
    "    print ((time.time()-start)/60.)\n",
    "    rewards_decay.append(training_agent.rewards)\n",
    "    \n",
    "    test_rewards = training_agent.test_agent(num_test_eps)\n",
    "    test_rewards_decay.append(test_rewards)\n",
    "    print(\"Average testing rewards: \",test_rewards.mean())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2559a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "moving_average_period = 50\n",
    "x = np.arange(moving_average_period,num_eps+1)\n",
    "fig = plt.figure()\n",
    "for i in range(len(decay_rates)):\n",
    "    moving_avg = moving_average(rewards_decay[i],n=moving_average_period)\n",
    "    plt.plot(x,moving_avg,label='decay rate = {}'.format(decay_rates[i]))\n",
    "\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.title('50 epsidoes moving average for different decay rates')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('decay_exp.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "means = np.zeros(len(test_rewards_decay))\n",
    "labels = ['0.9','0.95','0.99','0.999']\n",
    "for i in range(len(test_rewards_decay)):\n",
    "    means[i] = test_rewards_decay[i].mean()\n",
    "\n",
    "ax.bar(labels, means, width=0.8, edgecolor=\"white\", linewidth=0.7)\n",
    "\n",
    "\n",
    "# plt.set(xlim=(-100, 300),\n",
    "#        ylim=(0, 60), yticks=np.linspace(0, 56, 9))\n",
    "plt.title('Average rewards of 100 episodes for different decay rates')\n",
    "plt.ylabel('Average rewards')\n",
    "plt.xlabel('decay rate for epsilon greedy')\n",
    "plt.savefig('decay_exp_testing.pdf',format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcb756",
   "metadata": {},
   "source": [
    "## experiment 5: Different value of memory size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2205c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_sizes = [1000,5000,10000,15000]\n",
    "rewards_memory_sizes = []\n",
    "test_rewards_memory_sizes = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "#num_test_eps = 50\n",
    "for max_memory in memory_sizes:\n",
    "    training_agent = Training_agent(env,memory_max_size =max_memory )\n",
    "    start = time.time()\n",
    "    training_agent.train_agent(num_eps,64)\n",
    "    print ((time.time()-start)/60.)\n",
    "    rewards_memory_sizes.append(training_agent.rewards)\n",
    "    \n",
    "    test_rewards = training_agent.test_agent(num_test_eps)\n",
    "    test_rewards_memory_sizes.append(test_rewards)\n",
    "    print(\"Average testing rewards: \",test_rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_period = 50\n",
    "x = np.arange(moving_average_period,num_eps+1)\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for i in range(len(memory_sizes)):\n",
    "    moving_avg = moving_average(rewards_memory_sizes[i],n=moving_average_period)\n",
    "    plt.plot(x,moving_avg,label='Memory max size = {}'.format(memory_sizes[i]))\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.title('50 epsidoes moving average for different Replay memory size')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('memory_exp.pdf',format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot:\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "means = np.zeros(len(test_rewards_memory_sizes))\n",
    "labels = ['1000','5000','10000','15000']\n",
    "for i in range(len(test_rewards_decay)):\n",
    "    means[i] = test_rewards_memory_sizes[i].mean()\n",
    "\n",
    "ax.bar(labels, means, width=0.7, edgecolor=\"white\", linewidth=0.7)\n",
    "\n",
    "\n",
    "# plt.set(xlim=(-100, 300),\n",
    "#        ylim=(0, 60), yticks=np.linspace(0, 56, 9))\n",
    "plt.title('Average rewards of 100 episodes for different reply memory sizes')\n",
    "plt.ylabel('rewards')\n",
    "plt.xlabel('reply memory size')\n",
    "plt.savefig('memory_exp_testing.pdf',format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ee64b",
   "metadata": {},
   "source": [
    "# test with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('model_scripted.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"model.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Q_action = Deep_Q_Network(num_states=8,num_actions=4,nodes_1 =86, nodes_2 = 47).to(device)\n",
    "Q_action.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da812c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_action(Q_action,states,device):\n",
    "    Q_action.eval()\n",
    "    #make sure the state are tensor in order to feed it to the network\n",
    "    if not torch.is_tensor(states):\n",
    "        states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "    with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "        action = Q_action(states)\n",
    "    max_action = torch.argmax(action).item()\n",
    "    return max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a36656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.wrappers.Monitor(gym.make('LunarLander-v2'), './video', force=True)\n",
    "state = env.reset()\n",
    "Done = False \n",
    "total_rewards = 0\n",
    "\n",
    "while not Done: \n",
    "    action = get_max_action(Q_action,state,device)\n",
    "    next_state, reward, Done, info= env.step(action)\n",
    "    total_rewards += reward\n",
    "    \n",
    "    env.render()\n",
    "    state = next_state\n",
    "print(total_rewards)\n",
    "#print(\"The episode total rewards is \", total_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bbb893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b024f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
