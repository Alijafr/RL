{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb51a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque, namedtuple\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc21110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16c73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode total rewards is  -135.17469718325276\n",
      "The episode total rewards is  -116.7741200625006\n",
      "The episode total rewards is  -265.583606382973\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        #render the env\n",
    "        env.render()\n",
    "    print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15bf757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8755872e-04,  1.4036485e+00, -5.9532933e-02, -3.2317889e-01,\n",
       "        6.8766251e-04,  1.3485086e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e584f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self,num_states,num_actions,nodes_1 =50, nodes_2 = 50):\n",
    "        super(Deep_Q_Network,self).__init__()\n",
    "        self.fc1 = nn.Linear(num_states,nodes_1)\n",
    "        self.fc2 = nn.Linear(nodes_1,nodes_2)\n",
    "        self.fc3 = nn.Linear(nodes_2,num_actions)\n",
    "    \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f753be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1358, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deep_Q_Network(8,4).to(device)\n",
    "print(device)\n",
    "states = env.reset()\n",
    "states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "model(states).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2ee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MemoryReplay:\n",
    "#     def __init__(self, max_size):\n",
    "#         self.states = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.next_states = []\n",
    "#         self.Dones = []\n",
    "#         self.max_size = max_size\n",
    "#         self.idx = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def append(self, state,action, reward,next_state,Done):\n",
    "#         #first in, first out \n",
    "#         if self.idx <= self.max_size:\n",
    "#             self.states.append(state)\n",
    "#             self.actions.append(action)\n",
    "#             self.rewards.append(reward)\n",
    "#             self.next_states.append(next_state)\n",
    "#             self.Dones.append(Done)\n",
    "#         else:\n",
    "#             #overwrite older values \n",
    "#             self.states[self.idx] = state\n",
    "#             self.actions[self.idx]= action\n",
    "#             self.rewards[self.idx] = reward\n",
    "#             self.next_states[self.idx] =next_state\n",
    "#             self.Dones[self.idx] = Done\n",
    "#         self.size = min(self.size + 1, self.max_size)\n",
    "#         self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         indices = sample(range(self.size), batch_size)\n",
    "#         states = np.array(self.states)[indices]\n",
    "#         actions =  np.array(self.actions)[indices]\n",
    "#         rewards = np.array(self.rewards)[indices]\n",
    "#         next_states = np.array(self.next_states)[indices]\n",
    "#         Dones = np.array(self.Dones)[indices]\n",
    "        \n",
    "#         return states,actions, rewards, next_states, Dones\n",
    "#     def __len__(self):\n",
    "#         return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4824d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReplay:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None]).squeeze(1)\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None]).squeeze(1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None]).squeeze(1)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c1f8b",
   "metadata": {},
   "source": [
    "## test Memory Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6227d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MemoryReplay(10000)\n",
    "\n",
    "num_episodes = 150\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        m1.append(state,action,reward,next_state,Done)\n",
    "        state = next_state\n",
    "    #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "len(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "733f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2 = MemoryReplay_2(10000)\n",
    "\n",
    "# num_episodes = 150\n",
    "# for i in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     Done = False\n",
    "#     total_rewards = 0\n",
    "    \n",
    "#     while not Done: \n",
    "#         #take a random action\n",
    "#         action = env.action_space.sample()\n",
    "#         #implement the action \n",
    "#         next_state, reward, Done, info = env.step(action)\n",
    "#         #sum the rewards\n",
    "#         total_rewards += reward\n",
    "#         m2.append(state,action,reward,next_state,Done)\n",
    "#         state = next_state\n",
    "#     #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "# len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ba087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states1,actions1,rewards2,next_states1,Dones1 = m1.sample(32)\n",
    "# states2,actions2,rewards2,next_states2,Dones2 = m2.sample(32)\n",
    "# print(Dones1.shape, Dones2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e3351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 µs ± 7.07 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit m1.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c788cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL_Agent:\n",
    "    def __init__(self,env,memory_max_size =10_000,dicount= 0.99,lr_optim=1e-3,DQL_node1=50,DQL_node2=50,decay_rate = 0.996):\n",
    "        self.env = env\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        self.dicount = dicount\n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.decay_rate_eps = decay_rate\n",
    "        self.min_eps = 0.05\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.reply_memory = MemoryReplay(memory_max_size)\n",
    "        self.Q_action = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2).to(self.device)\n",
    "        self.Q_target = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2).to(self.device)\n",
    "        self.Q_target.eval() #will turn off any dropout or batch norm layer \n",
    "        #make sure both network has identical weights \n",
    "        self.update_target_weights()\n",
    "        \n",
    "        self.loss_fucntion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.Q_action.parameters(),lr=lr_optim)\n",
    "        \n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        self.Q_target.load_state_dict(self.Q_action.state_dict())\n",
    "    \n",
    "    def eps_greedy(self,states):\n",
    "        if np.random.rand()<self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            #act greedy\n",
    "            \n",
    "            #make sure the state are tensor in order to feed it to the network\n",
    "            if not torch.is_tensor(states):\n",
    "                states = torch.from_numpy(states[np.newaxis,:]).float().to(self.device)\n",
    "            with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "                action = self.Q_action(states)\n",
    "            max_action = torch.argmax(action).item()\n",
    "            return max_action\n",
    "    \n",
    "    def decay_eps(self):\n",
    "        self.eps = np.maximum(self.eps*self.decay_rate_eps,self.min_eps)\n",
    "    \n",
    "    def to_tensor(self,states, actions, rewards,next_states,Dones):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        Dones = torch.from_numpy(Dones).to(self.device)\n",
    "        return states, actions, rewards, next_states, Dones\n",
    "    def learnFromExperience(self,miniBatchSize): #hallucinations\n",
    "        if miniBatchSize <2:\n",
    "            raise ValueError(\"batch size must greater than 1\")\n",
    "        #make sure we have enough experiences \n",
    "        if len(self.reply_memory) < miniBatchSize:\n",
    "            return #not enough experience, sounds familiar right?\n",
    "        #else sample and learn\n",
    "        states, actions, rewards, next_states, Dones = self.reply_memory.sample(miniBatchSize)\n",
    "        #convert the result to tensor for model input \n",
    "        states, actions, rewards, next_states, Dones = self.to_tensor(states, actions, rewards, next_states, Dones)\n",
    "        #calculate the current Q estimation \n",
    "        Q_estimate = self.Q_action(states)\n",
    "        #obtain the q value for the actioned used in the experiences \n",
    "        Q_estimate_a = Q_estimate.gather(1, actions.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        #calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\n",
    "        #the max gives both the max values and the indices \n",
    "        Q_target = self.Q_target(next_states).max(dim=1).values\n",
    "        #note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\n",
    "        #if Done is true, we should not calculate Q for the next states \n",
    "        Q_target[Dones] = 0.0 \n",
    "        #final target calculation\n",
    "        Q_target = rewards + self.dicount*Q_target\n",
    "        \n",
    "        #make sure the grad is zero \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #calculate the loss \n",
    "        loss=self.loss_fucntion(Q_target,Q_estimate_a)\n",
    "        #calcualte the gradient dL/dw\n",
    "        loss.backward()\n",
    "        #optimize using gradient decent\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_max_action(self,states):\n",
    "        self.Q_action.eval()\n",
    "        #make sure the state are tensor in order to feed it to the network\n",
    "        if not torch.is_tensor(states):\n",
    "            states = torch.from_numpy(states[np.newaxis,:]).to(self.device)\n",
    "        with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "            action = self.Q_target(states)\n",
    "        max_action = torch.argmax(action).item()\n",
    "        return max_action\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d89db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 10:12:54,233]\u001b[0m A new study created in memory with name: no-name-c42203fc-fb45-4b84-8248-bcf7d169d01d\u001b[0m\n",
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -60.99651514608226\n",
      "The episode 11 total rewards is -45.07459334350835\n",
      "The episode 21 total rewards is -63.8012334359202\n",
      "The episode 31 total rewards is -384.6175657216391\n",
      "The episode 41 total rewards is -98.22663980922941\n",
      "The episode 51 total rewards is -151.95449956523214\n",
      "The episode 61 total rewards is -112.28316282765346\n",
      "The episode 71 total rewards is -115.48864030053866\n",
      "The episode 81 total rewards is -113.14083296814675\n",
      "The episode 91 total rewards is -173.68948627533342\n",
      "The episode 101 total rewards is -137.10765697578097\n",
      "The episode 111 total rewards is -109.57029347659477\n",
      "The episode 121 total rewards is -243.2007835745487\n",
      "The episode 131 total rewards is 22.160681057851004\n",
      "The episode 141 total rewards is 122.89363404769055\n",
      "The episode 151 total rewards is -54.741260214367756\n",
      "The episode 161 total rewards is -173.06106424620958\n",
      "The episode 171 total rewards is 232.42082658462397\n",
      "The episode 181 total rewards is 48.18079276994457\n",
      "The episode 191 total rewards is -39.73466602827768\n",
      "The episode 201 total rewards is -18.000592662360937\n",
      "The episode 211 total rewards is -6.8932790248444\n",
      "The episode 221 total rewards is 232.66196160782397\n",
      "The episode 231 total rewards is -62.926352514086666\n",
      "The episode 241 total rewards is -208.3625996239504\n",
      "The episode 251 total rewards is 15.287048011299518\n",
      "The episode 261 total rewards is -125.01400980835082\n",
      "The episode 271 total rewards is -133.30087381467163\n",
      "The episode 281 total rewards is -130.82975114059644\n",
      "The episode 291 total rewards is -65.9910299163875\n",
      "The episode 301 total rewards is 236.5745872261053\n",
      "The episode 311 total rewards is -53.212363359600445\n",
      "The episode 321 total rewards is -38.292075953031755\n",
      "The episode 331 total rewards is -101.33605930982708\n",
      "The episode 341 total rewards is -41.827316081056786\n",
      "The episode 351 total rewards is -110.26260171379172\n",
      "The episode 361 total rewards is -77.72697415568169\n",
      "The episode 371 total rewards is -79.96225522903626\n",
      "The episode 381 total rewards is -118.96458093796048\n",
      "The episode 391 total rewards is -53.012355097407095\n",
      "The episode 401 total rewards is -111.08509503575668\n",
      "The episode 411 total rewards is -92.80329450182478\n",
      "The episode 421 total rewards is -129.38410090800372\n",
      "The episode 431 total rewards is -67.4847848622665\n",
      "The episode 441 total rewards is -142.86948608438277\n",
      "The episode 451 total rewards is -52.825086675166396\n",
      "The episode 461 total rewards is -144.7555130647712\n",
      "The episode 471 total rewards is -78.59907062954599\n",
      "The episode 481 total rewards is -90.76134147532635\n",
      "The episode 491 total rewards is -64.02804199431164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 10:36:14,385]\u001b[0m Trial 0 finished with value: -98.68959214557484 and parameters: {'DQL_nodes 1': 56, 'nodes_2': 86, 'dicount rate': 0.9200860614467823, 'lr': 0.0005167841459005258, 'batch size': 35, 'decay': 0.9925543837806089}. Best is trial 0 with value: -98.68959214557484.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -98.68959214557484\n",
      "The episode 1 total rewards is -346.25640135940614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -116.04435462659444\n",
      "The episode 21 total rewards is -451.8135424705983\n",
      "The episode 31 total rewards is -91.64468078789406\n",
      "The episode 41 total rewards is -254.25068233959905\n",
      "The episode 51 total rewards is -127.99185776890248\n",
      "The episode 61 total rewards is -112.02489920386273\n",
      "The episode 71 total rewards is -100.26533521026681\n",
      "The episode 81 total rewards is -117.992142974567\n",
      "The episode 91 total rewards is -107.21264198512047\n",
      "The episode 101 total rewards is -22.892017233779313\n",
      "The episode 111 total rewards is 16.37300513492552\n",
      "The episode 121 total rewards is 30.724131055552586\n",
      "The episode 131 total rewards is 157.9438033444951\n",
      "The episode 141 total rewards is 50.90757534577479\n",
      "The episode 151 total rewards is -182.8760838884675\n",
      "The episode 161 total rewards is 104.57343015391491\n",
      "The episode 171 total rewards is -21.472956693277254\n",
      "The episode 181 total rewards is 97.1021480480355\n",
      "The episode 191 total rewards is -113.54362899068954\n",
      "The episode 201 total rewards is -53.71045480443179\n",
      "The episode 211 total rewards is -103.3224361557457\n",
      "The episode 221 total rewards is -98.66798412486153\n",
      "The episode 231 total rewards is -89.59289043265869\n",
      "The episode 241 total rewards is -83.81750645594123\n",
      "The episode 251 total rewards is -86.23437547868441\n",
      "The episode 261 total rewards is -120.5216844574912\n",
      "The episode 271 total rewards is -89.00109303845377\n",
      "The episode 281 total rewards is -106.55626346386838\n",
      "The episode 291 total rewards is -89.78857872000636\n",
      "The episode 301 total rewards is -82.2021713234321\n",
      "The episode 311 total rewards is -94.86085513531215\n",
      "The episode 321 total rewards is -122.85403175737626\n",
      "The episode 331 total rewards is -87.86744870252417\n",
      "The episode 341 total rewards is -83.41034152157624\n",
      "The episode 351 total rewards is -95.9829601693528\n",
      "The episode 361 total rewards is -210.15488124497776\n",
      "The episode 371 total rewards is -82.95379369704698\n",
      "The episode 381 total rewards is -108.29753213726235\n",
      "The episode 391 total rewards is -134.4231944662412\n",
      "The episode 401 total rewards is -118.18445649186171\n",
      "The episode 411 total rewards is -129.38151078412434\n",
      "The episode 421 total rewards is -79.0383777483507\n",
      "The episode 431 total rewards is -60.97434345312054\n",
      "The episode 441 total rewards is -101.86638965259112\n",
      "The episode 451 total rewards is -131.88947356530178\n",
      "The episode 461 total rewards is -121.22008606840684\n",
      "The episode 471 total rewards is -111.79973083074417\n",
      "The episode 481 total rewards is -78.77081908816609\n",
      "The episode 491 total rewards is -104.51607969945775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 11:02:46,914]\u001b[0m Trial 1 finished with value: -115.92403229660147 and parameters: {'DQL_nodes 1': 88, 'nodes_2': 72, 'dicount rate': 0.9215513107105942, 'lr': 0.0005734130909614983, 'batch size': 51, 'decay': 0.9919274394991446}. Best is trial 0 with value: -98.68959214557484.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -115.92403229660147\n",
      "The episode 1 total rewards is -121.29734129054489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -290.2991868185622\n",
      "The episode 21 total rewards is -106.29957571013477\n",
      "The episode 31 total rewards is -225.15388561106317\n",
      "The episode 41 total rewards is -74.66191518148676\n",
      "The episode 51 total rewards is -58.65015285580944\n",
      "The episode 61 total rewards is -175.21397751047206\n",
      "The episode 71 total rewards is -259.6855540256041\n",
      "The episode 81 total rewards is -41.363744966471714\n",
      "The episode 91 total rewards is -287.9264375382528\n",
      "The episode 101 total rewards is -35.85818120627927\n",
      "The episode 111 total rewards is -189.29493557514422\n",
      "The episode 121 total rewards is -74.43241749152244\n",
      "The episode 131 total rewards is -89.72510902819761\n",
      "The episode 141 total rewards is -63.30493383922134\n",
      "The episode 151 total rewards is -195.53140774318447\n",
      "The episode 161 total rewards is 19.010110785130905\n",
      "The episode 171 total rewards is 69.8800182558503\n",
      "The episode 181 total rewards is -85.61090278134402\n",
      "The episode 191 total rewards is -97.34602291644696\n",
      "The episode 201 total rewards is 71.71871475867114\n",
      "The episode 211 total rewards is -28.460076650338834\n",
      "The episode 221 total rewards is 67.59872311596028\n",
      "The episode 231 total rewards is 66.96223500193707\n",
      "The episode 241 total rewards is -83.90520207168439\n",
      "The episode 251 total rewards is 125.55426696260903\n",
      "The episode 261 total rewards is -13.561949121314122\n",
      "The episode 271 total rewards is 20.597226341864996\n",
      "The episode 281 total rewards is 94.17304461344777\n",
      "The episode 291 total rewards is -60.19746565492173\n",
      "The episode 301 total rewards is 115.54824513373599\n",
      "The episode 311 total rewards is -2.6876991788740563\n",
      "The episode 321 total rewards is -68.73033057717501\n",
      "The episode 331 total rewards is -7.43652578795158\n",
      "The episode 341 total rewards is -63.32515912208564\n",
      "The episode 351 total rewards is -35.609156273171735\n",
      "The episode 361 total rewards is 17.506289998847198\n",
      "The episode 371 total rewards is -135.98342509789865\n",
      "The episode 381 total rewards is -78.77489509181987\n",
      "The episode 391 total rewards is -38.790464813715644\n",
      "The episode 401 total rewards is -92.22267414992423\n",
      "The episode 411 total rewards is 174.54136527318124\n",
      "The episode 421 total rewards is -104.5802352170631\n",
      "The episode 431 total rewards is -53.47800667353845\n",
      "The episode 441 total rewards is 232.93422999061121\n",
      "The episode 451 total rewards is -44.51992718528259\n",
      "The episode 461 total rewards is -62.57968423370998\n",
      "The episode 471 total rewards is -64.83101534794294\n",
      "The episode 481 total rewards is 120.14323431339994\n",
      "The episode 491 total rewards is -65.10007927451437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 11:22:20,123]\u001b[0m Trial 2 finished with value: -52.40226163513256 and parameters: {'DQL_nodes 1': 36, 'nodes_2': 85, 'dicount rate': 0.9672087939831399, 'lr': 0.0006933635050814811, 'batch size': 32, 'decay': 0.995094635164832}. Best is trial 2 with value: -52.40226163513256.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -52.40226163513256\n",
      "The episode 1 total rewards is -91.5012028116018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -123.19871015122357\n",
      "The episode 21 total rewards is -199.74757981894203\n",
      "The episode 31 total rewards is -136.69066257074496\n",
      "The episode 41 total rewards is -206.0377410030237\n",
      "The episode 51 total rewards is -90.82079799851742\n",
      "The episode 61 total rewards is -58.44965603519461\n",
      "The episode 71 total rewards is -95.2550974899916\n",
      "The episode 81 total rewards is -121.27987493054465\n",
      "The episode 91 total rewards is 0.3538447329646111\n",
      "The episode 101 total rewards is -68.70317604029644\n",
      "The episode 111 total rewards is -127.10407491630617\n",
      "The episode 121 total rewards is 108.20715289276141\n",
      "The episode 131 total rewards is -60.919049835021994\n",
      "The episode 141 total rewards is -66.7550052954832\n",
      "The episode 151 total rewards is 66.1332251616194\n",
      "The episode 161 total rewards is -82.27789985000791\n",
      "The episode 171 total rewards is -49.11889627843669\n",
      "The episode 181 total rewards is -11.615619701899178\n",
      "The episode 191 total rewards is 130.1507794380093\n",
      "The episode 201 total rewards is -8.079600146863724\n",
      "The episode 211 total rewards is -123.85542346781018\n",
      "The episode 221 total rewards is -85.97530150724174\n",
      "The episode 231 total rewards is -106.39487121818914\n",
      "The episode 241 total rewards is -111.949030290895\n",
      "The episode 251 total rewards is -112.8925223453317\n",
      "The episode 261 total rewards is -33.36039047399701\n",
      "The episode 271 total rewards is -81.34308212137039\n",
      "The episode 281 total rewards is -77.82254827025153\n",
      "The episode 291 total rewards is -131.84173233583118\n",
      "The episode 301 total rewards is -111.15296543694122\n",
      "The episode 311 total rewards is -90.62112674927518\n",
      "The episode 321 total rewards is -130.31471912040098\n",
      "The episode 331 total rewards is -96.48276737901085\n",
      "The episode 341 total rewards is -123.79341154923233\n",
      "The episode 351 total rewards is -107.21123410202445\n",
      "The episode 361 total rewards is -114.11295459787426\n",
      "The episode 371 total rewards is -115.69921824798584\n",
      "The episode 381 total rewards is -74.23658436644413\n",
      "The episode 391 total rewards is -99.82872923239078\n",
      "The episode 401 total rewards is -117.57288821700246\n",
      "The episode 411 total rewards is -87.60013235203219\n",
      "The episode 421 total rewards is -143.47991175643463\n",
      "The episode 431 total rewards is -120.57813659969246\n",
      "The episode 441 total rewards is -123.36613497630965\n",
      "The episode 451 total rewards is -124.45248467533264\n",
      "The episode 461 total rewards is -133.3966677520172\n",
      "The episode 471 total rewards is -100.41767702743545\n",
      "The episode 481 total rewards is -96.85649926937923\n",
      "The episode 491 total rewards is -96.79479024826539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 11:48:18,104]\u001b[0m Trial 3 finished with value: -120.09212250370032 and parameters: {'DQL_nodes 1': 32, 'nodes_2': 59, 'dicount rate': 0.9316206749905309, 'lr': 0.0007177522197684909, 'batch size': 53, 'decay': 0.990399152706941}. Best is trial 2 with value: -52.40226163513256.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -120.09212250370032\n",
      "The episode 1 total rewards is -138.79879612815392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -212.24955370011372\n",
      "The episode 21 total rewards is -86.62011835689005\n",
      "The episode 31 total rewards is -117.04472407239213\n",
      "The episode 41 total rewards is -214.89929189334026\n",
      "The episode 51 total rewards is -321.79620600338137\n",
      "The episode 61 total rewards is -214.2070530485499\n",
      "The episode 71 total rewards is -177.6444000245687\n",
      "The episode 81 total rewards is -117.22229770412248\n",
      "The episode 91 total rewards is -106.80133512970585\n",
      "The episode 101 total rewards is -135.21793905940024\n",
      "The episode 111 total rewards is -117.1864769626681\n",
      "The episode 121 total rewards is -94.79391437307234\n",
      "The episode 131 total rewards is -166.57622667939546\n",
      "The episode 141 total rewards is -90.47309630372277\n",
      "The episode 151 total rewards is -40.44855456743164\n",
      "The episode 161 total rewards is -137.57673804884007\n",
      "The episode 171 total rewards is -148.63081658563323\n",
      "The episode 181 total rewards is -121.9645269794381\n",
      "The episode 191 total rewards is -82.94060706646164\n",
      "The episode 201 total rewards is -85.81013277638591\n",
      "The episode 211 total rewards is -37.3845028268097\n",
      "The episode 221 total rewards is -25.63949877561916\n",
      "The episode 231 total rewards is -60.86732491341024\n",
      "The episode 241 total rewards is -77.51702776083852\n",
      "The episode 251 total rewards is -69.84946020426811\n",
      "The episode 261 total rewards is -31.229893510055888\n",
      "The episode 271 total rewards is 11.693173870728984\n",
      "The episode 281 total rewards is 10.768160783359534\n",
      "The episode 291 total rewards is -17.142726751997944\n",
      "The episode 301 total rewards is -18.862330289226318\n",
      "The episode 311 total rewards is -32.28996215288096\n",
      "The episode 321 total rewards is 21.451269271730038\n",
      "The episode 331 total rewards is -38.19253617304889\n",
      "The episode 341 total rewards is -66.21730215039817\n",
      "The episode 351 total rewards is -63.402657482091065\n",
      "The episode 361 total rewards is 42.300179531088986\n",
      "The episode 371 total rewards is 104.80432115741235\n",
      "The episode 381 total rewards is 117.74042265662436\n",
      "The episode 391 total rewards is -11.560373440841607\n",
      "The episode 401 total rewards is 126.70978132867836\n",
      "The episode 411 total rewards is 42.86076381437543\n",
      "The episode 421 total rewards is -24.750190725040284\n",
      "The episode 431 total rewards is 28.872556562378207\n",
      "The episode 441 total rewards is 108.71036584905318\n",
      "The episode 451 total rewards is 72.49432135646701\n",
      "The episode 461 total rewards is 88.27292822961076\n",
      "The episode 471 total rewards is -56.3480136809128\n",
      "The episode 481 total rewards is 19.332327084193555\n",
      "The episode 491 total rewards is 15.371971494920963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 11:59:27,495]\u001b[0m Trial 4 finished with value: -87.79863478554114 and parameters: {'DQL_nodes 1': 40, 'nodes_2': 60, 'dicount rate': 0.944957058653445, 'lr': 0.0005828258631450602, 'batch size': 35, 'decay': 0.99747995514589}. Best is trial 2 with value: -52.40226163513256.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -87.79863478554114\n",
      "The episode 1 total rewards is -118.09663427100126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -252.5575108393656\n",
      "The episode 21 total rewards is -64.53725712311481\n",
      "The episode 31 total rewards is -368.62642363307634\n",
      "The episode 41 total rewards is -94.63828942671626\n",
      "The episode 51 total rewards is -172.1174682218946\n",
      "The episode 61 total rewards is -84.0396088767785\n",
      "The episode 71 total rewards is -84.20112306758213\n",
      "The episode 81 total rewards is -136.85678771111978\n",
      "The episode 91 total rewards is -22.609132637044382\n",
      "The episode 101 total rewards is -164.45289073111653\n",
      "The episode 111 total rewards is -320.49322336524256\n",
      "The episode 121 total rewards is -146.09647205912353\n",
      "The episode 131 total rewards is -56.68434983751689\n",
      "The episode 141 total rewards is -98.82593530351727\n",
      "The episode 151 total rewards is 49.18670573357895\n",
      "The episode 161 total rewards is -26.27119823942047\n",
      "The episode 171 total rewards is -46.04549242201886\n",
      "The episode 181 total rewards is -98.85890776553045\n",
      "The episode 191 total rewards is -275.8891139176028\n",
      "The episode 201 total rewards is -47.5982356608435\n",
      "The episode 211 total rewards is -14.172046053592538\n",
      "The episode 221 total rewards is -72.78220768976492\n",
      "The episode 231 total rewards is 72.17127679580925\n",
      "The episode 241 total rewards is 39.38262598934352\n",
      "The episode 251 total rewards is -17.665013373535828\n",
      "The episode 261 total rewards is 54.64181437186369\n",
      "The episode 271 total rewards is 32.35141535114792\n",
      "The episode 281 total rewards is 55.95207853574033\n",
      "The episode 291 total rewards is 58.02298200224189\n",
      "The episode 301 total rewards is 161.3303233835827\n",
      "The episode 311 total rewards is -107.72610695575985\n",
      "The episode 321 total rewards is -46.574553525121374\n",
      "The episode 331 total rewards is 16.055866181515583\n",
      "The episode 341 total rewards is 10.90645975256354\n",
      "The episode 351 total rewards is -86.25334372320815\n",
      "The episode 361 total rewards is -140.26076257528078\n",
      "The episode 371 total rewards is -68.52786871401571\n",
      "The episode 381 total rewards is -143.3720954479832\n",
      "The episode 391 total rewards is -7.862879007681968\n",
      "The episode 401 total rewards is 11.27122908897816\n",
      "The episode 411 total rewards is 38.26386573242337\n",
      "The episode 421 total rewards is -71.75781801906508\n",
      "The episode 431 total rewards is -120.19508214197532\n",
      "The episode 441 total rewards is -110.30371995590154\n",
      "The episode 451 total rewards is -93.85355513401595\n",
      "The episode 461 total rewards is -64.9389183965026\n",
      "The episode 471 total rewards is -145.41640993396487\n",
      "The episode 481 total rewards is -35.72884547736549\n",
      "The episode 491 total rewards is -99.49623622418245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 12:15:35,445]\u001b[0m Trial 5 finished with value: -96.83432855948108 and parameters: {'DQL_nodes 1': 49, 'nodes_2': 60, 'dicount rate': 0.9285492678615631, 'lr': 0.0008917517400097173, 'batch size': 56, 'decay': 0.9961184687755145}. Best is trial 2 with value: -52.40226163513256.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -96.83432855948108\n",
      "The episode 1 total rewards is -357.79628534802833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -253.0185493024813\n",
      "The episode 21 total rewards is -99.14210852809316\n",
      "The episode 31 total rewards is -184.21867104714738\n",
      "The episode 41 total rewards is -89.18544737778491\n",
      "The episode 51 total rewards is -70.21047093313545\n",
      "The episode 61 total rewards is -108.08860908386511\n",
      "The episode 71 total rewards is -0.6696935134519322\n",
      "The episode 81 total rewards is -241.23066125651104\n",
      "The episode 91 total rewards is -214.44292229587109\n",
      "The episode 101 total rewards is -54.48232003053671\n",
      "The episode 111 total rewards is -49.555810734047\n",
      "The episode 121 total rewards is 13.900162703031558\n",
      "The episode 131 total rewards is -42.72913783700712\n",
      "The episode 141 total rewards is -69.2339574913596\n",
      "The episode 151 total rewards is -12.39888017611679\n",
      "The episode 161 total rewards is 17.909120545254126\n",
      "The episode 171 total rewards is 62.73212891975407\n",
      "The episode 181 total rewards is 10.296162142139705\n",
      "The episode 191 total rewards is -67.26326089176983\n",
      "The episode 201 total rewards is -25.24139668962119\n",
      "The episode 211 total rewards is -64.98498974027167\n",
      "The episode 221 total rewards is 7.440544601960319\n",
      "The episode 231 total rewards is 74.72159532128299\n",
      "The episode 241 total rewards is 146.5818946653122\n",
      "The episode 251 total rewards is -59.052787984095616\n",
      "The episode 261 total rewards is 253.61823308878428\n",
      "The episode 271 total rewards is 229.26721406705414\n",
      "The episode 281 total rewards is 120.57261706779785\n",
      "The episode 291 total rewards is 209.30608958020974\n",
      "The episode 301 total rewards is 282.2965279022542\n",
      "The episode 311 total rewards is 168.41988059267132\n",
      "The episode 321 total rewards is 137.5622157274605\n",
      "The episode 331 total rewards is -43.54796395442205\n",
      "The episode 341 total rewards is -20.16040038478467\n",
      "The episode 351 total rewards is -45.155259958684844\n",
      "The episode 361 total rewards is 242.72442427458805\n",
      "The episode 371 total rewards is -68.03052348161745\n",
      "The episode 381 total rewards is 55.87174894081242\n",
      "The episode 391 total rewards is -35.160864457402454\n",
      "The episode 401 total rewards is 171.5865507049925\n",
      "The episode 411 total rewards is -80.93778647686561\n",
      "The episode 421 total rewards is -41.213787831504156\n",
      "The episode 431 total rewards is 176.61032628245138\n",
      "The episode 441 total rewards is 209.11158182815774\n",
      "The episode 451 total rewards is 192.1058965368427\n",
      "The episode 461 total rewards is -24.406893704652784\n",
      "The episode 471 total rewards is 196.18320830196217\n",
      "The episode 481 total rewards is -35.61228567853914\n",
      "The episode 491 total rewards is -33.07225552468466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 12:38:22,454]\u001b[0m Trial 6 finished with value: 1.8386306159401875 and parameters: {'DQL_nodes 1': 35, 'nodes_2': 87, 'dicount rate': 0.9767045571089166, 'lr': 0.0005318966176494221, 'batch size': 62, 'decay': 0.9930977369219552}. Best is trial 6 with value: 1.8386306159401875.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  1.8386306159401875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is 53.173251236392055\n",
      "The episode 11 total rewards is -395.1259610058723\n",
      "The episode 21 total rewards is -150.38965177711492\n",
      "The episode 31 total rewards is -176.91449476213103\n",
      "The episode 41 total rewards is -174.43588214648128\n",
      "The episode 51 total rewards is -324.19500557760915\n",
      "The episode 61 total rewards is -93.0472208953633\n",
      "The episode 71 total rewards is -137.2480735649618\n",
      "The episode 81 total rewards is -41.591363595732076\n",
      "The episode 91 total rewards is -318.68073427957245\n",
      "The episode 101 total rewards is -73.65525631331084\n",
      "The episode 111 total rewards is -93.70544147128211\n",
      "The episode 121 total rewards is -75.29870841155737\n",
      "The episode 131 total rewards is -54.00198230612341\n",
      "The episode 141 total rewards is -122.37096320932642\n",
      "The episode 151 total rewards is -91.98675443497362\n",
      "The episode 161 total rewards is -34.301166518909994\n",
      "The episode 171 total rewards is -284.99664099032975\n",
      "The episode 181 total rewards is -127.19554925721876\n",
      "The episode 191 total rewards is -3.7574995594238487\n",
      "The episode 201 total rewards is -31.801348239371137\n",
      "The episode 211 total rewards is -100.66988831306028\n",
      "The episode 221 total rewards is -5.8902936164105455\n",
      "The episode 231 total rewards is -39.9575961219563\n",
      "The episode 241 total rewards is -162.1309408341075\n",
      "The episode 251 total rewards is 15.614700544191534\n",
      "The episode 261 total rewards is -32.641666310520904\n",
      "The episode 271 total rewards is -97.51818609605455\n",
      "The episode 281 total rewards is -140.21985440188575\n",
      "The episode 291 total rewards is 21.445834857758648\n",
      "The episode 301 total rewards is -79.58773890306904\n",
      "The episode 311 total rewards is 51.622020766867045\n",
      "The episode 321 total rewards is -30.690673372401932\n",
      "The episode 331 total rewards is -23.843909884275476\n",
      "The episode 341 total rewards is -5.580990910531379\n",
      "The episode 351 total rewards is -22.645477184545683\n",
      "The episode 361 total rewards is 91.52768436515638\n",
      "The episode 371 total rewards is -10.515508840617343\n",
      "The episode 381 total rewards is 55.43157717770021\n",
      "The episode 391 total rewards is -14.57378797109489\n",
      "The episode 401 total rewards is 131.62107621191618\n",
      "The episode 411 total rewards is 83.75003630012526\n",
      "The episode 421 total rewards is 23.64205340189467\n",
      "The episode 431 total rewards is 128.96622908428327\n",
      "The episode 441 total rewards is 26.115686478434483\n",
      "The episode 451 total rewards is 82.85603110904813\n",
      "The episode 461 total rewards is 151.06783660886526\n",
      "The episode 471 total rewards is 174.83288896504354\n",
      "The episode 481 total rewards is 58.51568764812392\n",
      "The episode 491 total rewards is 52.29857768212726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 12:51:30,025]\u001b[0m Trial 7 finished with value: 74.84767285040733 and parameters: {'DQL_nodes 1': 34, 'nodes_2': 53, 'dicount rate': 0.9667789993027156, 'lr': 0.0006780624462819558, 'batch size': 47, 'decay': 0.997152372794684}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  74.84767285040733\n",
      "The episode 1 total rewards is -147.04112746104386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -2.4875377375809506\n",
      "The episode 21 total rewards is -137.4577559555832\n",
      "The episode 31 total rewards is -143.54466967441533\n",
      "The episode 41 total rewards is -323.3615856828459\n",
      "The episode 51 total rewards is -142.89832969376994\n",
      "The episode 61 total rewards is -264.6163928801037\n",
      "The episode 71 total rewards is -265.28408469382737\n",
      "The episode 81 total rewards is -112.36394860978157\n",
      "The episode 91 total rewards is -98.98401189833918\n",
      "The episode 101 total rewards is 101.24705258381977\n",
      "The episode 111 total rewards is -199.53506520392648\n",
      "The episode 121 total rewards is -51.116341811096405\n",
      "The episode 131 total rewards is -51.68523312930708\n",
      "The episode 141 total rewards is -82.40657537476342\n",
      "The episode 151 total rewards is -104.88501637323851\n",
      "The episode 161 total rewards is -106.99583129847777\n",
      "The episode 171 total rewards is -71.40852662142674\n",
      "The episode 181 total rewards is -29.03884038001368\n",
      "The episode 191 total rewards is -118.73763423576989\n",
      "The episode 201 total rewards is -60.82848226963425\n",
      "The episode 211 total rewards is -185.24923060780296\n",
      "The episode 221 total rewards is -57.51333527806622\n",
      "The episode 231 total rewards is -50.83751324103346\n",
      "The episode 241 total rewards is -32.86871532295966\n",
      "The episode 251 total rewards is -58.01553947078745\n",
      "The episode 261 total rewards is -55.36679929500698\n",
      "The episode 271 total rewards is -31.74060518999866\n",
      "The episode 281 total rewards is -121.19208319561028\n",
      "The episode 291 total rewards is -92.55127703679123\n",
      "The episode 301 total rewards is -65.19686189707974\n",
      "The episode 311 total rewards is -13.262854791969943\n",
      "The episode 321 total rewards is -58.24621798451035\n",
      "The episode 331 total rewards is 27.183632185795513\n",
      "The episode 341 total rewards is 121.47556919905074\n",
      "The episode 351 total rewards is -71.38588018079031\n",
      "The episode 361 total rewards is 144.67052011856148\n",
      "The episode 371 total rewards is -10.507878244865026\n",
      "The episode 381 total rewards is 8.626609333186863\n",
      "The episode 391 total rewards is -100.19695040769672\n",
      "The episode 401 total rewards is 15.9491585376811\n",
      "The episode 411 total rewards is -106.83229693475404\n",
      "The episode 421 total rewards is -148.96425397840738\n",
      "The episode 431 total rewards is 72.02617808612732\n",
      "The episode 441 total rewards is -11.492518902510497\n",
      "The episode 451 total rewards is 44.61780528174393\n",
      "The episode 461 total rewards is -19.40242635536636\n",
      "The episode 471 total rewards is 19.595834564329344\n",
      "The episode 481 total rewards is 72.59634347344422\n",
      "The episode 491 total rewards is 107.98248396797297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 12:59:55,838]\u001b[0m Trial 8 finished with value: -146.33505847512754 and parameters: {'DQL_nodes 1': 76, 'nodes_2': 70, 'dicount rate': 0.9125821182367071, 'lr': 0.0006627580919548853, 'batch size': 40, 'decay': 0.9977425049837204}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -146.33505847512754\n",
      "The episode 1 total rewards is -203.57905282522364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -395.0405018920602\n",
      "The episode 21 total rewards is -442.54827253957865\n",
      "The episode 31 total rewards is -148.88181962060517\n",
      "The episode 41 total rewards is -170.33736130797536\n",
      "The episode 51 total rewards is -147.70517044686886\n",
      "The episode 61 total rewards is -404.56536887334335\n",
      "The episode 71 total rewards is -220.1403093962095\n",
      "The episode 81 total rewards is -23.552458830527513\n",
      "The episode 91 total rewards is -32.173015392688825\n",
      "The episode 101 total rewards is -144.25672088484885\n",
      "The episode 111 total rewards is -59.00487677314677\n",
      "The episode 121 total rewards is -60.00166012982865\n",
      "The episode 131 total rewards is -11.710253555820884\n",
      "The episode 141 total rewards is -175.4998387492932\n",
      "The episode 151 total rewards is -33.61095697882959\n",
      "The episode 161 total rewards is -114.34949908170489\n",
      "The episode 171 total rewards is 86.82563557707964\n",
      "The episode 181 total rewards is 26.50285745253623\n",
      "The episode 191 total rewards is -10.573442130737618\n",
      "The episode 201 total rewards is -48.933091269641544\n",
      "The episode 211 total rewards is 67.87234227848975\n",
      "The episode 221 total rewards is -47.03062694768106\n",
      "The episode 231 total rewards is -89.27961312991584\n",
      "The episode 241 total rewards is -24.1106680094329\n",
      "The episode 251 total rewards is 118.75478375781006\n",
      "The episode 261 total rewards is 91.8653938850459\n",
      "The episode 271 total rewards is 2.2113041292603555\n",
      "The episode 281 total rewards is 29.90142948175571\n",
      "The episode 291 total rewards is -51.870613432635444\n",
      "The episode 301 total rewards is 125.39234431280035\n",
      "The episode 311 total rewards is -16.847583337877992\n",
      "The episode 321 total rewards is -83.47551509371577\n",
      "The episode 331 total rewards is 152.05948312036352\n",
      "The episode 341 total rewards is 116.45093408852391\n",
      "The episode 351 total rewards is -68.58087358528161\n",
      "The episode 361 total rewards is -131.57285295923134\n",
      "The episode 371 total rewards is -167.67690155967523\n",
      "The episode 381 total rewards is -124.45537985390068\n",
      "The episode 391 total rewards is -100.08041695484594\n",
      "The episode 401 total rewards is -66.44336741521221\n",
      "The episode 411 total rewards is -25.48998449010074\n",
      "The episode 421 total rewards is -65.29465723313801\n",
      "The episode 431 total rewards is -136.24787488472225\n",
      "The episode 441 total rewards is -84.74130210475064\n",
      "The episode 451 total rewards is -51.988594152065446\n",
      "The episode 461 total rewards is -107.48905679999004\n",
      "The episode 471 total rewards is -74.21093307283644\n",
      "The episode 481 total rewards is -47.25898534569822\n",
      "The episode 491 total rewards is -19.80770210337365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 13:24:34,836]\u001b[0m Trial 9 finished with value: -126.7356922802243 and parameters: {'DQL_nodes 1': 57, 'nodes_2': 55, 'dicount rate': 0.9437301292540166, 'lr': 0.0006755428257240211, 'batch size': 59, 'decay': 0.9932967524296751}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -126.7356922802243\n",
      "The episode 1 total rewards is -408.82244462702084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -182.83004896405026\n",
      "The episode 21 total rewards is -225.06405893235026\n",
      "The episode 31 total rewards is -140.60744055998518\n",
      "The episode 41 total rewards is -67.88863312502252\n",
      "The episode 51 total rewards is -191.07773075773437\n",
      "The episode 61 total rewards is -273.203591618546\n",
      "The episode 71 total rewards is -441.3396072993776\n",
      "The episode 81 total rewards is -260.85627080572175\n",
      "The episode 91 total rewards is -281.4229053492964\n",
      "The episode 101 total rewards is -68.02649480282872\n",
      "The episode 111 total rewards is -87.53508021106772\n",
      "The episode 121 total rewards is -114.96940181882309\n",
      "The episode 131 total rewards is -109.12424988660068\n",
      "The episode 141 total rewards is -216.71673207938514\n",
      "The episode 151 total rewards is -73.97011438296826\n",
      "The episode 161 total rewards is -151.87510300276972\n",
      "The episode 171 total rewards is 19.55520253836869\n",
      "The episode 181 total rewards is -94.31635274435446\n",
      "The episode 191 total rewards is -108.42877113796897\n",
      "The episode 201 total rewards is -61.63190925810386\n",
      "The episode 211 total rewards is -69.75112480913837\n",
      "The episode 221 total rewards is -131.7295385798853\n",
      "The episode 231 total rewards is -53.72353289527513\n",
      "The episode 241 total rewards is -90.95539324355438\n",
      "The episode 251 total rewards is -68.29346893067721\n",
      "The episode 261 total rewards is -95.11028887558017\n",
      "The episode 271 total rewards is -65.4885394749677\n",
      "The episode 281 total rewards is -99.20670327440048\n",
      "The episode 291 total rewards is -33.77793640815047\n",
      "The episode 301 total rewards is 4.8535244454245685\n",
      "The episode 311 total rewards is -66.4370752153622\n",
      "The episode 321 total rewards is -70.86732943487371\n",
      "The episode 331 total rewards is -78.90040993761596\n",
      "The episode 341 total rewards is -100.54948007808366\n",
      "The episode 351 total rewards is -61.48356698589066\n",
      "The episode 361 total rewards is -0.35763499906320817\n",
      "The episode 371 total rewards is -106.3528019460196\n",
      "The episode 381 total rewards is -65.65011369030782\n",
      "The episode 391 total rewards is -37.554811358570966\n",
      "The episode 401 total rewards is -38.41585241379788\n",
      "The episode 411 total rewards is -54.572492765188635\n",
      "The episode 421 total rewards is -57.763349939960335\n",
      "The episode 431 total rewards is -5.516746070712486\n",
      "The episode 441 total rewards is -36.40566914893226\n",
      "The episode 451 total rewards is 5.810649889138801\n",
      "The episode 461 total rewards is -11.718026630214496\n",
      "The episode 471 total rewards is -16.061623011846052\n",
      "The episode 481 total rewards is 15.263271531066806\n",
      "The episode 491 total rewards is -74.07511872315119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 13:28:10,767]\u001b[0m Trial 10 finished with value: -9.458961583530261 and parameters: {'DQL_nodes 1': 72, 'nodes_2': 34, 'dicount rate': 0.998769905229372, 'lr': 0.0008651451558213713, 'batch size': 44, 'decay': 0.9986334951331197}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -9.458961583530261\n",
      "The episode 1 total rewards is -234.0315982143153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -88.08304435869529\n",
      "The episode 21 total rewards is -169.34996026671323\n",
      "The episode 31 total rewards is -263.15249752556775\n",
      "The episode 41 total rewards is -282.16647390174705\n",
      "The episode 51 total rewards is -75.63985370398383\n",
      "The episode 61 total rewards is -338.40010676189706\n",
      "The episode 71 total rewards is -83.75124857804545\n",
      "The episode 81 total rewards is -53.08040038148262\n",
      "The episode 91 total rewards is -224.44416201649886\n",
      "The episode 101 total rewards is -122.61869176321379\n",
      "The episode 111 total rewards is -64.31445142016973\n",
      "The episode 121 total rewards is -268.0780823405721\n",
      "The episode 131 total rewards is -138.645821643409\n",
      "The episode 141 total rewards is -50.32332910820132\n",
      "The episode 151 total rewards is -23.71394853622826\n",
      "The episode 161 total rewards is -180.482270233617\n",
      "The episode 171 total rewards is -263.9005842402163\n",
      "The episode 181 total rewards is 155.19302767854208\n",
      "The episode 191 total rewards is -174.41623962324113\n",
      "The episode 201 total rewards is -35.00435186169601\n",
      "The episode 211 total rewards is 128.3279646850705\n",
      "The episode 221 total rewards is 235.22402999302685\n",
      "The episode 231 total rewards is 89.19892402581065\n",
      "The episode 241 total rewards is 21.567587730529723\n",
      "The episode 251 total rewards is 103.15833990503607\n",
      "The episode 261 total rewards is 83.32304187050826\n",
      "The episode 271 total rewards is -2.2995465226087504\n",
      "The episode 281 total rewards is 174.07084910414108\n",
      "The episode 291 total rewards is 74.72349887413095\n",
      "The episode 301 total rewards is 76.6561461670165\n",
      "The episode 311 total rewards is 127.94736963637558\n",
      "The episode 321 total rewards is 93.437424480201\n",
      "The episode 331 total rewards is -187.91014102536343\n",
      "The episode 341 total rewards is -159.45083553434588\n",
      "The episode 351 total rewards is 188.51675374653777\n",
      "The episode 361 total rewards is -36.03057897223984\n",
      "The episode 371 total rewards is -87.08672481011232\n",
      "The episode 381 total rewards is 22.73362255870694\n",
      "The episode 391 total rewards is -46.272887326483584\n",
      "The episode 401 total rewards is -56.994758079375316\n",
      "The episode 411 total rewards is -24.590690178246266\n",
      "The episode 421 total rewards is 93.98465115527719\n",
      "The episode 431 total rewards is -55.820005556254884\n",
      "The episode 441 total rewards is -115.44873841628416\n",
      "The episode 451 total rewards is -86.53320868567482\n",
      "The episode 461 total rewards is -29.315361033646496\n",
      "The episode 471 total rewards is 211.94180421055273\n",
      "The episode 481 total rewards is -15.209747474076732\n",
      "The episode 491 total rewards is 8.145399906926624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 13:47:24,338]\u001b[0m Trial 11 finished with value: -57.24595176890007 and parameters: {'DQL_nodes 1': 30, 'nodes_2': 97, 'dicount rate': 0.9756211585641507, 'lr': 0.0005010403470521505, 'batch size': 61, 'decay': 0.9944076574849041}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -57.24595176890007\n",
      "The episode 1 total rewards is -286.5278809253276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -114.17908575235953\n",
      "The episode 21 total rewards is -316.27889676165967\n",
      "The episode 31 total rewards is -252.54056248028485\n",
      "The episode 41 total rewards is -334.5465341404499\n",
      "The episode 51 total rewards is -254.80707728736448\n",
      "The episode 61 total rewards is -105.20259784127036\n",
      "The episode 71 total rewards is -94.44497761267236\n",
      "The episode 81 total rewards is -316.4549859066582\n",
      "The episode 91 total rewards is -149.45797910669702\n",
      "The episode 101 total rewards is -71.4153882783324\n",
      "The episode 111 total rewards is -128.16135863819562\n",
      "The episode 121 total rewards is -146.3969841304416\n",
      "The episode 131 total rewards is -41.33464215390199\n",
      "The episode 141 total rewards is -61.91122217823804\n",
      "The episode 151 total rewards is -81.11767562752681\n",
      "The episode 161 total rewards is -103.62507735188353\n",
      "The episode 171 total rewards is -62.18846418385023\n",
      "The episode 181 total rewards is -109.83444054731608\n",
      "The episode 191 total rewards is 4.0481898411820225\n",
      "The episode 201 total rewards is -55.0117478680687\n",
      "The episode 211 total rewards is -27.937662886603775\n",
      "The episode 221 total rewards is -1.4598144117729674\n",
      "The episode 231 total rewards is -19.671201132254737\n",
      "The episode 241 total rewards is -61.4916459244306\n",
      "The episode 251 total rewards is 84.2164710748371\n",
      "The episode 261 total rewards is -118.25887018250737\n",
      "The episode 271 total rewards is 107.0722795675276\n",
      "The episode 281 total rewards is 29.581550682108713\n",
      "The episode 291 total rewards is 118.92850074300618\n",
      "The episode 301 total rewards is 5.46172143854332\n",
      "The episode 311 total rewards is 127.17407517342787\n",
      "The episode 321 total rewards is -53.72727076360563\n",
      "The episode 331 total rewards is 164.9472698625091\n",
      "The episode 341 total rewards is -140.29468340998267\n",
      "The episode 351 total rewards is -29.443961626219846\n",
      "The episode 361 total rewards is 68.39987346969515\n",
      "The episode 371 total rewards is -108.21484344279627\n",
      "The episode 381 total rewards is 168.02872889557088\n",
      "The episode 391 total rewards is -23.636326728289234\n",
      "The episode 401 total rewards is 20.16251497747376\n",
      "The episode 411 total rewards is -131.26204323163535\n",
      "The episode 421 total rewards is -165.40897007467208\n",
      "The episode 431 total rewards is -12.120214985009474\n",
      "The episode 441 total rewards is -138.13397509497062\n",
      "The episode 451 total rewards is -128.93592235058244\n",
      "The episode 461 total rewards is 145.34295375661452\n",
      "The episode 471 total rewards is 177.3418426088772\n",
      "The episode 481 total rewards is 266.33016832040374\n",
      "The episode 491 total rewards is -80.74540602991344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 14:02:46,775]\u001b[0m Trial 12 finished with value: -6.212440040339124 and parameters: {'DQL_nodes 1': 46, 'nodes_2': 44, 'dicount rate': 0.9705169744524724, 'lr': 0.0007798173585676624, 'batch size': 46, 'decay': 0.9960030950847079}. Best is trial 7 with value: 74.84767285040733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -6.212440040339124\n",
      "The episode 1 total rewards is -105.77473165378544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -151.85307232322538\n",
      "The episode 21 total rewards is -181.64752517317916\n",
      "The episode 31 total rewards is -21.548416210019965\n",
      "The episode 41 total rewards is -96.92167134653958\n",
      "The episode 51 total rewards is -271.41149990360555\n",
      "The episode 61 total rewards is -35.99571199009408\n",
      "The episode 71 total rewards is -61.272736597124\n",
      "The episode 81 total rewards is 13.704668138622694\n",
      "The episode 91 total rewards is -69.51501871600517\n",
      "The episode 101 total rewards is -14.184032264502221\n",
      "The episode 111 total rewards is 19.098559576692253\n",
      "The episode 121 total rewards is 17.22411896261835\n",
      "The episode 131 total rewards is -58.948525239337\n",
      "The episode 141 total rewards is -76.94949581112107\n",
      "The episode 151 total rewards is 24.632299340559186\n",
      "The episode 161 total rewards is 84.8860451069468\n",
      "The episode 171 total rewards is 89.25771004096991\n",
      "The episode 181 total rewards is 136.57227592546138\n",
      "The episode 191 total rewards is 59.256526031647425\n",
      "The episode 201 total rewards is 214.12325186157022\n",
      "The episode 211 total rewards is 103.02567035698048\n",
      "The episode 221 total rewards is 106.7871241325112\n",
      "The episode 231 total rewards is 117.05608953118958\n",
      "The episode 241 total rewards is 70.05917124904016\n",
      "The episode 251 total rewards is -4.173167462117249\n",
      "The episode 261 total rewards is 144.14465811775437\n",
      "The episode 271 total rewards is 205.11208667978394\n",
      "The episode 281 total rewards is 217.6170171510968\n",
      "The episode 291 total rewards is 179.95107689468142\n",
      "The episode 301 total rewards is 220.52145842666442\n",
      "The episode 311 total rewards is 0.3976560308265533\n",
      "The episode 321 total rewards is 293.4369498935697\n",
      "The episode 331 total rewards is -54.155196769340876\n",
      "The episode 341 total rewards is 20.952476106615116\n",
      "The episode 351 total rewards is 234.94648111922257\n",
      "The episode 361 total rewards is 231.89635532636507\n",
      "The episode 371 total rewards is -14.227990649663269\n",
      "The episode 381 total rewards is 91.97368488538007\n",
      "The episode 391 total rewards is 253.99038857075374\n",
      "The episode 401 total rewards is 274.81185443000396\n",
      "The episode 411 total rewards is 300.5169624204576\n",
      "The episode 421 total rewards is 185.69156375526472\n",
      "The episode 431 total rewards is 267.4898280023457\n",
      "The episode 441 total rewards is 242.84269067497698\n",
      "The episode 451 total rewards is 201.31192575795376\n",
      "The episode 461 total rewards is 256.5778745366414\n",
      "The episode 471 total rewards is 261.2387800396747\n",
      "The episode 481 total rewards is 145.57477498867428\n",
      "The episode 491 total rewards is 253.03116614169477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 14:17:21,685]\u001b[0m Trial 13 finished with value: 118.23942056704516 and parameters: {'DQL_nodes 1': 44, 'nodes_2': 46, 'dicount rate': 0.9871921207737274, 'lr': 0.0009908902980792207, 'batch size': 63, 'decay': 0.9938790348937536}. Best is trial 13 with value: 118.23942056704516.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  118.23942056704516\n",
      "The episode 1 total rewards is -122.48223266123469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -295.3014539138801\n",
      "The episode 21 total rewards is -308.18935520360606\n",
      "The episode 31 total rewards is -141.87028794565103\n",
      "The episode 41 total rewards is -407.4153377057698\n",
      "The episode 51 total rewards is -97.88064840254876\n",
      "The episode 61 total rewards is -38.484702766897\n",
      "The episode 71 total rewards is -147.7703239659225\n",
      "The episode 81 total rewards is -156.66706050966857\n",
      "The episode 91 total rewards is -111.43710717411454\n",
      "The episode 101 total rewards is -122.70403009222795\n",
      "The episode 111 total rewards is -265.5988577592278\n",
      "The episode 121 total rewards is -50.5200742405419\n",
      "The episode 131 total rewards is -46.28999874262401\n",
      "The episode 141 total rewards is -91.73616716524137\n",
      "The episode 151 total rewards is -268.054408141241\n",
      "The episode 161 total rewards is 93.97740526266527\n",
      "The episode 171 total rewards is -148.17003199128197\n",
      "The episode 181 total rewards is -13.537783276338875\n",
      "The episode 191 total rewards is -51.319427734442925\n",
      "The episode 201 total rewards is 83.74738013587516\n",
      "The episode 211 total rewards is -36.41741523190117\n",
      "The episode 221 total rewards is 42.85432055457143\n",
      "The episode 231 total rewards is 11.97235691782156\n",
      "The episode 241 total rewards is 154.46893492222844\n",
      "The episode 251 total rewards is 134.59077332839348\n",
      "The episode 261 total rewards is 84.61630021828601\n",
      "The episode 271 total rewards is 241.5530438257112\n",
      "The episode 281 total rewards is 233.15350545069938\n",
      "The episode 291 total rewards is 240.98771086718767\n",
      "The episode 301 total rewards is 43.21429369884527\n",
      "The episode 311 total rewards is 206.08293827813867\n",
      "The episode 321 total rewards is 248.51533363979416\n",
      "The episode 331 total rewards is -38.608317860085265\n",
      "The episode 341 total rewards is -5.462225834610052\n",
      "The episode 351 total rewards is -169.70224095681039\n",
      "The episode 361 total rewards is 87.18701549138811\n",
      "The episode 371 total rewards is 71.19126802767485\n",
      "The episode 381 total rewards is 15.884704925963248\n",
      "The episode 391 total rewards is 85.24994882489526\n",
      "The episode 401 total rewards is 19.788052550168068\n",
      "The episode 411 total rewards is -17.41851221623224\n",
      "The episode 421 total rewards is -1.8910776189741345\n",
      "The episode 431 total rewards is 204.88588634337222\n",
      "The episode 441 total rewards is 152.157590021818\n",
      "The episode 451 total rewards is 157.0625363142883\n",
      "The episode 461 total rewards is -138.44230441144393\n",
      "The episode 471 total rewards is 251.02135122105628\n",
      "The episode 481 total rewards is -202.4586127697246\n",
      "The episode 491 total rewards is -349.49869739986434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 14:30:58,524]\u001b[0m Trial 14 finished with value: -83.03435652803937 and parameters: {'DQL_nodes 1': 60, 'nodes_2': 45, 'dicount rate': 0.999198716743539, 'lr': 0.000987596033582364, 'batch size': 49, 'decay': 0.994351232606529}. Best is trial 13 with value: 118.23942056704516.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -83.03435652803937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -78.14050792753551\n",
      "The episode 11 total rewards is -55.73267796508226\n",
      "The episode 21 total rewards is -157.8959140021062\n",
      "The episode 31 total rewards is -208.72471955838478\n",
      "The episode 41 total rewards is -380.2676665897751\n",
      "The episode 51 total rewards is -213.24132347342152\n",
      "The episode 61 total rewards is -153.22625428696196\n",
      "The episode 71 total rewards is -172.20205114639225\n",
      "The episode 81 total rewards is -152.17917947621058\n",
      "The episode 91 total rewards is -197.6708223379281\n",
      "The episode 101 total rewards is -161.2120533465535\n",
      "The episode 111 total rewards is -211.13160527126206\n",
      "The episode 121 total rewards is -97.96384178294618\n",
      "The episode 131 total rewards is -77.4209182222339\n",
      "The episode 141 total rewards is -103.15806465712798\n",
      "The episode 151 total rewards is -44.58637795258667\n",
      "The episode 161 total rewards is 9.838042416622955\n",
      "The episode 171 total rewards is -176.96499846074795\n",
      "The episode 181 total rewards is -12.060945870831219\n",
      "The episode 191 total rewards is -8.109715138525061\n",
      "The episode 201 total rewards is -131.9579507936037\n",
      "The episode 211 total rewards is 7.392177842357242\n",
      "The episode 221 total rewards is -39.30414163890429\n",
      "The episode 231 total rewards is -73.42697375926217\n",
      "The episode 241 total rewards is -71.10162261189079\n",
      "The episode 251 total rewards is 40.33006861575949\n",
      "The episode 261 total rewards is -34.12207701391965\n",
      "The episode 271 total rewards is 17.40492683000376\n",
      "The episode 281 total rewards is -64.33669880036423\n",
      "The episode 291 total rewards is 55.66191211910177\n",
      "The episode 301 total rewards is 33.520751229401355\n",
      "The episode 311 total rewards is -17.723928920298277\n",
      "The episode 321 total rewards is 45.654265134381376\n",
      "The episode 331 total rewards is 17.928886662960565\n",
      "The episode 341 total rewards is 56.2275626181305\n",
      "The episode 351 total rewards is -23.19768798674085\n",
      "The episode 361 total rewards is -25.141654602600475\n",
      "The episode 371 total rewards is -37.008659962132796\n",
      "The episode 381 total rewards is 225.45200163558593\n",
      "The episode 391 total rewards is -587.7193911696921\n",
      "The episode 401 total rewards is -114.39033654956816\n",
      "The episode 411 total rewards is -43.98213198143122\n",
      "The episode 421 total rewards is -16.74367810582939\n",
      "The episode 431 total rewards is -27.843670307323375\n",
      "The episode 441 total rewards is -65.16121630758086\n",
      "The episode 451 total rewards is -49.24309768406218\n",
      "The episode 461 total rewards is 29.201867505328018\n",
      "The episode 471 total rewards is 40.41948590575964\n",
      "The episode 481 total rewards is -54.92788265115385\n",
      "The episode 491 total rewards is 62.37042121106403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 14:46:34,086]\u001b[0m Trial 15 finished with value: -42.28329823410657 and parameters: {'DQL_nodes 1': 46, 'nodes_2': 30, 'dicount rate': 0.9612631510145881, 'lr': 0.0007937626830291313, 'batch size': 42, 'decay': 0.996419613879919}. Best is trial 13 with value: 118.23942056704516.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -42.28329823410657\n",
      "The episode 1 total rewards is -266.32305886740585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -324.1773199763536\n",
      "The episode 21 total rewards is -251.47479283344455\n",
      "The episode 31 total rewards is -268.6826199682899\n",
      "The episode 41 total rewards is -307.2545734199341\n",
      "The episode 51 total rewards is -116.40722055904364\n",
      "The episode 61 total rewards is -121.78749974017435\n",
      "The episode 71 total rewards is -300.479830651768\n",
      "The episode 81 total rewards is -131.12092932239642\n",
      "The episode 91 total rewards is -131.4711424224689\n",
      "The episode 101 total rewards is -169.53881760431727\n",
      "The episode 111 total rewards is -235.78660790363762\n",
      "The episode 121 total rewards is -63.666560337171425\n",
      "The episode 131 total rewards is 75.35580311579693\n",
      "The episode 141 total rewards is 98.57944109035255\n",
      "The episode 151 total rewards is 106.9818727167816\n",
      "The episode 161 total rewards is 137.5552136264336\n",
      "The episode 171 total rewards is 198.35003601202757\n",
      "The episode 181 total rewards is 258.6500391792373\n",
      "The episode 191 total rewards is 54.894380817249804\n",
      "The episode 201 total rewards is 178.24586516048333\n",
      "The episode 211 total rewards is 152.33112276171698\n",
      "The episode 221 total rewards is 271.5158721669754\n",
      "The episode 231 total rewards is -89.42529058234027\n",
      "The episode 241 total rewards is 207.57827105795985\n",
      "The episode 251 total rewards is -26.688506068413346\n",
      "The episode 261 total rewards is -98.84539208086483\n",
      "The episode 271 total rewards is 30.107247824132926\n",
      "The episode 281 total rewards is 239.87243174129074\n",
      "The episode 291 total rewards is -102.2642755355689\n",
      "The episode 301 total rewards is 224.50595559787928\n",
      "The episode 311 total rewards is 280.0400357354483\n",
      "The episode 321 total rewards is 225.45310463410482\n",
      "The episode 331 total rewards is 267.51499100726153\n",
      "The episode 341 total rewards is 200.44204464303877\n",
      "The episode 351 total rewards is 210.4526801745143\n",
      "The episode 361 total rewards is -54.74439533470133\n",
      "The episode 371 total rewards is 251.96497854709986\n",
      "The episode 381 total rewards is 198.46263063232172\n",
      "The episode 391 total rewards is 199.33589234087236\n",
      "The episode 401 total rewards is 234.6404949442393\n",
      "The episode 411 total rewards is -1.069588607893343\n",
      "The episode 421 total rewards is 222.67776286913377\n",
      "The episode 431 total rewards is -152.77345246505973\n",
      "The episode 441 total rewards is 248.27855603684705\n",
      "The episode 451 total rewards is 185.28437421160461\n",
      "The episode 461 total rewards is 223.684443751009\n",
      "The episode 471 total rewards is 54.620895809536364\n",
      "The episode 481 total rewards is 241.97400122225224\n",
      "The episode 491 total rewards is 257.9735524137203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 15:01:45,595]\u001b[0m Trial 16 finished with value: 199.03976563522082 and parameters: {'DQL_nodes 1': 98, 'nodes_2': 43, 'dicount rate': 0.9875408453479217, 'lr': 0.0006231960028170954, 'batch size': 55, 'decay': 0.990673991803187}. Best is trial 16 with value: 199.03976563522082.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  199.03976563522082\n",
      "The episode 1 total rewards is -164.09752483722048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -78.64329792189277\n",
      "The episode 21 total rewards is -150.42771576043606\n",
      "The episode 31 total rewards is -342.57099966153123\n",
      "The episode 41 total rewards is -421.12243725971973\n",
      "The episode 51 total rewards is -211.9463053714572\n",
      "The episode 61 total rewards is -106.04995928161378\n",
      "The episode 71 total rewards is -107.8762703574125\n",
      "The episode 81 total rewards is -72.87218063860959\n",
      "The episode 91 total rewards is -159.1276957647171\n",
      "The episode 101 total rewards is -112.81040567002171\n",
      "The episode 111 total rewards is -10.267752436592204\n",
      "The episode 121 total rewards is -44.17616280045748\n",
      "The episode 131 total rewards is -28.471846523255287\n",
      "The episode 141 total rewards is 237.74031255523036\n",
      "The episode 151 total rewards is 28.609942015668693\n",
      "The episode 161 total rewards is -80.86918617503306\n",
      "The episode 171 total rewards is 166.99069573984013\n",
      "The episode 181 total rewards is 173.567214055019\n",
      "The episode 191 total rewards is -72.90907778873071\n",
      "The episode 201 total rewards is 55.5324559650849\n",
      "The episode 211 total rewards is 157.80248834202018\n",
      "The episode 221 total rewards is -47.83638338805682\n",
      "The episode 231 total rewards is 200.02828480157953\n",
      "The episode 241 total rewards is 210.00988204306864\n",
      "The episode 251 total rewards is 136.13502211594266\n",
      "The episode 261 total rewards is -243.29333414943065\n",
      "The episode 271 total rewards is -11.383605912214009\n",
      "The episode 281 total rewards is 39.03155094930841\n",
      "The episode 291 total rewards is 58.72164404312457\n",
      "The episode 301 total rewards is -137.83574486470502\n",
      "The episode 311 total rewards is -85.06442074416907\n",
      "The episode 321 total rewards is 254.94136230777434\n",
      "The episode 331 total rewards is 15.58942060717365\n",
      "The episode 341 total rewards is 265.89130364678755\n",
      "The episode 351 total rewards is 160.06104281947827\n",
      "The episode 361 total rewards is -7.727466644030665\n",
      "The episode 371 total rewards is 2.3361432731972513\n",
      "The episode 381 total rewards is 233.27009987461958\n",
      "The episode 391 total rewards is -73.23545850908144\n",
      "The episode 401 total rewards is 196.4970506502697\n",
      "The episode 411 total rewards is 206.14690650112863\n",
      "The episode 421 total rewards is 108.65370515836995\n",
      "The episode 431 total rewards is 273.97043412211724\n",
      "The episode 441 total rewards is 251.29370902512048\n",
      "The episode 451 total rewards is 262.83826021202077\n",
      "The episode 461 total rewards is 254.95783076399633\n",
      "The episode 471 total rewards is 255.7341485860684\n",
      "The episode 481 total rewards is 247.28020408775765\n",
      "The episode 491 total rewards is 203.64785725375947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 15:14:54,792]\u001b[0m Trial 17 finished with value: 155.65988763357427 and parameters: {'DQL_nodes 1': 99, 'nodes_2': 40, 'dicount rate': 0.987185007452436, 'lr': 0.0006119990399420428, 'batch size': 57, 'decay': 0.9900519988585975}. Best is trial 16 with value: 199.03976563522082.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  155.65988763357427\n",
      "The episode 1 total rewards is -146.64713830420004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -92.10117542041833\n",
      "The episode 21 total rewards is -106.66276690708473\n",
      "The episode 31 total rewards is -153.63306360422848\n",
      "The episode 41 total rewards is -168.8574710328158\n",
      "The episode 51 total rewards is -117.74542382924189\n",
      "The episode 61 total rewards is -208.89530855600447\n",
      "The episode 71 total rewards is -364.02453174788593\n",
      "The episode 81 total rewards is -291.26882526004005\n",
      "The episode 91 total rewards is -219.36855535237194\n",
      "The episode 101 total rewards is -115.16720559623212\n",
      "The episode 111 total rewards is -155.7947340621335\n",
      "The episode 121 total rewards is -195.8998742560126\n",
      "The episode 131 total rewards is 112.31588498036584\n",
      "The episode 141 total rewards is -202.3244958412787\n",
      "The episode 151 total rewards is 275.2144976478055\n",
      "The episode 161 total rewards is 49.820900175774085\n",
      "The episode 171 total rewards is -220.47933603142053\n",
      "The episode 181 total rewards is 26.926523505450973\n",
      "The episode 191 total rewards is -77.06403318784827\n",
      "The episode 201 total rewards is -59.90073199548459\n",
      "The episode 211 total rewards is -94.28804843620094\n",
      "The episode 221 total rewards is -156.18799480337734\n",
      "The episode 231 total rewards is -69.7547005382887\n",
      "The episode 241 total rewards is -38.45550403279702\n",
      "The episode 251 total rewards is -111.10553108459104\n",
      "The episode 261 total rewards is -125.48105621448576\n",
      "The episode 271 total rewards is -74.78323439234288\n",
      "The episode 281 total rewards is -100.26204219920342\n",
      "The episode 291 total rewards is -106.79354394391481\n",
      "The episode 301 total rewards is -143.50175435367774\n",
      "The episode 311 total rewards is -128.66938204081933\n",
      "The episode 321 total rewards is -78.94323074930416\n",
      "The episode 331 total rewards is -105.28641284223694\n",
      "The episode 341 total rewards is -103.11781358801981\n",
      "The episode 351 total rewards is -115.2532181446683\n",
      "The episode 361 total rewards is -71.51455748411867\n",
      "The episode 371 total rewards is -140.9598339325072\n",
      "The episode 381 total rewards is -76.691744862367\n",
      "The episode 391 total rewards is -117.34286498450989\n",
      "The episode 401 total rewards is -115.68790369962325\n",
      "The episode 411 total rewards is -103.15858412128532\n",
      "The episode 421 total rewards is -107.04744497305873\n",
      "The episode 431 total rewards is -75.14804107434749\n",
      "The episode 441 total rewards is -77.39392849162337\n",
      "The episode 451 total rewards is -104.08548168174457\n",
      "The episode 461 total rewards is -138.22473030990585\n",
      "The episode 471 total rewards is -131.96731011243327\n",
      "The episode 481 total rewards is -123.96956840494434\n",
      "The episode 491 total rewards is -139.06612972817297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 15:41:35,419]\u001b[0m Trial 18 finished with value: -98.69655274861952 and parameters: {'DQL_nodes 1': 100, 'nodes_2': 36, 'dicount rate': 0.9008651961567202, 'lr': 0.0006085451535093734, 'batch size': 56, 'decay': 0.9900316200782556}. Best is trial 16 with value: 199.03976563522082.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -98.69655274861952\n",
      "The episode 1 total rewards is -68.70081455053102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -95.43815604335605\n",
      "The episode 21 total rewards is 9.085586484909484\n",
      "The episode 31 total rewards is -162.84445554895842\n",
      "The episode 41 total rewards is -256.86319786526997\n",
      "The episode 51 total rewards is -202.37965899194387\n",
      "The episode 61 total rewards is -143.8057570658659\n",
      "The episode 71 total rewards is 1.4931176220578095\n",
      "The episode 81 total rewards is -41.55919275249357\n",
      "The episode 91 total rewards is -74.51991964079411\n",
      "The episode 101 total rewards is -26.742250843896556\n",
      "The episode 111 total rewards is 12.61125178926325\n",
      "The episode 121 total rewards is -106.70284340799479\n",
      "The episode 131 total rewards is -169.75919510890822\n",
      "The episode 141 total rewards is -148.30307348309026\n",
      "The episode 151 total rewards is -75.94491359100516\n",
      "The episode 161 total rewards is -78.15043495361624\n",
      "The episode 171 total rewards is -125.43188358444854\n",
      "The episode 181 total rewards is 11.139973254676761\n",
      "The episode 191 total rewards is -18.030518223562105\n",
      "The episode 201 total rewards is 15.042262396637508\n",
      "The episode 211 total rewards is 181.11772988211908\n",
      "The episode 221 total rewards is 193.26185869014313\n",
      "The episode 231 total rewards is 142.61965698757993\n",
      "The episode 241 total rewards is 28.706692442660703\n",
      "The episode 251 total rewards is 193.02488953525835\n",
      "The episode 261 total rewards is 217.16649249308517\n",
      "The episode 271 total rewards is 2.9381887113605245\n",
      "The episode 281 total rewards is 294.72290188035475\n",
      "The episode 291 total rewards is -14.694533892524973\n",
      "The episode 301 total rewards is -31.35416406569617\n",
      "The episode 311 total rewards is 178.0304325513235\n",
      "The episode 321 total rewards is 230.76844097510363\n",
      "The episode 331 total rewards is 213.66855137316745\n",
      "The episode 341 total rewards is 22.36901820469157\n",
      "The episode 351 total rewards is -9.939898169662214\n",
      "The episode 361 total rewards is 3.324690253916046\n",
      "The episode 371 total rewards is -108.58132315946605\n",
      "The episode 381 total rewards is 273.3371851740171\n",
      "The episode 391 total rewards is -11.865365178876615\n",
      "The episode 401 total rewards is -0.9543198612809647\n",
      "The episode 411 total rewards is 162.07647920023896\n",
      "The episode 421 total rewards is -83.22797626762592\n",
      "The episode 431 total rewards is -101.86328114822153\n",
      "The episode 441 total rewards is -0.8326561126617436\n",
      "The episode 451 total rewards is -70.15683135099123\n",
      "The episode 461 total rewards is -57.52996673073078\n",
      "The episode 471 total rewards is 219.37663013701888\n",
      "The episode 481 total rewards is 215.52650737988245\n",
      "The episode 491 total rewards is 260.5365972025364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 15:58:26,481]\u001b[0m Trial 19 finished with value: 10.125324702597666 and parameters: {'DQL_nodes 1': 100, 'nodes_2': 40, 'dicount rate': 0.9857454278935129, 'lr': 0.0006230254580196351, 'batch size': 56, 'decay': 0.991233527452768}. Best is trial 16 with value: 199.03976563522082.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  10.125324702597666\n",
      "The episode 1 total rewards is -113.01965432047888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -342.07096461522497\n",
      "The episode 21 total rewards is -272.29791109929397\n",
      "The episode 31 total rewards is -83.2856261400411\n",
      "The episode 41 total rewards is -4.21469925631898\n",
      "The episode 51 total rewards is -276.3626584932763\n",
      "The episode 61 total rewards is -201.02420432653764\n",
      "The episode 71 total rewards is -6.562618060849333\n",
      "The episode 81 total rewards is -29.737230209399442\n",
      "The episode 91 total rewards is -254.88070191135475\n",
      "The episode 101 total rewards is -471.7494285759431\n",
      "The episode 111 total rewards is -54.67451934923406\n",
      "The episode 121 total rewards is 49.69716611380933\n",
      "The episode 131 total rewards is 57.54310388174634\n",
      "The episode 141 total rewards is 70.50152308015292\n",
      "The episode 151 total rewards is -22.138970614055705\n",
      "The episode 161 total rewards is 71.0559766206384\n",
      "The episode 171 total rewards is -36.27012031652485\n",
      "The episode 181 total rewards is -7.324856691074373\n",
      "The episode 191 total rewards is -38.314006754022465\n",
      "The episode 201 total rewards is -41.533020009776386\n",
      "The episode 211 total rewards is -94.29909753399622\n",
      "The episode 221 total rewards is -44.59871071258941\n",
      "The episode 231 total rewards is -70.89052218722712\n",
      "The episode 241 total rewards is -78.14520999973902\n",
      "The episode 251 total rewards is -118.83329996109723\n",
      "The episode 261 total rewards is -147.91957625139423\n",
      "The episode 271 total rewards is -102.38997794306233\n",
      "The episode 281 total rewards is -114.05744164862747\n",
      "The episode 291 total rewards is -90.10576871170659\n",
      "The episode 301 total rewards is -100.19496712488225\n",
      "The episode 311 total rewards is -136.40800093950196\n",
      "The episode 321 total rewards is -117.62933301573392\n",
      "The episode 331 total rewards is -60.08398047298567\n",
      "The episode 341 total rewards is -101.05217049406833\n",
      "The episode 351 total rewards is -47.60332306765918\n",
      "The episode 361 total rewards is -93.61206244326112\n",
      "The episode 371 total rewards is -42.49904936988639\n",
      "The episode 381 total rewards is 13.20243172006118\n",
      "The episode 391 total rewards is -123.14274922597674\n",
      "The episode 401 total rewards is -62.63262886925513\n",
      "The episode 411 total rewards is -144.08010238144973\n",
      "The episode 421 total rewards is -121.5319332617984\n",
      "The episode 431 total rewards is -12.143687784520484\n",
      "The episode 441 total rewards is -71.50966457797414\n",
      "The episode 451 total rewards is -72.66142763548697\n",
      "The episode 461 total rewards is -143.47459795466074\n",
      "The episode 471 total rewards is -85.17931755674803\n",
      "The episode 481 total rewards is -67.74990016596567\n",
      "The episode 491 total rewards is -70.05603913404735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 16:25:00,054]\u001b[0m Trial 20 finished with value: -66.178653107797 and parameters: {'DQL_nodes 1': 90, 'nodes_2': 52, 'dicount rate': 0.9559506005021265, 'lr': 0.0005627408451022686, 'batch size': 54, 'decay': 0.9911078779917469}. Best is trial 16 with value: 199.03976563522082.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -66.178653107797\n",
      "The episode 1 total rewards is -96.19089961382359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -82.89004102703046\n",
      "The episode 21 total rewards is -233.28843229434634\n",
      "The episode 31 total rewards is -25.919355638569385\n",
      "The episode 41 total rewards is -53.42481107834473\n",
      "The episode 51 total rewards is -126.07832664363316\n",
      "The episode 61 total rewards is -200.2987073904967\n",
      "The episode 71 total rewards is -46.50027365261162\n",
      "The episode 81 total rewards is -183.09700239143393\n",
      "The episode 91 total rewards is -103.17532568327596\n",
      "The episode 101 total rewards is 6.076015016715061\n",
      "The episode 111 total rewards is -87.67130284244992\n",
      "The episode 121 total rewards is 53.36407068476517\n",
      "The episode 131 total rewards is -18.80948120767603\n",
      "The episode 141 total rewards is 33.19398026343686\n",
      "The episode 151 total rewards is 22.784215406481174\n",
      "The episode 161 total rewards is 25.593593345324944\n",
      "The episode 171 total rewards is 20.227797389067515\n",
      "The episode 181 total rewards is -14.229180624221453\n",
      "The episode 191 total rewards is 20.85912652942134\n",
      "The episode 201 total rewards is 59.004968846849906\n",
      "The episode 211 total rewards is 199.07373564494895\n",
      "The episode 221 total rewards is -332.5491217386141\n",
      "The episode 231 total rewards is 21.43946591664431\n",
      "The episode 241 total rewards is 74.4345658893275\n",
      "The episode 251 total rewards is -277.08325073081335\n",
      "The episode 261 total rewards is -6.633262612640472\n",
      "The episode 271 total rewards is 302.1675747988814\n",
      "The episode 281 total rewards is 168.65121874814184\n",
      "The episode 291 total rewards is 217.43805720756285\n",
      "The episode 301 total rewards is 179.06901175985354\n",
      "The episode 311 total rewards is 198.90351560960772\n",
      "The episode 321 total rewards is -29.516513581034303\n",
      "The episode 331 total rewards is 92.17961346803645\n",
      "The episode 341 total rewards is 229.37238063182997\n",
      "The episode 351 total rewards is 249.01312050756962\n",
      "The episode 361 total rewards is 233.00080346172965\n",
      "The episode 371 total rewards is 246.62005516733984\n",
      "The episode 381 total rewards is 24.000723891228745\n",
      "The episode 391 total rewards is 230.43400059383964\n",
      "The episode 401 total rewards is 253.171429920027\n",
      "The episode 411 total rewards is 248.4014481295602\n",
      "The episode 421 total rewards is 238.71553099720543\n",
      "The episode 431 total rewards is 231.1933390998822\n",
      "The episode 441 total rewards is 217.2822877804146\n",
      "The episode 451 total rewards is 254.22802988335783\n",
      "The episode 461 total rewards is 211.23400498749538\n",
      "The episode 471 total rewards is 266.70882598680305\n",
      "The episode 481 total rewards is 197.67788446742858\n",
      "The episode 491 total rewards is 241.80994076721848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 16:40:54,488]\u001b[0m Trial 21 finished with value: 237.05220143270762 and parameters: {'DQL_nodes 1': 86, 'nodes_2': 47, 'dicount rate': 0.987274166464953, 'lr': 0.0006301375995468469, 'batch size': 64, 'decay': 0.9909761690253862}. Best is trial 21 with value: 237.05220143270762.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  237.05220143270762\n",
      "The episode 1 total rewards is -142.02065635197803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -36.1733091786725\n",
      "The episode 21 total rewards is -447.65973039992275\n",
      "The episode 31 total rewards is -321.7873655840776\n",
      "The episode 41 total rewards is -186.2563412933335\n",
      "The episode 51 total rewards is -329.7569024347948\n",
      "The episode 61 total rewards is -222.9260451129606\n",
      "The episode 71 total rewards is -71.18053016764064\n",
      "The episode 81 total rewards is -54.08287091785104\n",
      "The episode 91 total rewards is -109.09828053045831\n",
      "The episode 101 total rewards is -102.55785883949979\n",
      "The episode 111 total rewards is -46.27254295046115\n",
      "The episode 121 total rewards is -67.18456845035078\n",
      "The episode 131 total rewards is 50.63404930103414\n",
      "The episode 141 total rewards is 114.39086718802602\n",
      "The episode 151 total rewards is -11.62964569979946\n",
      "The episode 161 total rewards is -76.35805489315136\n",
      "The episode 171 total rewards is -35.6410763555108\n",
      "The episode 181 total rewards is -45.3749506451714\n",
      "The episode 191 total rewards is -63.522563253779055\n",
      "The episode 201 total rewards is -33.35070083765987\n",
      "The episode 211 total rewards is 237.25690008437493\n",
      "The episode 221 total rewards is 231.2522293762543\n",
      "The episode 231 total rewards is -54.8785571245896\n",
      "The episode 241 total rewards is -113.94560126884798\n",
      "The episode 251 total rewards is 13.357805007273216\n",
      "The episode 261 total rewards is -56.459372768302444\n",
      "The episode 271 total rewards is 32.636925119182685\n",
      "The episode 281 total rewards is -129.85382450216756\n",
      "The episode 291 total rewards is -35.66127538772632\n",
      "The episode 301 total rewards is 161.87015545002316\n",
      "The episode 311 total rewards is -174.95081552652874\n",
      "The episode 321 total rewards is -50.31560884404651\n",
      "The episode 331 total rewards is 64.73020654739182\n",
      "The episode 341 total rewards is 3.3814877140631365\n",
      "The episode 351 total rewards is 19.252341883874028\n",
      "The episode 361 total rewards is 193.63056361074234\n",
      "The episode 371 total rewards is 149.10791102489304\n",
      "The episode 381 total rewards is 255.80477306147674\n",
      "The episode 391 total rewards is 253.60994036578438\n",
      "The episode 401 total rewards is 261.0425098169217\n",
      "The episode 411 total rewards is 266.21893827360844\n",
      "The episode 421 total rewards is -8.556248125866574\n",
      "The episode 431 total rewards is 45.03194598828864\n",
      "The episode 441 total rewards is -45.912919821411464\n",
      "The episode 451 total rewards is 296.75366312868755\n",
      "The episode 461 total rewards is 274.841121003628\n",
      "The episode 471 total rewards is 185.48146817264086\n",
      "The episode 481 total rewards is 252.0507539079838\n",
      "The episode 491 total rewards is 249.14201997587676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 16:57:38,966]\u001b[0m Trial 22 finished with value: 204.3274661000718 and parameters: {'DQL_nodes 1': 90, 'nodes_2': 39, 'dicount rate': 0.988165539707612, 'lr': 0.0006293421780798123, 'batch size': 59, 'decay': 0.990974556201814}. Best is trial 21 with value: 237.05220143270762.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  204.3274661000718\n",
      "The episode 1 total rewards is -114.7849070437401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -408.01244336796003\n",
      "The episode 21 total rewards is -96.48748399021903\n",
      "The episode 31 total rewards is -224.21263678228158\n",
      "The episode 41 total rewards is -238.51574266263938\n",
      "The episode 51 total rewards is -51.39144962126595\n",
      "The episode 61 total rewards is -134.90381574113113\n",
      "The episode 71 total rewards is 45.90450722750661\n",
      "The episode 81 total rewards is -130.65048819001868\n",
      "The episode 91 total rewards is -17.51139890787259\n",
      "The episode 101 total rewards is -33.66776574098199\n",
      "The episode 111 total rewards is -260.4890898511591\n",
      "The episode 121 total rewards is -46.94246585489089\n",
      "The episode 131 total rewards is -39.166583372899616\n",
      "The episode 141 total rewards is 47.94597012657893\n",
      "The episode 151 total rewards is 67.12958877507093\n",
      "The episode 161 total rewards is 93.18201271952262\n",
      "The episode 171 total rewards is -31.71795421802936\n",
      "The episode 181 total rewards is -59.48590243735738\n",
      "The episode 191 total rewards is -45.92776785170896\n",
      "The episode 201 total rewards is -13.447323065384833\n",
      "The episode 211 total rewards is 91.9592931105294\n",
      "The episode 221 total rewards is -61.145481143808496\n",
      "The episode 231 total rewards is -29.914938020091622\n",
      "The episode 241 total rewards is -65.94336935568988\n",
      "The episode 251 total rewards is 110.91664364766511\n",
      "The episode 261 total rewards is -90.02944263013109\n",
      "The episode 271 total rewards is -51.74596298536915\n",
      "The episode 281 total rewards is 153.1905346775194\n",
      "The episode 291 total rewards is 126.58724548797879\n",
      "The episode 301 total rewards is 274.411555986088\n",
      "The episode 311 total rewards is 190.57522664523032\n",
      "The episode 321 total rewards is 195.07173608823967\n",
      "The episode 331 total rewards is -144.1880516258489\n",
      "The episode 341 total rewards is 159.24563268918638\n",
      "The episode 351 total rewards is 136.5675073856588\n",
      "The episode 361 total rewards is 260.1531643748551\n",
      "The episode 371 total rewards is 3.8678266044672895\n",
      "The episode 381 total rewards is 282.40651326256136\n",
      "The episode 391 total rewards is 173.26773258901295\n",
      "The episode 401 total rewards is 38.895160050974624\n",
      "The episode 411 total rewards is 16.93696218438525\n",
      "The episode 421 total rewards is 196.00129138267798\n",
      "The episode 431 total rewards is 225.90864162156794\n",
      "The episode 441 total rewards is 239.28365562901476\n",
      "The episode 451 total rewards is 159.3856481760956\n",
      "The episode 461 total rewards is 36.33146463636004\n",
      "The episode 471 total rewards is 225.87233990828474\n",
      "The episode 481 total rewards is -45.99457226153375\n",
      "The episode 491 total rewards is 234.36850440195246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 17:16:33,610]\u001b[0m Trial 23 finished with value: -98.48494955551172 and parameters: {'DQL_nodes 1': 88, 'nodes_2': 49, 'dicount rate': 0.9816104002872322, 'lr': 0.0007385921182408337, 'batch size': 64, 'decay': 0.991278483615693}. Best is trial 21 with value: 237.05220143270762.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -98.48494955551172\n",
      "The episode 1 total rewards is -183.89032108562122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -218.61847086955615\n",
      "The episode 21 total rewards is -185.3370815790176\n",
      "The episode 31 total rewards is -68.79510292624516\n",
      "The episode 41 total rewards is -237.11867949415864\n",
      "The episode 51 total rewards is -98.46806198880404\n",
      "The episode 61 total rewards is -82.82766177433379\n",
      "The episode 71 total rewards is -99.82102939676878\n",
      "The episode 81 total rewards is -117.81215970957783\n",
      "The episode 91 total rewards is -158.2538999281802\n",
      "The episode 101 total rewards is -114.6706000430968\n",
      "The episode 111 total rewards is -102.49111932621419\n",
      "The episode 121 total rewards is -179.29785059737316\n",
      "The episode 131 total rewards is -64.35774168063948\n",
      "The episode 141 total rewards is -212.8027993244143\n",
      "The episode 151 total rewards is 71.23961578581199\n",
      "The episode 161 total rewards is 62.28692386110956\n",
      "The episode 171 total rewards is -156.40508550050188\n",
      "The episode 181 total rewards is 116.73431654295572\n",
      "The episode 191 total rewards is 4.334901655621792\n",
      "The episode 201 total rewards is -7.150309628371528\n",
      "The episode 211 total rewards is 149.24668291741125\n",
      "The episode 221 total rewards is 107.97068108054167\n",
      "The episode 231 total rewards is 144.38324885062548\n",
      "The episode 241 total rewards is -165.9774030945989\n",
      "The episode 251 total rewards is -32.34592981204891\n",
      "The episode 261 total rewards is -37.62911503185696\n",
      "The episode 271 total rewards is -105.21482429089195\n",
      "The episode 281 total rewards is -61.19561237509343\n",
      "The episode 291 total rewards is -35.40697556679523\n",
      "The episode 301 total rewards is -192.35694757990444\n",
      "The episode 311 total rewards is -73.15471823922329\n",
      "The episode 321 total rewards is 7.791448695704659\n",
      "The episode 331 total rewards is -39.54323176204802\n",
      "The episode 341 total rewards is -119.40153803748046\n",
      "The episode 351 total rewards is 0.6093228567775136\n",
      "The episode 361 total rewards is -108.66708849728768\n",
      "The episode 371 total rewards is 84.71104988484296\n",
      "The episode 381 total rewards is -29.98301934515613\n",
      "The episode 391 total rewards is -6.31417703764547\n",
      "The episode 401 total rewards is 5.794125510326377\n",
      "The episode 411 total rewards is -137.20933666215836\n",
      "The episode 421 total rewards is -65.48626644655396\n",
      "The episode 431 total rewards is -25.6179872111256\n",
      "The episode 441 total rewards is -29.02201950664943\n",
      "The episode 451 total rewards is -115.10800136794586\n",
      "The episode 461 total rewards is -147.5018492159858\n",
      "The episode 471 total rewards is -53.159051168747126\n",
      "The episode 481 total rewards is -74.47838057546612\n",
      "The episode 491 total rewards is -143.58785765594635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-13 17:39:10,342]\u001b[0m Trial 24 finished with value: -71.29088692345806 and parameters: {'DQL_nodes 1': 77, 'nodes_2': 31, 'dicount rate': 0.9947268349384646, 'lr': 0.0006287030217073865, 'batch size': 60, 'decay': 0.9921039094104541}. Best is trial 21 with value: 237.05220143270762.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing mean rewards is  -71.29088692345806\n",
      "The episode 1 total rewards is -222.35031793562314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labuser\\miniconda3\\envs\\rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 11 total rewards is -66.4235962809289\n",
      "The episode 21 total rewards is -211.98115940280323\n",
      "The episode 31 total rewards is -521.5981586074945\n",
      "The episode 41 total rewards is -128.4356103318369\n",
      "The episode 51 total rewards is -317.9105449664891\n",
      "The episode 61 total rewards is -153.3844725478342\n",
      "The episode 71 total rewards is -148.940168102953\n",
      "The episode 81 total rewards is -75.05418767409185\n",
      "The episode 91 total rewards is -94.85180408697684\n",
      "The episode 101 total rewards is -103.43280922435864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards_testing)\n\u001b[0;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Create a new study.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl_env\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl_env\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     32\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[0;32m     37\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[1;34m(self, miniBatchSize)\u001b[0m\n\u001b[0;32m     74\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdicount\u001b[38;5;241m*\u001b[39mQ_target\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#make sure the grad is zero \u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#calculate the loss \u001b[39;00m\n\u001b[0;32m     80\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fucntion(Q_target,Q_estimate_a)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl_env\\lib\\site-packages\\torch\\optim\\optimizer.py:222\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    220\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m foreach \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse):\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     per_device_and_dtype_grads[p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdevice][p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype]\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#using optuna\n",
    "\n",
    "\n",
    "def train(trial):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    memory_max_size = 10_000\n",
    "    DQL_node1 = trial.suggest_int('DQL_nodes 1', 30, 100)\n",
    "    DQL_node2 = trial.suggest_int('nodes_2', 30, 100)\n",
    "    dicount = trial.suggest_float('dicount rate', 0.9, 1.0, log=True)\n",
    "    lr_optim = trial.suggest_float('lr', 5e-4, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int('batch size', 32, 64)\n",
    "    decay_rate = trial.suggest_float('decay', 0.99,0.999, log=True)\n",
    "    agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,DQL_node1,DQL_node2,decay_rate)\n",
    "    update_freq = 1000\n",
    "    steps = 0 \n",
    "    \n",
    "    num_episodes = 500\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    moving_average = []\n",
    "    for i in range(num_episodes):\n",
    "        state = agent.env.reset()\n",
    "        Done = False\n",
    "        total_rewards = 0\n",
    "        while not Done: \n",
    "            #take an action using the greedy policy\n",
    "            action = agent.eps_greedy(state)\n",
    "            #implement the action \n",
    "            next_state, reward, Done, info = agent.env.step(action)\n",
    "            #save the experience in the memory of the agent \n",
    "            agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "            #sum the rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            #learn from experience (if there is enough)\n",
    "            agent.learnFromExperience(batch_size)\n",
    "            #update the tarqet network per the desired frequency \n",
    "            steps +=1 \n",
    "            if (steps % update_freq) == 0:\n",
    "                agent.update_target_weights()\n",
    "            state = next_state\n",
    "        #append the rewards\n",
    "        rewards[i] = total_rewards\n",
    "        moving_average.append(np.mean(rewards[-50:]))\n",
    "        #update the eps \n",
    "        agent.decay_eps()\n",
    "        if i %10 == 0:\n",
    "            print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "            #print(len(agent.reply_memory))\n",
    "        \n",
    "    #testing \n",
    "    num_tests = 20\n",
    "    rewards_testing = np.zeros(num_tests)\n",
    "    for i in range(num_tests):\n",
    "        state = env.reset()\n",
    "        Done = False \n",
    "        total_rewards = 0\n",
    "        while not Done: \n",
    "            action = agent.get_max_action(state)\n",
    "            next_state, reward,Done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "        rewards_testing[i]= total_rewards\n",
    "\n",
    "    print(\"The testing mean rewards is \", np.mean(rewards_testing))\n",
    "        \n",
    "    return np.mean(rewards_testing)\n",
    "study = optuna.create_study(direction=\"maximize\")  # Create a new study.\n",
    "study.optimize(train, n_trials=30)  # Invoke optimization of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_agent:\n",
    "    #Agent and environment interaction\n",
    "    def __init__(self, env,memory_max_size =10_000,dicount= 0.957,lr_optim=6.6e-3,update_freq =1000,DQL_node1=48,DQL_node2=79,decay_rate=0.9956):\n",
    "        self.agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,DQL_node1,DQL_node2,decay_rate)\n",
    "        self.update_freq = update_freq\n",
    "        self.steps = 0 \n",
    "    \n",
    "    def train_agent(self, num_episodes,batch_size):\n",
    "        self.rewards = np.zeros(num_episodes)\n",
    "        self.moving_average = []\n",
    "        for i in range(num_episodes):\n",
    "            state = self.agent.env.reset()\n",
    "            Done = False\n",
    "            total_rewards = 0\n",
    "            while not Done: \n",
    "                #take an action using the greedy policy\n",
    "                action = self.agent.eps_greedy(state)\n",
    "                #implement the action \n",
    "                next_state, reward, Done, info = self.agent.env.step(action)\n",
    "                #save the experience in the memory of the agent \n",
    "                self.agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "                #sum the rewards\n",
    "                total_rewards += reward\n",
    "                \n",
    "                #learn from experience (if there is enough)\n",
    "                self.agent.learnFromExperience(batch_size)\n",
    "                #update the tarqet network per the desired frequency \n",
    "                self.steps +=1 \n",
    "                if (self.steps % self.update_freq) == 0:\n",
    "                    self.agent.update_target_weights()\n",
    "                state = next_state\n",
    "            #append the rewards\n",
    "            self.rewards[i] = total_rewards\n",
    "            self.moving_average.append(np.mean(self.rewards[-50:]))\n",
    "            #update the eps \n",
    "            self.agent.decay_eps()\n",
    "            if i %10 == 0:\n",
    "                print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "                print(len(self.agent.reply_memory))\n",
    "    def test_agent(self,env,num_run, render=False):\n",
    "        rewards = np.zeros(num_run)\n",
    "        for i in range(num_run):\n",
    "            state = env.reset()\n",
    "            Done = False \n",
    "            total_rewards = 0\n",
    "\n",
    "            while not Done: \n",
    "                action = agent.get_max_action(state)\n",
    "                next_state, reward,Done, info = env.step(action)\n",
    "                total_rewards += reward\n",
    "                if render:\n",
    "                    env.render()\n",
    "                state = next_state\n",
    "            rewards[i]= total_rewards\n",
    "            print(\"The episode total rewards is \", total_rewards)\n",
    "        env.close()\n",
    "        return rewards\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a2dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -104.76862202251367\n",
      "70\n",
      "The episode 11 total rewards is -161.5043664297409\n",
      "969\n",
      "The episode 21 total rewards is -63.89878976471971\n",
      "1917\n",
      "The episode 31 total rewards is -422.2351310560372\n",
      "2821\n",
      "The episode 41 total rewards is -108.94253449066348\n",
      "3984\n",
      "The episode 51 total rewards is -343.28524699276517\n",
      "5039\n",
      "The episode 61 total rewards is -247.83418765912046\n",
      "6092\n",
      "The episode 71 total rewards is -183.6416698759798\n",
      "7147\n",
      "The episode 81 total rewards is -213.9115354856329\n",
      "8362\n",
      "The episode 91 total rewards is -73.56223305560535\n",
      "10000\n",
      "The episode 101 total rewards is -31.724966269467487\n",
      "10000\n",
      "The episode 111 total rewards is -96.30695703373149\n",
      "10000\n",
      "The episode 121 total rewards is -216.7753973301118\n",
      "10000\n",
      "The episode 131 total rewards is -55.0252370062708\n",
      "10000\n",
      "The episode 141 total rewards is -114.19691335443041\n",
      "10000\n",
      "The episode 151 total rewards is -192.0828198593851\n",
      "10000\n",
      "The episode 161 total rewards is 8.74077258792532\n",
      "10000\n",
      "The episode 171 total rewards is 4.3692146263049665\n",
      "10000\n",
      "The episode 181 total rewards is -59.88697119508632\n",
      "10000\n",
      "The episode 191 total rewards is -109.15042390744884\n",
      "10000\n",
      "The episode 201 total rewards is -67.62460322812915\n",
      "10000\n",
      "The episode 211 total rewards is -29.393205153937963\n",
      "10000\n",
      "The episode 221 total rewards is -18.881470678310976\n",
      "10000\n",
      "The episode 231 total rewards is 26.14120127319459\n",
      "10000\n",
      "The episode 241 total rewards is -48.648576272565535\n",
      "10000\n",
      "The episode 251 total rewards is -5.6268883167310975\n",
      "10000\n",
      "The episode 261 total rewards is 76.44524440682814\n",
      "10000\n",
      "The episode 271 total rewards is 139.930006248851\n",
      "10000\n",
      "The episode 281 total rewards is 27.782718654842917\n",
      "10000\n",
      "The episode 291 total rewards is 169.56593482137947\n",
      "10000\n",
      "The episode 301 total rewards is 121.72380001157248\n",
      "10000\n",
      "The episode 311 total rewards is 39.94309573622317\n",
      "10000\n",
      "The episode 321 total rewards is -192.42534065571607\n",
      "10000\n",
      "The episode 331 total rewards is 234.25594425437927\n",
      "10000\n",
      "The episode 341 total rewards is -182.95079206407053\n",
      "10000\n",
      "The episode 351 total rewards is 132.5668942960888\n",
      "10000\n",
      "The episode 361 total rewards is 112.5451016311882\n",
      "10000\n",
      "The episode 371 total rewards is 85.66023560369771\n",
      "10000\n",
      "The episode 381 total rewards is -63.731345902172066\n",
      "10000\n",
      "The episode 391 total rewards is 141.70518112602366\n",
      "10000\n",
      "The episode 401 total rewards is 192.8387001121746\n",
      "10000\n",
      "The episode 411 total rewards is 2.5567212074641787\n",
      "10000\n",
      "The episode 421 total rewards is 27.459494083355708\n",
      "10000\n",
      "The episode 431 total rewards is 194.39983740464703\n",
      "10000\n",
      "The episode 441 total rewards is 107.86825303039036\n",
      "10000\n",
      "The episode 451 total rewards is -21.432079773844507\n",
      "10000\n",
      "The episode 461 total rewards is 175.36253627459251\n",
      "10000\n",
      "The episode 471 total rewards is 165.368133183091\n",
      "10000\n",
      "The episode 481 total rewards is 63.87207181481863\n",
      "10000\n",
      "The episode 491 total rewards is -20.475150009385537\n",
      "10000\n",
      "The episode 501 total rewards is 260.5516288810599\n",
      "10000\n",
      "The episode 511 total rewards is 268.2081400187353\n",
      "10000\n",
      "The episode 521 total rewards is 139.65806279638528\n",
      "10000\n",
      "The episode 531 total rewards is -122.88757563795164\n",
      "10000\n",
      "The episode 541 total rewards is -25.26603787730934\n",
      "10000\n",
      "The episode 551 total rewards is 230.97220582814924\n",
      "10000\n",
      "The episode 561 total rewards is -6.373248965938492\n",
      "10000\n",
      "The episode 571 total rewards is 208.05449285063392\n",
      "10000\n",
      "The episode 581 total rewards is 151.61772360930107\n",
      "10000\n",
      "The episode 591 total rewards is 182.06499008567448\n",
      "10000\n",
      "The episode 601 total rewards is 252.36385151119916\n",
      "10000\n",
      "The episode 611 total rewards is 269.73040524239923\n",
      "10000\n",
      "The episode 621 total rewards is -178.29513844618174\n",
      "10000\n",
      "The episode 631 total rewards is -17.544497080236255\n",
      "10000\n",
      "The episode 641 total rewards is -17.312100707568234\n",
      "10000\n",
      "The episode 651 total rewards is 2.0751752692419245\n",
      "10000\n",
      "The episode 661 total rewards is 266.3725000944165\n",
      "10000\n",
      "The episode 671 total rewards is 277.5313170787274\n",
      "10000\n",
      "The episode 681 total rewards is 208.87206795186358\n",
      "10000\n",
      "The episode 691 total rewards is 171.4691188538962\n",
      "10000\n",
      "The episode 701 total rewards is 199.73465568139915\n",
      "10000\n",
      "The episode 711 total rewards is 260.2825945881806\n",
      "10000\n",
      "The episode 721 total rewards is 241.7041724936784\n",
      "10000\n",
      "The episode 731 total rewards is 247.378339454918\n",
      "10000\n",
      "The episode 741 total rewards is 263.5568475397487\n",
      "10000\n",
      "The episode 751 total rewards is 154.7634242747203\n",
      "10000\n",
      "The episode 761 total rewards is 251.99238787707702\n",
      "10000\n",
      "The episode 771 total rewards is 241.46697909123426\n",
      "10000\n",
      "The episode 781 total rewards is 247.90971015646448\n",
      "10000\n",
      "The episode 791 total rewards is 231.15633292085806\n",
      "10000\n",
      "The episode 801 total rewards is 261.0711815451124\n",
      "10000\n",
      "The episode 811 total rewards is 276.07330425646194\n",
      "10000\n",
      "The episode 821 total rewards is -17.672949435080525\n",
      "10000\n",
      "The episode 831 total rewards is 254.42566755854142\n",
      "10000\n",
      "The episode 841 total rewards is 22.811516492245133\n",
      "10000\n",
      "The episode 851 total rewards is 6.6538610214958\n",
      "10000\n",
      "The episode 861 total rewards is -348.72972143127504\n",
      "10000\n",
      "The episode 871 total rewards is 277.7060059051171\n",
      "10000\n",
      "The episode 881 total rewards is 121.28466928071347\n",
      "10000\n",
      "The episode 891 total rewards is 29.19418420653892\n",
      "10000\n",
      "The episode 901 total rewards is 4.927050147964721\n",
      "10000\n",
      "The episode 911 total rewards is 257.95461300059736\n",
      "10000\n",
      "The episode 921 total rewards is 236.57863194959654\n",
      "10000\n",
      "The episode 931 total rewards is 272.1236839878835\n",
      "10000\n",
      "The episode 941 total rewards is -27.217862691223814\n",
      "10000\n",
      "The episode 951 total rewards is 292.3523818752195\n",
      "10000\n",
      "The episode 961 total rewards is 182.89796477946192\n",
      "10000\n",
      "The episode 971 total rewards is -23.782898367817552\n",
      "10000\n",
      "The episode 981 total rewards is 11.322283594812689\n",
      "10000\n",
      "The episode 991 total rewards is 98.47469582412918\n",
      "10000\n",
      "891.5481688976288\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('LunarLander-v2')\n",
    "training_agent = Training_agent(env)\n",
    "start = time.time()\n",
    "training_agent.train_agent(1000,32)\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69786029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa16eeac3a0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_agent.rewards)\n",
    "plt.plot(training_agent.moving_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba259d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:01:46,357]\u001b[0m A new study created in memory with name: no-name-eb36011f-f9de-44f6-b253-153428d9990a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -257.92890228366923\n",
      "The episode 11 total rewards is -127.07684855971345\n",
      "The episode 21 total rewards is -391.1194038258198\n",
      "The episode 31 total rewards is -228.0570674900716\n",
      "The episode 41 total rewards is -122.43016885560566\n",
      "The episode 51 total rewards is -386.9165278683204\n",
      "The episode 61 total rewards is -319.44215308816035\n",
      "The episode 71 total rewards is -35.42086237725613\n",
      "The episode 81 total rewards is -109.6761235353246\n",
      "The episode 91 total rewards is -60.87882010940865\n",
      "The episode 101 total rewards is -219.23740342917264\n",
      "The episode 111 total rewards is -74.3909092674826\n",
      "The episode 121 total rewards is -5.705532270335283\n",
      "The episode 131 total rewards is -165.1297574612692\n",
      "The episode 141 total rewards is -76.22493373043018\n",
      "The episode 151 total rewards is -174.16627508462122\n",
      "The episode 161 total rewards is 72.42279704375073\n",
      "The episode 171 total rewards is 49.64904718470949\n",
      "The episode 181 total rewards is 21.113414967814382\n",
      "The episode 191 total rewards is -26.029874178098837\n",
      "The episode 201 total rewards is 42.346525653454584\n",
      "The episode 211 total rewards is -3.6027171016611366\n",
      "The episode 221 total rewards is 16.99008081286668\n",
      "The episode 231 total rewards is -11.43609179153021\n",
      "The episode 241 total rewards is -74.11165658206889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:06:39,331]\u001b[0m Trial 0 finished with value: -29.99549182288824 and parameters: {'DQL_nodes 1': 48, 'nodes_2': 45, 'dicount rate': 0.9783032937004372, 'lr': 0.000502586256733191, 'DQL nodes 2': 54, 'decay': 0.9933602260587918}. Best is trial 0 with value: -29.99549182288824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -242.14047645391224\n",
      "The episode 11 total rewards is -96.02134826642487\n",
      "The episode 21 total rewards is -131.1083963397374\n",
      "The episode 31 total rewards is -157.3766723407089\n",
      "The episode 41 total rewards is -115.52319736649063\n",
      "The episode 51 total rewards is -65.10516296320682\n",
      "The episode 61 total rewards is -172.7050514325747\n",
      "The episode 71 total rewards is -153.9633908748865\n",
      "The episode 81 total rewards is -182.9481175723144\n",
      "The episode 91 total rewards is -76.65861940230687\n",
      "The episode 101 total rewards is -221.7798107130646\n",
      "The episode 111 total rewards is -161.43506754646856\n",
      "The episode 121 total rewards is 103.26753139923105\n",
      "The episode 131 total rewards is -187.83480063108826\n",
      "The episode 141 total rewards is 36.23032762683923\n",
      "The episode 151 total rewards is -81.88513789875897\n",
      "The episode 161 total rewards is -69.79033658340974\n",
      "The episode 171 total rewards is -107.83392852973236\n",
      "The episode 181 total rewards is -105.51990110700473\n",
      "The episode 191 total rewards is -338.8086670291749\n",
      "The episode 201 total rewards is -94.23521250412\n",
      "The episode 211 total rewards is -103.61767418715596\n",
      "The episode 221 total rewards is -55.990931365999046\n",
      "The episode 231 total rewards is -62.7737149110514\n",
      "The episode 241 total rewards is -119.75578397743675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:07:32,804]\u001b[0m Trial 1 finished with value: -99.70520742207957 and parameters: {'DQL_nodes 1': 96, 'nodes_2': 66, 'dicount rate': 0.9370978791284981, 'lr': 0.0008819628347575319, 'DQL nodes 2': 63, 'decay': 0.9987200264772003}. Best is trial 0 with value: -29.99549182288824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -118.84076448612475\n",
      "The episode 11 total rewards is -30.048832734759856\n",
      "The episode 21 total rewards is -258.8910951021716\n",
      "The episode 31 total rewards is -186.5633227500574\n",
      "The episode 41 total rewards is -250.26277293500098\n",
      "The episode 51 total rewards is -86.8366262312989\n",
      "The episode 61 total rewards is -208.25239753950783\n",
      "The episode 71 total rewards is -229.74307820321445\n",
      "The episode 81 total rewards is -281.3417966583273\n",
      "The episode 91 total rewards is -368.08375156922665\n",
      "The episode 101 total rewards is -96.04442395055457\n",
      "The episode 111 total rewards is -198.46926237759632\n",
      "The episode 121 total rewards is 18.235874631674875\n",
      "The episode 131 total rewards is 106.78825113498769\n",
      "The episode 141 total rewards is -225.24364891518263\n",
      "The episode 151 total rewards is 58.04676089535188\n",
      "The episode 161 total rewards is -160.44947398759143\n",
      "The episode 171 total rewards is -35.396639320536536\n",
      "The episode 181 total rewards is 25.198656447466117\n",
      "The episode 191 total rewards is 101.10702598633567\n",
      "The episode 201 total rewards is -233.70951496600762\n",
      "The episode 211 total rewards is 180.85689766089536\n",
      "The episode 221 total rewards is -102.71218820695422\n",
      "The episode 231 total rewards is 265.37411506589126\n",
      "The episode 241 total rewards is 0.5758144237045997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:12:07,828]\u001b[0m Trial 2 finished with value: -28.12598937440493 and parameters: {'DQL_nodes 1': 34, 'nodes_2': 33, 'dicount rate': 0.9010952121615743, 'lr': 0.0005776518285238604, 'DQL nodes 2': 51, 'decay': 0.9906050684717242}. Best is trial 2 with value: -28.12598937440493.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -248.8812737353379\n",
      "The episode 11 total rewards is -189.79354456877493\n",
      "The episode 21 total rewards is -207.02021018726646\n",
      "The episode 31 total rewards is -195.10764487751268\n",
      "The episode 41 total rewards is -65.20316259323408\n",
      "The episode 51 total rewards is -144.98490126812396\n",
      "The episode 61 total rewards is -57.194373538407\n",
      "The episode 71 total rewards is -157.4786779819663\n",
      "The episode 81 total rewards is -98.48646157024288\n",
      "The episode 91 total rewards is -183.56198402882205\n",
      "The episode 101 total rewards is -20.67067075827923\n",
      "The episode 111 total rewards is 38.63655428898477\n",
      "The episode 121 total rewards is 11.008286642639263\n",
      "The episode 131 total rewards is -63.875490497050016\n",
      "The episode 141 total rewards is -161.06686235607526\n",
      "The episode 151 total rewards is 100.63889555270663\n",
      "The episode 161 total rewards is 54.37035189439922\n",
      "The episode 171 total rewards is 73.05151962415488\n",
      "The episode 181 total rewards is 6.022259402713544\n",
      "The episode 191 total rewards is 84.37630331604633\n",
      "The episode 201 total rewards is 17.23067212424419\n",
      "The episode 211 total rewards is 138.43992464862373\n",
      "The episode 221 total rewards is 294.67136227547496\n",
      "The episode 231 total rewards is 271.087626158831\n",
      "The episode 241 total rewards is 192.34474147825654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:16:46,968]\u001b[0m Trial 3 finished with value: 41.82946896658695 and parameters: {'DQL_nodes 1': 46, 'nodes_2': 96, 'dicount rate': 0.9915670677042686, 'lr': 0.0009074084589674853, 'DQL nodes 2': 58, 'decay': 0.9942005887509291}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -204.80913664701438\n",
      "The episode 11 total rewards is -156.0009112389271\n",
      "The episode 21 total rewards is -68.69661597354194\n",
      "The episode 31 total rewards is -330.6442446490594\n",
      "The episode 41 total rewards is -80.02039248577776\n",
      "The episode 51 total rewards is -77.59155110811218\n",
      "The episode 61 total rewards is -261.61180158355273\n",
      "The episode 71 total rewards is -422.63440737608283\n",
      "The episode 81 total rewards is -149.26567097268085\n",
      "The episode 91 total rewards is -158.5186877959187\n",
      "The episode 101 total rewards is -89.53279422752964\n",
      "The episode 111 total rewards is -301.72743365334986\n",
      "The episode 121 total rewards is -93.89360150577662\n",
      "The episode 131 total rewards is -98.2125845983281\n",
      "The episode 141 total rewards is -100.64548183750995\n",
      "The episode 151 total rewards is -175.2854198530402\n",
      "The episode 161 total rewards is -76.25362422147404\n",
      "The episode 171 total rewards is -218.03460672798596\n",
      "The episode 181 total rewards is -104.8834703841826\n",
      "The episode 191 total rewards is -265.2194318549151\n",
      "The episode 201 total rewards is -128.59259543228555\n",
      "The episode 211 total rewards is -25.85358553858522\n",
      "The episode 221 total rewards is -132.34401977599398\n",
      "The episode 231 total rewards is -98.01824607803597\n",
      "The episode 241 total rewards is -214.0865786784385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:17:32,755]\u001b[0m Trial 4 finished with value: -120.84191837912216 and parameters: {'DQL_nodes 1': 37, 'nodes_2': 35, 'dicount rate': 0.9610071961820451, 'lr': 0.0006602756754233055, 'DQL nodes 2': 62, 'decay': 0.9988539795818744}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -100.19262856899421\n",
      "The episode 11 total rewards is -322.88827562829124\n",
      "The episode 21 total rewards is -434.34617949033935\n",
      "The episode 31 total rewards is -401.8855499655979\n",
      "The episode 41 total rewards is -245.35393582898004\n",
      "The episode 51 total rewards is -321.14167226379993\n",
      "The episode 61 total rewards is -210.98478942494546\n",
      "The episode 71 total rewards is -177.56486434779117\n",
      "The episode 81 total rewards is -255.2868039110522\n",
      "The episode 91 total rewards is -66.8370197805145\n",
      "The episode 101 total rewards is -42.13134429334145\n",
      "The episode 111 total rewards is -204.86416812002392\n",
      "The episode 121 total rewards is -2.486425917965306\n",
      "The episode 131 total rewards is -55.3265115941852\n",
      "The episode 141 total rewards is -135.31584121611695\n",
      "The episode 151 total rewards is -44.353394331780244\n",
      "The episode 161 total rewards is -59.7140324540089\n",
      "The episode 171 total rewards is 63.29405789725937\n",
      "The episode 181 total rewards is -27.34164518187319\n",
      "The episode 191 total rewards is 133.44463127512765\n",
      "The episode 201 total rewards is -167.753811076795\n",
      "The episode 211 total rewards is 74.96265843668041\n",
      "The episode 221 total rewards is 109.32458450113725\n",
      "The episode 231 total rewards is -83.9693296146518\n",
      "The episode 241 total rewards is 23.65403460937354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:20:41,976]\u001b[0m Trial 5 finished with value: -41.019745802296164 and parameters: {'DQL_nodes 1': 49, 'nodes_2': 79, 'dicount rate': 0.9572936847858522, 'lr': 0.0006565298716658261, 'DQL nodes 2': 32, 'decay': 0.9956080725479213}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -325.639409699011\n",
      "The episode 11 total rewards is -346.54936249396303\n",
      "The episode 21 total rewards is -134.62079112097672\n",
      "The episode 31 total rewards is -33.04240742127993\n",
      "The episode 41 total rewards is -270.21451488600894\n",
      "The episode 51 total rewards is -178.0780565137514\n",
      "The episode 61 total rewards is -179.5482366669986\n",
      "The episode 71 total rewards is -195.18538591730908\n",
      "The episode 81 total rewards is -104.67151049054483\n",
      "The episode 91 total rewards is -62.79261311001281\n",
      "The episode 101 total rewards is -104.74160340864799\n",
      "The episode 111 total rewards is -53.19956528336384\n",
      "The episode 121 total rewards is -82.94282307094386\n",
      "The episode 131 total rewards is -51.14817011429786\n",
      "The episode 141 total rewards is -148.31430421438736\n",
      "The episode 151 total rewards is -93.89670250559861\n",
      "The episode 161 total rewards is -65.82731059773099\n",
      "The episode 171 total rewards is 20.019350262700172\n",
      "The episode 181 total rewards is -83.32218472818609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m150\u001b[39m:])\n\u001b[1;32m     51\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Create a new study.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     32\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[1;32m     37\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[0;34m(self, miniBatchSize)\u001b[0m\n\u001b[1;32m     65\u001b[0m Q_estimate_a \u001b[38;5;241m=\u001b[39m Q_estimate\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#the max gives both the max values and the indices \u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#if Done is true, we should not calculate Q for the next states \u001b[39;00m\n\u001b[1;32m     72\u001b[0m Q_target[Dones] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cdf4ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "The episode total rewards is  199.85354865513972\n"
     ]
    }
   ],
   "source": [
    "print(training_agent.agent.eps)\n",
    "training_agent.test_agent(env,render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265a412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
