{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb51a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labuser/miniconda3/envs/rl_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque, namedtuple\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc21110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16c73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode total rewards is  -135.17469718325276\n",
      "The episode total rewards is  -116.7741200625006\n",
      "The episode total rewards is  -265.583606382973\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        #render the env\n",
    "        env.render()\n",
    "    print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15bf757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8755872e-04,  1.4036485e+00, -5.9532933e-02, -3.2317889e-01,\n",
       "        6.8766251e-04,  1.3485086e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e584f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self,num_states,num_actions,nodes_1 =50, nodes_2 = 50):\n",
    "        super(Deep_Q_Network,self).__init__()\n",
    "        self.fc1 = nn.Linear(num_states,nodes_1)\n",
    "        self.fc2 = nn.Linear(nodes_1,nodes_2)\n",
    "        self.fc3 = nn.Linear(nodes_2,num_actions)\n",
    "    \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f753be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1069, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Deep_Q_Network(8,4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "states = env.reset()\n",
    "states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "model(states).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2ee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MemoryReplay:\n",
    "#     def __init__(self, max_size):\n",
    "#         self.states = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.next_states = []\n",
    "#         self.Dones = []\n",
    "#         self.max_size = max_size\n",
    "#         self.idx = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def append(self, state,action, reward,next_state,Done):\n",
    "#         #first in, first out \n",
    "#         if self.idx <= self.max_size:\n",
    "#             self.states.append(state)\n",
    "#             self.actions.append(action)\n",
    "#             self.rewards.append(reward)\n",
    "#             self.next_states.append(next_state)\n",
    "#             self.Dones.append(Done)\n",
    "#         else:\n",
    "#             #overwrite older values \n",
    "#             self.states[self.idx] = state\n",
    "#             self.actions[self.idx]= action\n",
    "#             self.rewards[self.idx] = reward\n",
    "#             self.next_states[self.idx] =next_state\n",
    "#             self.Dones[self.idx] = Done\n",
    "#         self.size = min(self.size + 1, self.max_size)\n",
    "#         self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         indices = sample(range(self.size), batch_size)\n",
    "#         states = np.array(self.states)[indices]\n",
    "#         actions =  np.array(self.actions)[indices]\n",
    "#         rewards = np.array(self.rewards)[indices]\n",
    "#         next_states = np.array(self.next_states)[indices]\n",
    "#         Dones = np.array(self.Dones)[indices]\n",
    "        \n",
    "#         return states,actions, rewards, next_states, Dones\n",
    "#     def __len__(self):\n",
    "#         return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4824d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReplay:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None]).squeeze(1)\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None]).squeeze(1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None]).squeeze(1)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c1f8b",
   "metadata": {},
   "source": [
    "## test Memory Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6227d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MemoryReplay(10000)\n",
    "\n",
    "num_episodes = 150\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        m1.append(state,action,reward,next_state,Done)\n",
    "        state = next_state\n",
    "    #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "len(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "733f1096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m2 = MemoryReplay_2(10000)\n",
    "\n",
    "# num_episodes = 150\n",
    "# for i in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     Done = False\n",
    "#     total_rewards = 0\n",
    "    \n",
    "#     while not Done: \n",
    "#         #take a random action\n",
    "#         action = env.action_space.sample()\n",
    "#         #implement the action \n",
    "#         next_state, reward, Done, info = env.step(action)\n",
    "#         #sum the rewards\n",
    "#         total_rewards += reward\n",
    "#         m2.append(state,action,reward,next_state,Done)\n",
    "#         state = next_state\n",
    "#     #print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "# len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70ba087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32,)\n"
     ]
    }
   ],
   "source": [
    "states1,actions1,rewards2,next_states1,Dones1 = m1.sample(32)\n",
    "states2,actions2,rewards2,next_states2,Dones2 = m2.sample(32)\n",
    "print(Dones1.shape, Dones2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73e3351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 ms ± 204 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit m.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c788cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL_Agent:\n",
    "    def __init__(self,env,memory_max_size =10_000,dicount= 0.99,lr_optim=1e-3,DQL_node1=50,DQL_node2=50,decay_rate = 0.996):\n",
    "        self.env = env\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        self.dicount = dicount\n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.decay_rate_eps = decay_rate\n",
    "        self.min_eps = 0.05\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.reply_memory = MemoryReplay(memory_max_size)\n",
    "        self.Q_action = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2).to(self.device)\n",
    "        self.Q_target = Deep_Q_Network(self.num_states,self.num_action,DQL_node1,DQL_node2).to(self.device)\n",
    "        self.Q_target.eval() #will turn off any dropout or batch norm layer \n",
    "        #make sure both network has identical weights \n",
    "        self.update_target_weights()\n",
    "        \n",
    "        self.loss_fucntion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.Q_action.parameters(),lr=lr_optim)\n",
    "        \n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        self.Q_target.load_state_dict(self.Q_action.state_dict())\n",
    "    \n",
    "    def eps_greedy(self,states):\n",
    "        if np.random.rand()<self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            #act greedy\n",
    "            \n",
    "            #make sure the state are tensor in order to feed it to the network\n",
    "            if not torch.is_tensor(states):\n",
    "                states = torch.from_numpy(states[np.newaxis,:]).float().to(self.device)\n",
    "            with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "                action = self.Q_action(states)\n",
    "            max_action = torch.argmax(action).item()\n",
    "            return max_action\n",
    "    \n",
    "    def decay_eps(self):\n",
    "        self.eps = np.maximum(self.eps*self.decay_rate_eps,self.min_eps)\n",
    "    \n",
    "    def to_tensor(self,states, actions, rewards,next_states,Dones):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        Dones = torch.from_numpy(Dones).to(self.device)\n",
    "        return states, actions, rewards, next_states, Dones\n",
    "    def learnFromExperience(self,miniBatchSize): #hallucinations\n",
    "        if miniBatchSize <2:\n",
    "            raise ValueError(\"batch size must greater than 1\")\n",
    "        #make sure we have enough experiences \n",
    "        if len(self.reply_memory) < miniBatchSize:\n",
    "            return #not enough experience, sounds familiar right?\n",
    "        #else sample and learn\n",
    "        states, actions, rewards, next_states, Dones = self.reply_memory.sample(miniBatchSize)\n",
    "        #convert the result to tensor for model input \n",
    "        states, actions, rewards, next_states, Dones = self.to_tensor(states, actions, rewards, next_states, Dones)\n",
    "        #calculate the current Q estimation \n",
    "        Q_estimate = self.Q_action(states)\n",
    "        #obtain the q value for the actioned used in the experiences \n",
    "        Q_estimate_a = Q_estimate.gather(1, actions.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        #calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\n",
    "        #the max gives both the max values and the indices \n",
    "        Q_target = self.Q_target(next_states).max(dim=1).values\n",
    "        #note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\n",
    "        #if Done is true, we should not calculate Q for the next states \n",
    "        Q_target[Dones] = 0.0 \n",
    "        #final target calculation\n",
    "        Q_target = rewards + self.dicount*Q_target\n",
    "        \n",
    "        #make sure the grad is zero \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #calculate the loss \n",
    "        loss=self.loss_fucntion(Q_target,Q_estimate_a)\n",
    "        #calcualte the gradient dL/dw\n",
    "        loss.backward()\n",
    "        #optimize using gradient decent\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_max_action(self,states):\n",
    "        self.Q_action.eval()\n",
    "        #make sure the state are tensor in order to feed it to the network\n",
    "        if not torch.is_tensor(states):\n",
    "            states = torch.from_numpy(states[np.newaxis,:]).to(self.device)\n",
    "        with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "            action = self.Q_target(states)\n",
    "        max_action = torch.argmax(action).item()\n",
    "        return max_action\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d89db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using optuna\n",
    "import optuna\n",
    "\n",
    "def train(trial):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    memory_max_size = 10_000\n",
    "    DQL_node1 = trial.suggest_int('DQL_nodes 1', 30, 100)\n",
    "    DQL_node2 = trial.suggest_int('nodes_2', 30, 100)\n",
    "    dicount = trial.suggest_float('dicount rate', 0.9, 1.0, log=True)\n",
    "    lr_optim = trial.suggest_float('lr', 5e-4, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int('batch size', 32, 64)\n",
    "    decay_rate = trial.suggest_float('decay', 0.99,0.999, log=True)\n",
    "    agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,DQL_node1,DQL_node2,decay_rate)\n",
    "    update_freq = 1000\n",
    "    steps = 0 \n",
    "    \n",
    "    num_episodes = 250\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    moving_average = []\n",
    "    for i in range(num_episodes):\n",
    "        state = agent.env.reset()\n",
    "        Done = False\n",
    "        total_rewards = 0\n",
    "        while not Done: \n",
    "            #take an action using the greedy policy\n",
    "            action = agent.eps_greedy(state)\n",
    "            #implement the action \n",
    "            next_state, reward, Done, info = agent.env.step(action)\n",
    "            #save the experience in the memory of the agent \n",
    "            agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "            #sum the rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            #learn from experience (if there is enough)\n",
    "            agent.learnFromExperience(batch_size)\n",
    "            #update the tarqet network per the desired frequency \n",
    "            steps +=1 \n",
    "            if (steps % update_freq) == 0:\n",
    "                agent.update_target_weights()\n",
    "            state = next_state\n",
    "        #append the rewards\n",
    "        rewards[i] = total_rewards\n",
    "        moving_average.append(np.mean(rewards[-50:]))\n",
    "        #update the eps \n",
    "        agent.decay_eps()\n",
    "        if i %10 == 0:\n",
    "            print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "            #print(len(agent.reply_memory))\n",
    "        \n",
    "    return np.mean(rewards[-150:])\n",
    "study = optuna.create_study(direction=\"maximize\")  # Create a new study.\n",
    "study.optimize(train, n_trials=30)  # Invoke optimization of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ede0ef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-43f4db9d0565>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [17]\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,,DQL_node1,DQL_node2,decay_rate)\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Training_agent:\n",
    "    #Agent and environment interaction\n",
    "    def __init__(self, env,memory_max_size =10_000,dicount= 0.957,lr_optim=6.6e-3,update_freq =1000,DQL_node1=48,DQL_node2=79,decay_rate=0.9956):\n",
    "        self.agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim,DQL_node1,DQL_node2,decay_rate)\n",
    "        self.update_freq = update_freq\n",
    "        self.steps = 0 \n",
    "    \n",
    "    def train_agent(self, num_episodes,batch_size):\n",
    "        self.rewards = np.zeros(num_episodes)\n",
    "        self.moving_average = []\n",
    "        for i in range(num_episodes):\n",
    "            state = self.agent.env.reset()\n",
    "            Done = False\n",
    "            total_rewards = 0\n",
    "            while not Done: \n",
    "                #take an action using the greedy policy\n",
    "                action = self.agent.eps_greedy(state)\n",
    "                #implement the action \n",
    "                next_state, reward, Done, info = self.agent.env.step(action)\n",
    "                #save the experience in the memory of the agent \n",
    "                self.agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "                #sum the rewards\n",
    "                total_rewards += reward\n",
    "                \n",
    "                #learn from experience (if there is enough)\n",
    "                self.agent.learnFromExperience(batch_size)\n",
    "                #update the tarqet network per the desired frequency \n",
    "                self.steps +=1 \n",
    "                if (self.steps % self.update_freq) == 0:\n",
    "                    self.agent.update_target_weights()\n",
    "                state = next_state\n",
    "            #append the rewards\n",
    "            self.rewards[i] = total_rewards\n",
    "            self.moving_average.append(np.mean(self.rewards[-50:]))\n",
    "            #update the eps \n",
    "            self.agent.decay_eps()\n",
    "            if i %10 == 0:\n",
    "                print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "                print(len(self.agent.reply_memory))\n",
    "    def test_agent(self,env,num_run, render=False):\n",
    "        rewards = np.zeros(num_run)\n",
    "        for i in range(num_run):\n",
    "            state = env.reset()\n",
    "            Done = False \n",
    "            total_rewards = 0\n",
    "\n",
    "            while not Done: \n",
    "                action = agent.get_max_action(state)\n",
    "                next_state, reward,Done, info = env.step(action)\n",
    "                total_rewards += reward\n",
    "                if render:\n",
    "                    env.render()\n",
    "                state = next_state\n",
    "            rewards[i]= total_rewards\n",
    "            print(\"The episode total rewards is \", total_rewards)\n",
    "        env.close()\n",
    "        return rewards\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a2dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -104.76862202251367\n",
      "70\n",
      "The episode 11 total rewards is -161.5043664297409\n",
      "969\n",
      "The episode 21 total rewards is -63.89878976471971\n",
      "1917\n",
      "The episode 31 total rewards is -422.2351310560372\n",
      "2821\n",
      "The episode 41 total rewards is -108.94253449066348\n",
      "3984\n",
      "The episode 51 total rewards is -343.28524699276517\n",
      "5039\n",
      "The episode 61 total rewards is -247.83418765912046\n",
      "6092\n",
      "The episode 71 total rewards is -183.6416698759798\n",
      "7147\n",
      "The episode 81 total rewards is -213.9115354856329\n",
      "8362\n",
      "The episode 91 total rewards is -73.56223305560535\n",
      "10000\n",
      "The episode 101 total rewards is -31.724966269467487\n",
      "10000\n",
      "The episode 111 total rewards is -96.30695703373149\n",
      "10000\n",
      "The episode 121 total rewards is -216.7753973301118\n",
      "10000\n",
      "The episode 131 total rewards is -55.0252370062708\n",
      "10000\n",
      "The episode 141 total rewards is -114.19691335443041\n",
      "10000\n",
      "The episode 151 total rewards is -192.0828198593851\n",
      "10000\n",
      "The episode 161 total rewards is 8.74077258792532\n",
      "10000\n",
      "The episode 171 total rewards is 4.3692146263049665\n",
      "10000\n",
      "The episode 181 total rewards is -59.88697119508632\n",
      "10000\n",
      "The episode 191 total rewards is -109.15042390744884\n",
      "10000\n",
      "The episode 201 total rewards is -67.62460322812915\n",
      "10000\n",
      "The episode 211 total rewards is -29.393205153937963\n",
      "10000\n",
      "The episode 221 total rewards is -18.881470678310976\n",
      "10000\n",
      "The episode 231 total rewards is 26.14120127319459\n",
      "10000\n",
      "The episode 241 total rewards is -48.648576272565535\n",
      "10000\n",
      "The episode 251 total rewards is -5.6268883167310975\n",
      "10000\n",
      "The episode 261 total rewards is 76.44524440682814\n",
      "10000\n",
      "The episode 271 total rewards is 139.930006248851\n",
      "10000\n",
      "The episode 281 total rewards is 27.782718654842917\n",
      "10000\n",
      "The episode 291 total rewards is 169.56593482137947\n",
      "10000\n",
      "The episode 301 total rewards is 121.72380001157248\n",
      "10000\n",
      "The episode 311 total rewards is 39.94309573622317\n",
      "10000\n",
      "The episode 321 total rewards is -192.42534065571607\n",
      "10000\n",
      "The episode 331 total rewards is 234.25594425437927\n",
      "10000\n",
      "The episode 341 total rewards is -182.95079206407053\n",
      "10000\n",
      "The episode 351 total rewards is 132.5668942960888\n",
      "10000\n",
      "The episode 361 total rewards is 112.5451016311882\n",
      "10000\n",
      "The episode 371 total rewards is 85.66023560369771\n",
      "10000\n",
      "The episode 381 total rewards is -63.731345902172066\n",
      "10000\n",
      "The episode 391 total rewards is 141.70518112602366\n",
      "10000\n",
      "The episode 401 total rewards is 192.8387001121746\n",
      "10000\n",
      "The episode 411 total rewards is 2.5567212074641787\n",
      "10000\n",
      "The episode 421 total rewards is 27.459494083355708\n",
      "10000\n",
      "The episode 431 total rewards is 194.39983740464703\n",
      "10000\n",
      "The episode 441 total rewards is 107.86825303039036\n",
      "10000\n",
      "The episode 451 total rewards is -21.432079773844507\n",
      "10000\n",
      "The episode 461 total rewards is 175.36253627459251\n",
      "10000\n",
      "The episode 471 total rewards is 165.368133183091\n",
      "10000\n",
      "The episode 481 total rewards is 63.87207181481863\n",
      "10000\n",
      "The episode 491 total rewards is -20.475150009385537\n",
      "10000\n",
      "The episode 501 total rewards is 260.5516288810599\n",
      "10000\n",
      "The episode 511 total rewards is 268.2081400187353\n",
      "10000\n",
      "The episode 521 total rewards is 139.65806279638528\n",
      "10000\n",
      "The episode 531 total rewards is -122.88757563795164\n",
      "10000\n",
      "The episode 541 total rewards is -25.26603787730934\n",
      "10000\n",
      "The episode 551 total rewards is 230.97220582814924\n",
      "10000\n",
      "The episode 561 total rewards is -6.373248965938492\n",
      "10000\n",
      "The episode 571 total rewards is 208.05449285063392\n",
      "10000\n",
      "The episode 581 total rewards is 151.61772360930107\n",
      "10000\n",
      "The episode 591 total rewards is 182.06499008567448\n",
      "10000\n",
      "The episode 601 total rewards is 252.36385151119916\n",
      "10000\n",
      "The episode 611 total rewards is 269.73040524239923\n",
      "10000\n",
      "The episode 621 total rewards is -178.29513844618174\n",
      "10000\n",
      "The episode 631 total rewards is -17.544497080236255\n",
      "10000\n",
      "The episode 641 total rewards is -17.312100707568234\n",
      "10000\n",
      "The episode 651 total rewards is 2.0751752692419245\n",
      "10000\n",
      "The episode 661 total rewards is 266.3725000944165\n",
      "10000\n",
      "The episode 671 total rewards is 277.5313170787274\n",
      "10000\n",
      "The episode 681 total rewards is 208.87206795186358\n",
      "10000\n",
      "The episode 691 total rewards is 171.4691188538962\n",
      "10000\n",
      "The episode 701 total rewards is 199.73465568139915\n",
      "10000\n",
      "The episode 711 total rewards is 260.2825945881806\n",
      "10000\n",
      "The episode 721 total rewards is 241.7041724936784\n",
      "10000\n",
      "The episode 731 total rewards is 247.378339454918\n",
      "10000\n",
      "The episode 741 total rewards is 263.5568475397487\n",
      "10000\n",
      "The episode 751 total rewards is 154.7634242747203\n",
      "10000\n",
      "The episode 761 total rewards is 251.99238787707702\n",
      "10000\n",
      "The episode 771 total rewards is 241.46697909123426\n",
      "10000\n",
      "The episode 781 total rewards is 247.90971015646448\n",
      "10000\n",
      "The episode 791 total rewards is 231.15633292085806\n",
      "10000\n",
      "The episode 801 total rewards is 261.0711815451124\n",
      "10000\n",
      "The episode 811 total rewards is 276.07330425646194\n",
      "10000\n",
      "The episode 821 total rewards is -17.672949435080525\n",
      "10000\n",
      "The episode 831 total rewards is 254.42566755854142\n",
      "10000\n",
      "The episode 841 total rewards is 22.811516492245133\n",
      "10000\n",
      "The episode 851 total rewards is 6.6538610214958\n",
      "10000\n",
      "The episode 861 total rewards is -348.72972143127504\n",
      "10000\n",
      "The episode 871 total rewards is 277.7060059051171\n",
      "10000\n",
      "The episode 881 total rewards is 121.28466928071347\n",
      "10000\n",
      "The episode 891 total rewards is 29.19418420653892\n",
      "10000\n",
      "The episode 901 total rewards is 4.927050147964721\n",
      "10000\n",
      "The episode 911 total rewards is 257.95461300059736\n",
      "10000\n",
      "The episode 921 total rewards is 236.57863194959654\n",
      "10000\n",
      "The episode 931 total rewards is 272.1236839878835\n",
      "10000\n",
      "The episode 941 total rewards is -27.217862691223814\n",
      "10000\n",
      "The episode 951 total rewards is 292.3523818752195\n",
      "10000\n",
      "The episode 961 total rewards is 182.89796477946192\n",
      "10000\n",
      "The episode 971 total rewards is -23.782898367817552\n",
      "10000\n",
      "The episode 981 total rewards is 11.322283594812689\n",
      "10000\n",
      "The episode 991 total rewards is 98.47469582412918\n",
      "10000\n",
      "891.5481688976288\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('LunarLander-v2')\n",
    "training_agent = Training_agent(env)\n",
    "start = time.time()\n",
    "training_agent.train_agent(1000,32)\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69786029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa16eeac3a0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_agent.rewards)\n",
    "plt.plot(training_agent.moving_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba259d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:01:46,357]\u001b[0m A new study created in memory with name: no-name-eb36011f-f9de-44f6-b253-153428d9990a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -257.92890228366923\n",
      "The episode 11 total rewards is -127.07684855971345\n",
      "The episode 21 total rewards is -391.1194038258198\n",
      "The episode 31 total rewards is -228.0570674900716\n",
      "The episode 41 total rewards is -122.43016885560566\n",
      "The episode 51 total rewards is -386.9165278683204\n",
      "The episode 61 total rewards is -319.44215308816035\n",
      "The episode 71 total rewards is -35.42086237725613\n",
      "The episode 81 total rewards is -109.6761235353246\n",
      "The episode 91 total rewards is -60.87882010940865\n",
      "The episode 101 total rewards is -219.23740342917264\n",
      "The episode 111 total rewards is -74.3909092674826\n",
      "The episode 121 total rewards is -5.705532270335283\n",
      "The episode 131 total rewards is -165.1297574612692\n",
      "The episode 141 total rewards is -76.22493373043018\n",
      "The episode 151 total rewards is -174.16627508462122\n",
      "The episode 161 total rewards is 72.42279704375073\n",
      "The episode 171 total rewards is 49.64904718470949\n",
      "The episode 181 total rewards is 21.113414967814382\n",
      "The episode 191 total rewards is -26.029874178098837\n",
      "The episode 201 total rewards is 42.346525653454584\n",
      "The episode 211 total rewards is -3.6027171016611366\n",
      "The episode 221 total rewards is 16.99008081286668\n",
      "The episode 231 total rewards is -11.43609179153021\n",
      "The episode 241 total rewards is -74.11165658206889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:06:39,331]\u001b[0m Trial 0 finished with value: -29.99549182288824 and parameters: {'DQL_nodes 1': 48, 'nodes_2': 45, 'dicount rate': 0.9783032937004372, 'lr': 0.000502586256733191, 'DQL nodes 2': 54, 'decay': 0.9933602260587918}. Best is trial 0 with value: -29.99549182288824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -242.14047645391224\n",
      "The episode 11 total rewards is -96.02134826642487\n",
      "The episode 21 total rewards is -131.1083963397374\n",
      "The episode 31 total rewards is -157.3766723407089\n",
      "The episode 41 total rewards is -115.52319736649063\n",
      "The episode 51 total rewards is -65.10516296320682\n",
      "The episode 61 total rewards is -172.7050514325747\n",
      "The episode 71 total rewards is -153.9633908748865\n",
      "The episode 81 total rewards is -182.9481175723144\n",
      "The episode 91 total rewards is -76.65861940230687\n",
      "The episode 101 total rewards is -221.7798107130646\n",
      "The episode 111 total rewards is -161.43506754646856\n",
      "The episode 121 total rewards is 103.26753139923105\n",
      "The episode 131 total rewards is -187.83480063108826\n",
      "The episode 141 total rewards is 36.23032762683923\n",
      "The episode 151 total rewards is -81.88513789875897\n",
      "The episode 161 total rewards is -69.79033658340974\n",
      "The episode 171 total rewards is -107.83392852973236\n",
      "The episode 181 total rewards is -105.51990110700473\n",
      "The episode 191 total rewards is -338.8086670291749\n",
      "The episode 201 total rewards is -94.23521250412\n",
      "The episode 211 total rewards is -103.61767418715596\n",
      "The episode 221 total rewards is -55.990931365999046\n",
      "The episode 231 total rewards is -62.7737149110514\n",
      "The episode 241 total rewards is -119.75578397743675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:07:32,804]\u001b[0m Trial 1 finished with value: -99.70520742207957 and parameters: {'DQL_nodes 1': 96, 'nodes_2': 66, 'dicount rate': 0.9370978791284981, 'lr': 0.0008819628347575319, 'DQL nodes 2': 63, 'decay': 0.9987200264772003}. Best is trial 0 with value: -29.99549182288824.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -118.84076448612475\n",
      "The episode 11 total rewards is -30.048832734759856\n",
      "The episode 21 total rewards is -258.8910951021716\n",
      "The episode 31 total rewards is -186.5633227500574\n",
      "The episode 41 total rewards is -250.26277293500098\n",
      "The episode 51 total rewards is -86.8366262312989\n",
      "The episode 61 total rewards is -208.25239753950783\n",
      "The episode 71 total rewards is -229.74307820321445\n",
      "The episode 81 total rewards is -281.3417966583273\n",
      "The episode 91 total rewards is -368.08375156922665\n",
      "The episode 101 total rewards is -96.04442395055457\n",
      "The episode 111 total rewards is -198.46926237759632\n",
      "The episode 121 total rewards is 18.235874631674875\n",
      "The episode 131 total rewards is 106.78825113498769\n",
      "The episode 141 total rewards is -225.24364891518263\n",
      "The episode 151 total rewards is 58.04676089535188\n",
      "The episode 161 total rewards is -160.44947398759143\n",
      "The episode 171 total rewards is -35.396639320536536\n",
      "The episode 181 total rewards is 25.198656447466117\n",
      "The episode 191 total rewards is 101.10702598633567\n",
      "The episode 201 total rewards is -233.70951496600762\n",
      "The episode 211 total rewards is 180.85689766089536\n",
      "The episode 221 total rewards is -102.71218820695422\n",
      "The episode 231 total rewards is 265.37411506589126\n",
      "The episode 241 total rewards is 0.5758144237045997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:12:07,828]\u001b[0m Trial 2 finished with value: -28.12598937440493 and parameters: {'DQL_nodes 1': 34, 'nodes_2': 33, 'dicount rate': 0.9010952121615743, 'lr': 0.0005776518285238604, 'DQL nodes 2': 51, 'decay': 0.9906050684717242}. Best is trial 2 with value: -28.12598937440493.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -248.8812737353379\n",
      "The episode 11 total rewards is -189.79354456877493\n",
      "The episode 21 total rewards is -207.02021018726646\n",
      "The episode 31 total rewards is -195.10764487751268\n",
      "The episode 41 total rewards is -65.20316259323408\n",
      "The episode 51 total rewards is -144.98490126812396\n",
      "The episode 61 total rewards is -57.194373538407\n",
      "The episode 71 total rewards is -157.4786779819663\n",
      "The episode 81 total rewards is -98.48646157024288\n",
      "The episode 91 total rewards is -183.56198402882205\n",
      "The episode 101 total rewards is -20.67067075827923\n",
      "The episode 111 total rewards is 38.63655428898477\n",
      "The episode 121 total rewards is 11.008286642639263\n",
      "The episode 131 total rewards is -63.875490497050016\n",
      "The episode 141 total rewards is -161.06686235607526\n",
      "The episode 151 total rewards is 100.63889555270663\n",
      "The episode 161 total rewards is 54.37035189439922\n",
      "The episode 171 total rewards is 73.05151962415488\n",
      "The episode 181 total rewards is 6.022259402713544\n",
      "The episode 191 total rewards is 84.37630331604633\n",
      "The episode 201 total rewards is 17.23067212424419\n",
      "The episode 211 total rewards is 138.43992464862373\n",
      "The episode 221 total rewards is 294.67136227547496\n",
      "The episode 231 total rewards is 271.087626158831\n",
      "The episode 241 total rewards is 192.34474147825654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:16:46,968]\u001b[0m Trial 3 finished with value: 41.82946896658695 and parameters: {'DQL_nodes 1': 46, 'nodes_2': 96, 'dicount rate': 0.9915670677042686, 'lr': 0.0009074084589674853, 'DQL nodes 2': 58, 'decay': 0.9942005887509291}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -204.80913664701438\n",
      "The episode 11 total rewards is -156.0009112389271\n",
      "The episode 21 total rewards is -68.69661597354194\n",
      "The episode 31 total rewards is -330.6442446490594\n",
      "The episode 41 total rewards is -80.02039248577776\n",
      "The episode 51 total rewards is -77.59155110811218\n",
      "The episode 61 total rewards is -261.61180158355273\n",
      "The episode 71 total rewards is -422.63440737608283\n",
      "The episode 81 total rewards is -149.26567097268085\n",
      "The episode 91 total rewards is -158.5186877959187\n",
      "The episode 101 total rewards is -89.53279422752964\n",
      "The episode 111 total rewards is -301.72743365334986\n",
      "The episode 121 total rewards is -93.89360150577662\n",
      "The episode 131 total rewards is -98.2125845983281\n",
      "The episode 141 total rewards is -100.64548183750995\n",
      "The episode 151 total rewards is -175.2854198530402\n",
      "The episode 161 total rewards is -76.25362422147404\n",
      "The episode 171 total rewards is -218.03460672798596\n",
      "The episode 181 total rewards is -104.8834703841826\n",
      "The episode 191 total rewards is -265.2194318549151\n",
      "The episode 201 total rewards is -128.59259543228555\n",
      "The episode 211 total rewards is -25.85358553858522\n",
      "The episode 221 total rewards is -132.34401977599398\n",
      "The episode 231 total rewards is -98.01824607803597\n",
      "The episode 241 total rewards is -214.0865786784385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:17:32,755]\u001b[0m Trial 4 finished with value: -120.84191837912216 and parameters: {'DQL_nodes 1': 37, 'nodes_2': 35, 'dicount rate': 0.9610071961820451, 'lr': 0.0006602756754233055, 'DQL nodes 2': 62, 'decay': 0.9988539795818744}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -100.19262856899421\n",
      "The episode 11 total rewards is -322.88827562829124\n",
      "The episode 21 total rewards is -434.34617949033935\n",
      "The episode 31 total rewards is -401.8855499655979\n",
      "The episode 41 total rewards is -245.35393582898004\n",
      "The episode 51 total rewards is -321.14167226379993\n",
      "The episode 61 total rewards is -210.98478942494546\n",
      "The episode 71 total rewards is -177.56486434779117\n",
      "The episode 81 total rewards is -255.2868039110522\n",
      "The episode 91 total rewards is -66.8370197805145\n",
      "The episode 101 total rewards is -42.13134429334145\n",
      "The episode 111 total rewards is -204.86416812002392\n",
      "The episode 121 total rewards is -2.486425917965306\n",
      "The episode 131 total rewards is -55.3265115941852\n",
      "The episode 141 total rewards is -135.31584121611695\n",
      "The episode 151 total rewards is -44.353394331780244\n",
      "The episode 161 total rewards is -59.7140324540089\n",
      "The episode 171 total rewards is 63.29405789725937\n",
      "The episode 181 total rewards is -27.34164518187319\n",
      "The episode 191 total rewards is 133.44463127512765\n",
      "The episode 201 total rewards is -167.753811076795\n",
      "The episode 211 total rewards is 74.96265843668041\n",
      "The episode 221 total rewards is 109.32458450113725\n",
      "The episode 231 total rewards is -83.9693296146518\n",
      "The episode 241 total rewards is 23.65403460937354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-12 22:20:41,976]\u001b[0m Trial 5 finished with value: -41.019745802296164 and parameters: {'DQL_nodes 1': 49, 'nodes_2': 79, 'dicount rate': 0.9572936847858522, 'lr': 0.0006565298716658261, 'DQL nodes 2': 32, 'decay': 0.9956080725479213}. Best is trial 3 with value: 41.82946896658695.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -325.639409699011\n",
      "The episode 11 total rewards is -346.54936249396303\n",
      "The episode 21 total rewards is -134.62079112097672\n",
      "The episode 31 total rewards is -33.04240742127993\n",
      "The episode 41 total rewards is -270.21451488600894\n",
      "The episode 51 total rewards is -178.0780565137514\n",
      "The episode 61 total rewards is -179.5482366669986\n",
      "The episode 71 total rewards is -195.18538591730908\n",
      "The episode 81 total rewards is -104.67151049054483\n",
      "The episode 91 total rewards is -62.79261311001281\n",
      "The episode 101 total rewards is -104.74160340864799\n",
      "The episode 111 total rewards is -53.19956528336384\n",
      "The episode 121 total rewards is -82.94282307094386\n",
      "The episode 131 total rewards is -51.14817011429786\n",
      "The episode 141 total rewards is -148.31430421438736\n",
      "The episode 151 total rewards is -93.89670250559861\n",
      "The episode 161 total rewards is -65.82731059773099\n",
      "The episode 171 total rewards is 20.019350262700172\n",
      "The episode 181 total rewards is -83.32218472818609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m150\u001b[39m:])\n\u001b[1;32m     51\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Create a new study.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     32\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[1;32m     37\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[0;34m(self, miniBatchSize)\u001b[0m\n\u001b[1;32m     65\u001b[0m Q_estimate_a \u001b[38;5;241m=\u001b[39m Q_estimate\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#the max gives both the max values and the indices \u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#if Done is true, we should not calculate Q for the next states \u001b[39;00m\n\u001b[1;32m     72\u001b[0m Q_target[Dones] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cdf4ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "The episode total rewards is  199.85354865513972\n"
     ]
    }
   ],
   "source": [
    "print(training_agent.agent.eps)\n",
    "training_agent.test_agent(env,render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265a412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
