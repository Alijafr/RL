{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "cb51a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc21110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16c73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode total rewards is  -90.89642023477242\n",
      "The episode total rewards is  -62.172966114239614\n",
      "The episode total rewards is  -174.4161362167783\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        #render the env\n",
    "        env.render()\n",
    "    print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15bf757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8755872e-04,  1.4036485e+00, -5.9532933e-02, -3.2317889e-01,\n",
       "        6.8766251e-04,  1.3485086e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self,num_states,num_actions):\n",
    "        super(Deep_Q_Network,self).__init__()\n",
    "        self.fc1 = nn.Linear(num_states,200)\n",
    "        self.fc2 = nn.Linear(200,300)\n",
    "        self.fc3 = nn.Linear(300,num_actions)\n",
    "    \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4f753be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0241, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Deep_Q_Network(8,4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "states = env.reset()\n",
    "states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "model(states).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7a2ee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReplay:\n",
    "    def __init__(self, max_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.Dones = []\n",
    "        self.max_size = max_size\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, state,action, reward,next_state,Done):\n",
    "        #first in, first out \n",
    "        if self.idx <= self.max_size:\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.next_states.append(next_state)\n",
    "            self.Dones.append(Done)\n",
    "        else:\n",
    "            #overwrite older values \n",
    "            self.states[self.idx] = state\n",
    "            self.actions[self.idx]= action\n",
    "            self.rewards[self.idx] = reward\n",
    "            self.next_states[self.idx] =next_state\n",
    "            self.Dones[self.idx] = Done\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = sample(range(self.size), batch_size)\n",
    "        states = np.array(self.states)[indices]\n",
    "        actions =  np.array(self.actions)[indices]\n",
    "        rewards = np.array(self.rewards)[indices]\n",
    "        next_states = np.array(self.next_states)[indices]\n",
    "        Dones = np.array(self.Dones)[indices]\n",
    "        \n",
    "        return states,actions, rewards, next_states, Dones\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c1f8b",
   "metadata": {},
   "source": [
    "## test Memory Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a6227d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode total rewards is  -163.82915213010523\n",
      "The episode total rewards is  -104.65921664725411\n",
      "The episode total rewards is  -369.69208311776833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MemoryReplay(2000)\n",
    "\n",
    "num_episodes = 3\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not Done: \n",
    "        #take a random action\n",
    "        action = env.action_space.sample()\n",
    "        #implement the action \n",
    "        next_state, reward, Done, info = env.step(action)\n",
    "        #sum the rewards\n",
    "        total_rewards += reward\n",
    "        m.append(state,action,reward,next_state,Done)\n",
    "        state = next_state\n",
    "    print(\"The episode total rewards is \", total_rewards)\n",
    "\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "89c788cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL_Agent:\n",
    "    def __init__(self,env,memory_max_size =100_000,dicount= 0.99,lr_optim=1e-3):\n",
    "        self.env = env\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        self.dicount = dicount\n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.decay_rate_eps = 0.993\n",
    "        self.min_eps = 0.05\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.reply_memory = MemoryReplay(memory_max_size)\n",
    "        self.Q_action = Deep_Q_Network(self.num_states,self.num_action).to(self.device)\n",
    "        self.Q_target = Deep_Q_Network(self.num_states,self.num_action).to(self.device)\n",
    "        self.Q_target.eval() #will turn off any dropout or batch norm layer \n",
    "        #make sure both network has identical weights \n",
    "        self.update_target_weights()\n",
    "        \n",
    "        self.loss_fucntion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.Q_action.parameters(),lr=lr_optim)\n",
    "        \n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        self.Q_target.load_state_dict(self.Q_action.state_dict())\n",
    "    \n",
    "    def eps_greedy(self,states):\n",
    "        if np.random.rand()<self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            #act greedy\n",
    "            \n",
    "            #make sure the state are tensor in order to feed it to the network\n",
    "            if not torch.is_tensor(states):\n",
    "                states = torch.from_numpy(states[np.newaxis,:]).float().to(device)\n",
    "            with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "                action = self.Q_action(states)\n",
    "            max_action = torch.argmax(action).item()\n",
    "            return max_action\n",
    "    \n",
    "    def decay_eps(self):\n",
    "        self.eps = np.maximum(self.eps*self.decay_rate_eps,self.min_eps)\n",
    "    \n",
    "    def to_tensor(self,states, actions, rewards,next_states,Dones):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        Dones = torch.from_numpy(Dones).to(device)\n",
    "        return states, actions, rewards, next_states, Dones\n",
    "    def learnFromExperience(self,miniBatchSize): #hallucinations\n",
    "        if miniBatchSize <2:\n",
    "            raise ValueError(\"batch size must greater than 1\")\n",
    "        #make sure we have enough experiences \n",
    "        if len(self.reply_memory) < miniBatchSize:\n",
    "            return #not enough experience, sounds familiar right?\n",
    "        #else sample and learn\n",
    "        states, actions, rewards, next_states, Dones = self.reply_memory.sample(miniBatchSize)\n",
    "        #convert the result to tensor for model input \n",
    "        states, actions, rewards, next_states, Dones = self.to_tensor(states, actions, rewards, next_states, Dones)\n",
    "        #calculate the current Q estimation \n",
    "        Q_estimate = self.Q_action(states)\n",
    "        #obtain the q value for the actioned used in the experiences \n",
    "        Q_estimate_a = Q_estimate.gather(1, actions.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        #calculate the target value using --> rewards + discount* argmax_a Q(next_state, target_network_weight)\n",
    "        #the max gives both the max values and the indices \n",
    "        Q_target = self.Q_target(next_states).max(dim=1).values\n",
    "        #note that one the state is terminal, we only count the reward, therefore, we need to check if the state is Dones\n",
    "        #if Done is true, we should not calculate Q for the next states \n",
    "        Q_target[Dones] = 0.0 \n",
    "        #final target calculation\n",
    "        Q_target = rewards + self.dicount*Q_target\n",
    "        \n",
    "        #make sure the grad is zero \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #calculate the loss \n",
    "        loss=self.loss_fucntion(Q_target,Q_estimate_a)\n",
    "        #calcualte the gradient dL/dw\n",
    "        loss.backward()\n",
    "        #optimize using gradient decent\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_max_action(self,states):\n",
    "        self.Q_action.eval()\n",
    "        #make sure the state are tensor in order to feed it to the network\n",
    "        if not torch.is_tensor(states):\n",
    "            states = torch.from_numpy(states[np.newaxis,:]).to(device)\n",
    "        with torch.no_grad(): #will disable tracking the gradient --> reduce cpu/memory usage\n",
    "            action = self.Q_target(states)\n",
    "        max_action = torch.argmax(action).item()\n",
    "        return max_action\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "8ede0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_agent:\n",
    "    #Agent and environment interaction\n",
    "    def __init__(self, env,memory_max_size =10_000,dicount= 0.99,lr_optim=1e-3,update_freq =1000):\n",
    "        self.agent = DQL_Agent(env,memory_max_size ,dicount ,lr_optim)\n",
    "        self.update_freq = update_freq\n",
    "        self.steps = 0 \n",
    "    \n",
    "    def train_agent(self, num_episodes,batch_size):\n",
    "        for i in range(num_episodes):\n",
    "            state = self.agent.env.reset()\n",
    "            Done = False\n",
    "            total_rewards = 0\n",
    "            self.rewards = np.zeros(num_episodes)\n",
    "            self.moving_average = []\n",
    "            while not Done: \n",
    "                #take an action using the greedy policy\n",
    "                action = self.agent.eps_greedy(state)\n",
    "                #implement the action \n",
    "                next_state, reward, Done, info = self.agent.env.step(action)\n",
    "                #save the experience in the memory of the agent \n",
    "                self.agent.reply_memory.append(state,action,reward,next_state,Done)\n",
    "                #sum the rewards\n",
    "                total_rewards += reward\n",
    "                \n",
    "                #learn from experience (if there is enough)\n",
    "                self.agent.learnFromExperience(batch_size)\n",
    "                #update the tarqet network per the desired frequency \n",
    "                self.steps +=1 \n",
    "                if (self.steps % self.update_freq) == 0:\n",
    "                    self.agent.update_target_weights()\n",
    "                state = next_state\n",
    "            #append the rewards\n",
    "            self.rewards[i] = total_rewards\n",
    "            self.moving_average.append(np.mean(self.rewards[-100:]))\n",
    "            #update the eps \n",
    "            self.agent.decay_eps()\n",
    "            if i %10 == 0:\n",
    "                print(\"The episode {} total rewards is {}\".format(i+1, total_rewards))\n",
    "                print(len(self.agent.reply_memory))\n",
    "    def test_agent(self,env,render=False):\n",
    "        state = env.reset()\n",
    "        Done = False \n",
    "        total_rewards = 0\n",
    "        \n",
    "        while not Done: \n",
    "            action = self.agent.get_max_action(state)\n",
    "            next_state, reward,Done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = next_state\n",
    "        print(\"The episode total rewards is \", total_rewards)\n",
    "        env.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "51a2dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode 1 total rewards is -83.48552272891344\n",
      "86\n",
      "The episode 11 total rewards is -93.68225262614358\n",
      "1052\n",
      "The episode 21 total rewards is -4.408354793649394\n",
      "2032\n",
      "The episode 31 total rewards is -246.6434031939541\n",
      "3059\n",
      "The episode 41 total rewards is -187.50530293427195\n",
      "4094\n",
      "The episode 51 total rewards is -120.815077347589\n",
      "5199\n",
      "The episode 61 total rewards is -110.68261276500253\n",
      "6507\n",
      "The episode 71 total rewards is -92.45140215619787\n",
      "7842\n",
      "The episode 81 total rewards is 23.989188263593533\n",
      "9269\n",
      "The episode 91 total rewards is -52.041594283426065\n",
      "10000\n",
      "The episode 101 total rewards is -26.757078775261476\n",
      "10000\n",
      "The episode 111 total rewards is -43.806606998942726\n",
      "10000\n",
      "The episode 121 total rewards is -61.39790847334486\n",
      "10000\n",
      "The episode 131 total rewards is 1.47430386640427\n",
      "10000\n",
      "The episode 141 total rewards is -61.522098252218484\n",
      "10000\n",
      "The episode 151 total rewards is -47.03472454214743\n",
      "10000\n",
      "The episode 161 total rewards is -277.85724990651045\n",
      "10000\n",
      "The episode 171 total rewards is -119.24417038971198\n",
      "10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [338]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m training_agent \u001b[38;5;241m=\u001b[39m Training_agent(env)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36mTraining_agent.train_agent\u001b[0;34m(self, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m total_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#learn from experience (if there is enough)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnFromExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#update the tarqet network per the desired frequency \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[0;32mIn [336]\u001b[0m, in \u001b[0;36mDQL_Agent.learnFromExperience\u001b[0;34m(self, miniBatchSize)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;66;03m#not enough experience, sounds familiar right?\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#else sample and learn\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m states, actions, rewards, next_states, Dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreply_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminiBatchSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#convert the result to tensor for model input \u001b[39;00m\n\u001b[1;32m     61\u001b[0m states, actions, rewards, next_states, Dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(states, actions, rewards, next_states, Dones)\n",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36mMemoryReplay.sample\u001b[0;34m(self, batch_size, to_tesnor)\u001b[0m\n\u001b[1;32m     32\u001b[0m actions \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)[indices]\n\u001b[1;32m     33\u001b[0m rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards)[indices]\n\u001b[0;32m---> 34\u001b[0m next_states \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m[indices]\n\u001b[1;32m     35\u001b[0m Dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDones)[indices]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states,actions, rewards, next_states, Dones\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "training_agent = Training_agent(env)\n",
    "training_agent.train_agent(300,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8ba259d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(training_agent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cdf4ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "The episode total rewards is  -720.0402136060908\n"
     ]
    }
   ],
   "source": [
    "print(training_agent.agent.eps)\n",
    "training_agent.test_agent(render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
