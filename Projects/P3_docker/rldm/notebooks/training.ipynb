{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d858fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9046fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2022-07-05 06:57:07,700\tWARNING services.py:1759 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=9.97gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:18815',\n",
       " 'object_store_address': '/tmp/ray/session_2022-07-05_06-57-07_192528_17353/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-07-05_06-57-07_192528_17353/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-07-05_06-57-07_192528_17353',\n",
       " 'metrics_export_port': 63541,\n",
       " 'node_id': 'b895acec01f5fc71ce3727ff2acfae37c57d4b3efb931badbb827316'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rldm.utils import system_tools as st\n",
    "n_cpus, n_gpus = st.get_cpu_gpu_count()\n",
    "debug = False\n",
    "ray.init(num_cpus=n_cpus, num_gpus=n_gpus, local_mode=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd9024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df70f3",
   "metadata": {},
   "source": [
    "## register the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a531ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfootball import env as fe\n",
    "from rldm.utils import football_tools as ft\n",
    "from ray.tune.registry import register_env\n",
    "import numpy as np\n",
    "num_players = 3\n",
    "shared_policy = False\n",
    "n_policies = 1 if shared_policy else num_players - 1 # hard-coding\n",
    "env_name = ft.n_players_to_env_name(num_players, True)\n",
    "register_env(env_name, lambda _: ft.RllibGFootball(env_name=env_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914e743",
   "metadata": {},
   "source": [
    "## configuration for the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016f1b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bafe233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space, act_space = ft.get_obs_act_space(env_name)\n",
    "\n",
    "def gen_policy(idx):\n",
    "    return (None, obs_space[f'player_{idx}'], act_space[f'player_{idx}'], {})\n",
    "\n",
    "policies = {\n",
    "        'agent_{}'.format(idx): gen_policy(idx) for idx in range(n_policies)\n",
    "    }\n",
    "\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "policy_mapping_fn = lambda agent_id, episode, **kwargs: \\\n",
    "        policy_ids[0 if len(policy_ids) == 1 else int(agent_id.split('_')[1])]\n",
    "#in case of using a indiviual policy\n",
    "default_multiagent = {\n",
    "        'policies': policies,\n",
    "        'policy_mapping_fn': policy_mapping_fn,\n",
    "    }\n",
    "#in case of using a shared policy \n",
    "shared_policy = {'agent_0': gen_policy(0)}\n",
    "shared_policy_mapping_fn = lambda agent_id, episode, **kwargs: 'agent_0'\n",
    "shared_multiagent = {\n",
    "    'policies': shared_policy,\n",
    "    'policy_mapping_fn': shared_policy_mapping_fn,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b35ef2",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f868dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': '3_vs_3_auto_GK', 'evaluation_interval': 10, 'evaluation_num_episodes': 20, 'rollout_fragment_length': 100, 'train_batch_size': 2800, 'num_gpus': 1, 'num_workers': 19, 'multiagent': <ray.tune.sample.Categorical object at 0x7f41fdd86750>, 'Q_model': {'fcnet_hiddens': <ray.tune.sample.Function object at 0x7f41fddbb9d0>}, 'policy_model': {'fcnet_hiddens': <ray.tune.sample.Function object at 0x7f41fddbbf10>}, 'tau': <ray.tune.sample.Float object at 0x7f41fddbbe90>, 'optimization': {'actor_learning_rate': <ray.tune.sample.Float object at 0x7f41fddbbf50>, 'critic_learning_rate': <ray.tune.sample.Float object at 0x7f41fdd86a50>, 'entropy_learning_rate': <ray.tune.sample.Float object at 0x7f41fdd86910>}, '_deterministic_loss': <ray.tune.sample.Categorical object at 0x7f41fdd86e10>}\n"
     ]
    }
   ],
   "source": [
    "from rldm.utils.collection_tools import deep_merge\n",
    "import random\n",
    "use_tune_config = True\n",
    "config = {\"env\":env_name,\n",
    "           \"evaluation_interval\":10,\n",
    "           \"evaluation_num_episodes\":20,\n",
    "            \"rollout_fragment_length\": 100,\n",
    "          \"train_batch_size\": 2_800,\n",
    "           \"num_gpus\":n_gpus,\n",
    "           \"num_workers\":n_cpus-1,\n",
    "           'multiagent': default_multiagent,}\n",
    "\n",
    "if use_tune_config:\n",
    "    tune_config = {\n",
    "        \"Q_model\": {\n",
    "            \"fcnet_hiddens\": tune.sample_from(\n",
    "                    lambda _: random.sample([\n",
    "                        [256, 256],\n",
    "                        [128, 256],\n",
    "                        [256, 128],\n",
    "                        [128, 128],\n",
    "                    ], 1)[0])\n",
    "        },\n",
    "        \n",
    "        \"policy_model\": {\n",
    "            \"fcnet_hiddens\": tune.sample_from(\n",
    "                    lambda _: random.sample([\n",
    "                        [256, 256],\n",
    "                        [128, 256],\n",
    "                        [256, 128],\n",
    "                        [128, 128],\n",
    "                    ], 1)[0])\n",
    "        },\n",
    "        \"tau\": tune.uniform(0.00005, 0.0005),\n",
    "        # === Optimization ===\n",
    "        \"optimization\": {\n",
    "            \"actor_learning_rate\": tune.uniform(0.001, 1e-5),\n",
    "            \"critic_learning_rate\": tune.uniform(0.001, 1e-5),\n",
    "            \"entropy_learning_rate\": tune.uniform(0.001, 1e-5),\n",
    "        },\n",
    "        \"_deterministic_loss\": tune.choice([\"tanh\", \"relu\"]),\n",
    "        \n",
    "        'multiagent': tune.choice([default_multiagent, shared_multiagent]),\n",
    "    }\n",
    "    config = deep_merge(config, tune_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cfcf6",
   "metadata": {},
   "source": [
    "## add a scheduler to terminate any bad trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8595d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "use_callbacks = False\n",
    "if use_callbacks:\n",
    "    config['callbacks'] = ft.FootballCallbacks\n",
    "\n",
    "use_scheduler = True\n",
    "n_timesteps =20_000_000\n",
    "scheduler = None\n",
    "stop = {\n",
    "    \"timesteps_total\": n_timesteps,\n",
    "}\n",
    "if use_scheduler: \n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr='timesteps_total',\n",
    "        metric='episode_reward_mean',\n",
    "        mode='max',\n",
    "        max_t=n_timesteps,\n",
    "        grace_period=int(n_timesteps*0.10),\n",
    "        reduction_factor=3,\n",
    "        brackets=1)\n",
    "    stop = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce364ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17353/3890767975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a536bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_vs_3_auto_GK_search_20000000_asha_tune\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "filename_stem = os.path.basename(__file__).split(\".\")[0]\n",
    "policy_type = 'search' if use_tune_config else \\\n",
    "    'shared' if n_policies == 1 else 'independent'\n",
    "scheduler_type = 'asha' if use_scheduler else 'fifo'\n",
    "config_type = 'tune' if use_tune_config else 'fixed'\n",
    "experiment_name =f\"{filename_stem}_{env_name}_{policy_type}_{n_timesteps}_{scheduler_type}_{config_type}\"\n",
    "script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "local_dir = os.path.join(script_dir, '..', '..', 'logs')\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1969678",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad5b6c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>PENDING </td><td>     </td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m 2022-07-05 07:56:51,742\tINFO trainer.py:714 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m 2022-07-05 07:56:51,743\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m 2022-07-05 07:57:17,733\tWARNING deprecation.py:39 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m 2022-07-05 07:57:24,274\tINFO trainable.py:109 -- Trainable.setup took 32.571 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_3_vs_3_auto_GK_052d0_00000:\n",
      "  agent_timesteps_total: 3800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_07-57-27\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 364e664aea254cfa92b79839278a231c\n",
      "  hostname: b87bc6aeba4a\n",
      "  info:\n",
      "    last_target_update_ts: 1900\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9440126419067383\n",
      "          alpha_loss: 0.0\n",
      "          alpha_value: 1.0\n",
      "          critic_loss: 2.4138424396514893\n",
      "          max_q: 0.012492471374571323\n",
      "          mean_q: -0.0004241162387188524\n",
      "          mean_td_error: 2.9138424396514893\n",
      "          min_q: -0.015866102650761604\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.9125385 2.9140475 2.9143581 ... 2.9121308 2.9132519 2.9149926]'\n",
      "        train: null\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9446167945861816\n",
      "          alpha_loss: 0.0\n",
      "          alpha_value: 1.0\n",
      "          critic_loss: 2.4139223098754883\n",
      "          max_q: 0.016889724880456924\n",
      "          mean_q: 0.0001795673306332901\n",
      "          mean_td_error: 2.9139223098754883\n",
      "          min_q: -0.008438616059720516\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.9139009 2.912618  2.9156904 ... 2.9168382 2.9133158 2.9153576]'\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 3800\n",
      "    num_agent_steps_trained: 5600\n",
      "    num_steps_sampled: 1900\n",
      "    num_steps_trained: 2800\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 19\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.125\n",
      "    gpu_util_percent0: 0.0025\n",
      "    ram_util_percent: 53.0\n",
      "    vram_util_percent0: 0.4608154296875\n",
      "  pid: 22153\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 3.472827672958374\n",
      "  time_this_iter_s: 3.472827672958374\n",
      "  time_total_s: 3.472827672958374\n",
      "  timers:\n",
      "    learn_throughput: 4278.734\n",
      "    learn_time_ms: 654.399\n",
      "    load_throughput: 29482.776\n",
      "    load_time_ms: 94.971\n",
      "    update_time_ms: 16.275\n",
      "  timestamp: 1657007847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1900\n",
      "  training_iteration: 1\n",
      "  trial_id: 052d0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 20.0/20 CPUs, 1.0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>RUNNING </td><td>172.17.0.2:22153</td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.47283</td><td style=\"text-align: right;\">1900</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_3_vs_3_auto_GK_052d0_00000:\n",
      "  agent_timesteps_total: 15200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_07-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 158.69565217391303\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.5043478258278059\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 23\n",
      "  experiment_id: 364e664aea254cfa92b79839278a231c\n",
      "  hostname: b87bc6aeba4a\n",
      "  info:\n",
      "    last_target_update_ts: 7600\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.952843427658081\n",
      "          alpha_loss: -1.7664715414866805e-05\n",
      "          alpha_value: 0.9997000694274902\n",
      "          critic_loss: 2.395547866821289\n",
      "          max_q: 0.07300771772861481\n",
      "          mean_q: 0.009286877699196339\n",
      "          mean_td_error: 2.8950254917144775\n",
      "          min_q: 0.0038774393033236265\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.903942  2.9058862 2.9039352 ... 2.9019675 2.904931  2.9013946]'\n",
      "        train: null\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9537713527679443\n",
      "          alpha_loss: -1.766484092513565e-05\n",
      "          alpha_value: 0.9997000694274902\n",
      "          critic_loss: 2.398775815963745\n",
      "          max_q: 0.08237091451883316\n",
      "          mean_q: 0.010214538313448429\n",
      "          mean_td_error: 2.8981008529663086\n",
      "          min_q: 0.00407329760491848\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.9068027 2.9021938 2.9010293 ... 2.9040976 2.904117  2.9029021]'\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 15200\n",
      "    num_agent_steps_trained: 22400\n",
      "    num_steps_sampled: 7600\n",
      "    num_steps_trained: 11200\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 19\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.4\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 53.4\n",
      "    vram_util_percent0: 0.4697265625\n",
      "  pid: 22153\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2608695652173913\n",
      "    agent_1: -0.24347826061041458\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20606496044331535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.44008959237529\n",
      "    mean_inference_ms: 4.693427266118463\n",
      "    mean_raw_obs_processing_ms: 0.5536464357202044\n",
      "  time_since_restore: 9.608132362365723\n",
      "  time_this_iter_s: 2.0193302631378174\n",
      "  time_total_s: 9.608132362365723\n",
      "  timers:\n",
      "    learn_throughput: 12937.863\n",
      "    learn_time_ms: 216.419\n",
      "    load_throughput: 102123.072\n",
      "    load_time_ms: 27.418\n",
      "    update_time_ms: 13.97\n",
      "  timestamp: 1657007854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7600\n",
      "  training_iteration: 4\n",
      "  trial_id: 052d0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 20.0/20 CPUs, 1.0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>RUNNING </td><td>172.17.0.2:22153</td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         9.60813</td><td style=\"text-align: right;\">7600</td><td style=\"text-align: right;\">-0.504348</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           158.696</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_3_vs_3_auto_GK_052d0_00000:\n",
      "  agent_timesteps_total: 26600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_07-57-40\n",
      "  done: false\n",
      "  episode_len_mean: 208.9111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.3466666665342119\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 45\n",
      "  experiment_id: 364e664aea254cfa92b79839278a231c\n",
      "  hostname: b87bc6aeba4a\n",
      "  info:\n",
      "    last_target_update_ts: 13300\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9620001316070557\n",
      "          alpha_loss: -3.533079507178627e-05\n",
      "          alpha_value: 0.9994001388549805\n",
      "          critic_loss: 2.3870372772216797\n",
      "          max_q: 0.12748725712299347\n",
      "          mean_q: 0.01932591013610363\n",
      "          mean_td_error: 2.8860080242156982\n",
      "          min_q: 0.012125525623559952\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.8946853 2.8960662 2.8965225 ... 2.8918705 2.896205  2.8940802]'\n",
      "        train: null\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9630067348480225\n",
      "          alpha_loss: -3.533148264978081e-05\n",
      "          alpha_value: 0.9994001388549805\n",
      "          critic_loss: 2.383511543273926\n",
      "          max_q: 0.13567401468753815\n",
      "          mean_q: 0.020332306623458862\n",
      "          mean_td_error: 2.8818061351776123\n",
      "          min_q: 0.01203181967139244\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.8953528 2.8974338 2.895935  ... 2.893115  2.8918214 2.8904223]'\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 26600\n",
      "    num_agent_steps_trained: 39200\n",
      "    num_steps_sampled: 13300\n",
      "    num_steps_trained: 19600\n",
      "    num_target_updates: 7\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 19\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 53.6\n",
      "    vram_util_percent0: 0.4642333984375\n",
      "  pid: 22153\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17777777777777778\n",
      "    agent_1: -0.16888888875643412\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20485469626777283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.377637189084986\n",
      "    mean_inference_ms: 4.533470075515623\n",
      "    mean_raw_obs_processing_ms: 0.5049475632991224\n",
      "  time_since_restore: 16.034510612487793\n",
      "  time_this_iter_s: 2.0505785942077637\n",
      "  time_total_s: 16.034510612487793\n",
      "  timers:\n",
      "    learn_throughput: 17338.809\n",
      "    learn_time_ms: 161.487\n",
      "    load_throughput: 157768.358\n",
      "    load_time_ms: 17.748\n",
      "    update_time_ms: 17.707\n",
      "  timestamp: 1657007860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13300\n",
      "  training_iteration: 7\n",
      "  trial_id: 052d0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 20.0/20 CPUs, 1.0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>RUNNING </td><td>172.17.0.2:22153</td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         16.0345</td><td style=\"text-align: right;\">13300</td><td style=\"text-align: right;\">-0.346667</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           208.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 07:57:55,936\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SAC_3_vs_3_auto_GK_052d0_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_07-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 225.95774647887325\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.3478873236708238\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 71\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 225.52\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: -0.7919999998807907\n",
      "    episode_reward_min: -2.0\n",
      "    episodes_this_iter: 25\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 242\n",
      "      - 104\n",
      "      - 384\n",
      "      - 120\n",
      "      - 257\n",
      "      - 72\n",
      "      - 192\n",
      "      - 188\n",
      "      - 174\n",
      "      - 200\n",
      "      - 435\n",
      "      - 77\n",
      "      - 501\n",
      "      - 78\n",
      "      - 230\n",
      "      - 148\n",
      "      - 501\n",
      "      - 78\n",
      "      - 409\n",
      "      - 147\n",
      "      - 203\n",
      "      - 79\n",
      "      - 124\n",
      "      - 194\n",
      "      - 501\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -1.8999999985098839\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.8999999985098839\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_agent_0_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_agent_1_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -0.8999999985098839\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -0.8999999985098839\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      agent_0: 0.0\n",
      "      agent_1: 0.0\n",
      "    policy_reward_mean:\n",
      "      agent_0: -0.4\n",
      "      agent_1: -0.3919999998807907\n",
      "    policy_reward_min:\n",
      "      agent_0: -1.0\n",
      "      agent_1: -1.0\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.12096307615498994\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.280831209626851\n",
      "      mean_inference_ms: 6.746055260049596\n",
      "      mean_raw_obs_processing_ms: 0.24642515613754118\n",
      "  experiment_id: 364e664aea254cfa92b79839278a231c\n",
      "  hostname: b87bc6aeba4a\n",
      "  info:\n",
      "    last_target_update_ts: 19000\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.9711642265319824\n",
      "          alpha_loss: -5.2996936574345455e-05\n",
      "          alpha_value: 0.9991004467010498\n",
      "          critic_loss: 2.372973918914795\n",
      "          max_q: 0.18591079115867615\n",
      "          mean_q: 0.029371829703450203\n",
      "          mean_td_error: 2.8713135719299316\n",
      "          min_q: 0.02021688222885132\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.8824456 2.835108  2.882975  ... 2.8821397 2.8802595 2.878104 ]'\n",
      "        train: null\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          actor_loss: -2.972583055496216\n",
      "          alpha_loss: -5.299739495967515e-05\n",
      "          alpha_value: 0.9991004467010498\n",
      "          critic_loss: 2.37101411819458\n",
      "          max_q: 0.1952502727508545\n",
      "          mean_q: 0.030791357159614563\n",
      "          mean_td_error: 2.869190216064453\n",
      "          min_q: 0.020072925835847855\n",
      "          model: {}\n",
      "          target_entropy: 2.8855501556396486\n",
      "        td_error: '[2.8757186 2.8630013 2.880835  ... 2.8847394 2.8840938 2.8887491]'\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 28000\n",
      "    num_target_updates: 10\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 19\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.164634146341463\n",
      "    gpu_util_percent0: 0.16512195121951218\n",
      "    ram_util_percent: 53.890243902439\n",
      "    vram_util_percent0: 0.4681441144245427\n",
      "  pid: 22153\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.20000000298023224\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18028169009886996\n",
      "    agent_1: -0.16760563357195385\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20399194162433376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.35725457801629\n",
      "    mean_inference_ms: 4.43246148678169\n",
      "    mean_raw_obs_processing_ms: 0.4955583621792773\n",
      "  time_since_restore: 86.7453362941742\n",
      "  time_this_iter_s: 66.5640332698822\n",
      "  time_total_s: 86.7453362941742\n",
      "  timers:\n",
      "    learn_throughput: 20865.408\n",
      "    learn_time_ms: 134.193\n",
      "    load_throughput: 199328.748\n",
      "    load_time_ms: 14.047\n",
      "    update_time_ms: 16.305\n",
      "  timestamp: 1657007931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 10\n",
      "  trial_id: 052d0_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 20.0/20 CPUs, 1.0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>RUNNING </td><td>172.17.0.2:22153</td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         86.7453</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.347887</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           225.958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 20.0/20 CPUs, 1.0/1 GPUs, 0.0/18.12 GiB heap, 0.0/9.06 GiB objects<br>Result logdir: /home/jovyan/ray_results/SAC<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th>Q_model/fcnet_hiddens  </th><th>_deterministic_loss  </th><th>multiagent  </th><th style=\"text-align: right;\">  optimization/actor_learning_rate</th><th style=\"text-align: right;\">  optimization/critic_learning_rate</th><th style=\"text-align: right;\">  optimization/entropy_learning_rate</th><th>policy_model/fcnet_hiddens  </th><th style=\"text-align: right;\">        tau</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_3_vs_3_auto_GK_052d0_00000</td><td>RUNNING </td><td>172.17.0.2:22153</td><td>[128, 256]             </td><td>tanh                 </td><td>{&#x27;policies&#x27;: {&#x27;agent_0&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {}), &#x27;agent_1&#x27;: (None, Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       " -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
       " inf inf inf inf inf inf inf], (43,), float32), Discrete(19), {})}, &#x27;policy_mapping_fn&#x27;: &lt;function &lt;lambda&gt; at 0x7f42000e1200&gt;}             </td><td style=\"text-align: right;\">                       0.000844197</td><td style=\"text-align: right;\">                        0.000504442</td><td style=\"text-align: right;\">                         0.000405481</td><td>[256, 256]                  </td><td style=\"text-align: right;\">0.000197781</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         86.7453</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.347887</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           225.958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m 2022-07-05 07:58:51,645\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/tune/trainable.py\", line 178, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 637, in train\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     result = Trainable.train(self)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/tune/trainable.py\", line 237, in train\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     res = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1075, in build_union\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     item = next(it)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 1615, in get\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     object_refs, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 348, in get_objects\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     object_refs, self.current_task_id, timeout_ms)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22153)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22181)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m 2022-07-05 07:58:51,632\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 629, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     active_episodes=active_episodes,\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1031, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     episodes=[active_episodes[t.env_id] for t in eval_data])\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py\", line 428, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 44, in get\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     run_metadata_ptr)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1147, in _run\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     subfeed_dtype, subfeed_val) != subfeed_val:\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22189)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m 2022-07-05 07:58:51,645\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22186)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m 2022-07-05 07:58:51,632\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 629, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     active_episodes=active_episodes,\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1031, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     episodes=[active_episodes[t.env_id] for t in eval_data])\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py\", line 428, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 44, in get\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     run_metadata_ptr)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     feed_dict_tensor, options, run_metadata)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     run_metadata)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1377, in _do_call\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     target_list, run_metadata)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1455, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     run_metadata)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22183)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m 2022-07-05 07:58:51,634\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 629, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     active_episodes=active_episodes,\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1031, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     episodes=[active_episodes[t.env_id] for t in eval_data])\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py\", line 428, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 44, in get\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     run_metadata_ptr)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1176, in _run\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 504, in __init__\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     fetch.op.type == 'GetSessionHandleV2')):\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22182)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m 2022-07-05 07:58:51,645\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22192)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m 2022-07-05 07:58:51,647\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22198)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m 2022-07-05 07:58:51,632\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 629, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     active_episodes=active_episodes,\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1031, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     episodes=[active_episodes[t.env_id] for t in eval_data])\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py\", line 428, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 44, in get\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     run_metadata_ptr)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1163, in _run\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 754, in get_shape\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     return self.shape\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 569, in shape\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     if self._shape_val is None:\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22207)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22203)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22210)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 149, in _get_actions\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     right_player_position)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22218)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m 2022-07-05 07:58:51,645\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22215)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m 2022-07-05 07:58:51,634\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 629, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     active_episodes=active_episodes,\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 1031, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     episodes=[active_episodes[t.env_id] for t in eval_data])\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py\", line 428, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 44, in get\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/utils/tf_run_builder.py\", line 89, in run_timeline\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     run_metadata_ptr)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1163, in _run\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1147, in is_compatible_with\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     if self.rank != other.rank:\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 837, in rank\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     if self._dims is not None:\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22221)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22223)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m 2022-07-05 07:58:51,643\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 246, in step\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     single_observation = copy.deepcopy(self._observation)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     y = copier(x, memo)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/copy.py\", line 152, in deepcopy\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     try:\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22227)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m 2022-07-05 07:58:51,647\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22269)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m 2022-07-05 07:58:51,645\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22243)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m 2022-07-05 07:58:51,656\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 423, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     obs, rewards, dones, infos = env.step(agent_dict)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/mnt/rldm/utils/football_tools.py\", line 111, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     o, r, d, i = self.env.step(actions)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 248, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 282, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env.py\", line 176, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     _, reward, done, info = self._env.step(self._get_actions())\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/gfootball/env/football_env_core.py\", line 193, in step\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     self._env.step()\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=22206)\u001b[0m SystemExit: 1\n",
      "2022-07-05 07:58:51,841\tERROR tune.py:557 -- Trials did not complete: [SAC_3_vs_3_auto_GK_052d0_00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 07:58:51,843\tINFO tune.py:561 -- Total run time: 124.16 seconds (123.89 seconds for the tuning loop).\n",
      "2022-07-05 07:58:51,873\tWARNING tune.py:566 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f42014a9d10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tune.run(\"SAC\",\n",
    "        name=experiment_name,\n",
    "        reuse_actors=False,\n",
    "        scheduler=scheduler,\n",
    "        raise_on_failed_trial=True,\n",
    "        fail_fast=True,\n",
    "        max_failures=0,\n",
    "        num_samples=n_samples,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=100,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=local_dir,\n",
    "        config=config,\n",
    "        verbose=1 if not debug else 3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = a.get_best_checkpoint(a.get_best_trial(\"episode_reward_mean\", \"max\"), \"episode_reward_mean\", \"max\")\n",
    "print('Best checkpoint found:', checkpoint_path)\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
