{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d9856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "from argparse import RawTextHelpFormatter\n",
    "\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.cloudpickle as cloudpickle\n",
    "import torch\n",
    "from gfootball import env as fe\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.spaces.space_utils import flatten_to_single_ndarray\n",
    "from ray.tune.registry import get_trainable_cls, register_env,  register_trainable\n",
    "from rldm.utils import football_tools as ft\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import system_tools as st\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff263097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gfootball.env.wrappers import CheckpointRewardWrapper\n",
    "\n",
    "# def reward_modified(self,reward):\n",
    "#     '''\n",
    "#     o['active'] => 1 or 2 \n",
    "#     '''\n",
    "#     #change checkpoints to only 4\n",
    "#     #self._num_checkpoints = 4\n",
    "    \n",
    "#     #assuming max of 500 steps \n",
    "#     passing_rewards = 0.002 \n",
    "#     shoting_rewards = 0.1\n",
    "#     not_owning_ball_team = -0.001\n",
    "#     owning_ball = 0.001\n",
    "#     observation = self.env.unwrapped.observation()\n",
    "#     actions = self.env.unwrapped._get_actions()\n",
    "#     #print(observation)\n",
    "#     if observation is None:\n",
    "#         return reward\n",
    "\n",
    "#     assert len(reward) == len(observation)\n",
    "\n",
    "#     for rew_index in range(len(reward)):\n",
    "#         o = observation[rew_index]\n",
    "#         a = actions[rew_index]\n",
    "#         if reward[rew_index] == 1: #scoring a goal\n",
    "#             reward[rew_index] += self._checkpoint_reward * (\n",
    "#                 self._num_checkpoints -\n",
    "#                 self._collected_checkpoints.get(rew_index, 0))\n",
    "#             self._collected_checkpoints[rew_index] = self._num_checkpoints\n",
    "#             #add extra points to compensate for the extra postive rewards\n",
    "#             reward[rew_index] += 0.5\n",
    "#             continue\n",
    "        \n",
    "#         #check if the right team does own the ball, this should be the first objective in the game\n",
    "#         if o['ball_owned_team'] != 0:\n",
    "#             #reward[rew_index] += not_owning_ball_team\n",
    "#             #player need to get closer to get the ball \n",
    "#             player_pos = o['right_team'][o['active']]\n",
    "#             player_ball_dist = ((o['ball'][0] - player_pos[0]) ** 2 + (o['ball'][1]-player_pos[1]) ** 2) ** 0.5\n",
    "#             reward[rew_index] += player_ball_dist*not_owning_ball_team\n",
    "#         else:\n",
    "#             #they should positive reward for owning the ball too\n",
    "#             reward[rew_index] += owning_ball\n",
    "            \n",
    "        \n",
    "        \n",
    "#         # Check if the active player has the ball.\n",
    "#         if ('ball_owned_team' not in o or\n",
    "#           o['ball_owned_team'] != 0 or\n",
    "#           'ball_owned_player' not in o or\n",
    "#           o['ball_owned_player'] != o['active']):\n",
    "#             continue\n",
    "#         #now, we know that the active player has the ball\n",
    "#         #reward passing if the second player is closer to goal\n",
    "#         #reward shooting if active player is near the goal\n",
    "#         passing_thre = 0.05\n",
    "#         position_second_player = o['right_team'][1 if o['active']==2 else 2]\n",
    "#         dist_second_player = ((position_second_player[0] - 1) ** 2 + position_second_player[1] ** 2) ** 0.5\n",
    "#         d = ((o['ball'][0] - 1) ** 2 + o['ball'][1] ** 2) ** 0.5 #ball dist to goal (the active player too)\n",
    "#         player_distances = ((o['ball'][0] - position_second_player[0]) ** 2 + (o['ball'][1]-position_second_player[1]) ** 2) ** 0.5\n",
    "#         #if the ball is very close, shooting would be better than passing\n",
    "#         if dist_second_player + passing_thre < d and d > 0.3: \n",
    "#             if player_distances < 0.2 and a == 11: #short pass when they are near each other\n",
    "#                 reward[rew_index] += passing_rewards\n",
    "#             elif player_distances > 0.2 and (a == 9 or a==10):\n",
    "#                 reward[rew_index] += passing_rewards\n",
    "#         #now check if the ball is near target, then rewarding shotting \n",
    "#         if d < 0.4 and a ==12:\n",
    "#             reward[rew_index] += shoting_rewards\n",
    "            \n",
    "\n",
    "#         # Collect the checkpoints.\n",
    "#         # We give reward for distance 1 to 0.2.\n",
    "#         while (self._collected_checkpoints.get(rew_index, 0) <\n",
    "#              self._num_checkpoints):\n",
    "#             if self._num_checkpoints == 1:\n",
    "#                 threshold = 0.99 - 0.8\n",
    "#             else:\n",
    "#                 threshold = (0.99 - 0.8 / (self._num_checkpoints - 1) *\n",
    "#                            self._collected_checkpoints.get(rew_index, 0))\n",
    "#             if d > threshold:\n",
    "#                 break\n",
    "#             reward[rew_index] += self._checkpoint_reward\n",
    "#             self._collected_checkpoints[rew_index] = (\n",
    "#                 self._collected_checkpoints.get(rew_index, 0) + 1)\n",
    "#     return reward\n",
    "\n",
    "# CheckpointRewardWrapper.reward = reward_modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805dfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultMapping(collections.defaultdict):\n",
    "    \"\"\"default_factory now takes as an argument the missing key.\"\"\"\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        self[key] = value = self.default_factory(key)\n",
    "        return value\n",
    "\n",
    "STAT_FUNC = {\n",
    "    'min': np.min,\n",
    "    'mean': np.mean,\n",
    "    'median': np.median,\n",
    "    'max': np.max,\n",
    "    'std': np.std,\n",
    "    'var': np.var,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c616bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/football/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945ce600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 2186 Jun 22 17:19 /mnt/logs/baselines/baseline_1/params.json\n",
      "-rw-r--r-- 1 jovyan users 3090 Jun 22 17:19 /mnt/logs/baselines/baseline_1/params.pkl\n",
      "-rw-r--r-- 1 jovyan users 2199 Jun 22 17:19 /mnt/logs/baselines/baseline_2/params.json\n",
      "-rw-r--r-- 1 jovyan users 3095 Jun 22 17:19 /mnt/logs/baselines/baseline_2/params.pkl\n",
      "-rw-r--r-- 1 jovyan users 2185 Jun 22 17:19 /mnt/logs/baselines/baseline_3/params.json\n",
      "-rw-r--r-- 1 jovyan users 3090 Jun 22 17:19 /mnt/logs/baselines/baseline_3/params.pkl\n",
      "\n",
      "/mnt/logs/baselines/baseline_1/checkpoint_0:\n",
      "total 14776\n",
      "-rw-r--r-- 1 jovyan users 15122984 Jun 22 17:19 checkpoint-0\n",
      "-rw-r--r-- 1 jovyan users      184 Jun 22 17:19 checkpoint-0.tune_metadata\n",
      "\n",
      "/mnt/logs/baselines/baseline_2/checkpoint_0:\n",
      "total 13876\n",
      "-rw-r--r-- 1 jovyan users 14201365 Jun 22 17:19 checkpoint-0\n",
      "-rw-r--r-- 1 jovyan users      184 Jun 22 17:19 checkpoint-0.tune_metadata\n",
      "\n",
      "/mnt/logs/baselines/baseline_3/checkpoint_0:\n",
      "total 10932\n",
      "-rw-r--r-- 1 jovyan users 11187733 Jun 22 17:19 checkpoint-0\n",
      "-rw-r--r-- 1 jovyan users      184 Jun 22 17:19 checkpoint-0.tune_metadata\n"
     ]
    }
   ],
   "source": [
    "!ls -l /mnt/logs/baselines/**/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64485f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT TO LOAD\n",
    "# here are some example checkpoints from the baseline teams...\n",
    "baseline1 = \"/mnt/logs/baselines/baseline_1/checkpoint_0/checkpoint-0\"\n",
    "baseline2 = \"/mnt/logs/baselines/baseline_2/checkpoint_0/checkpoint-0\"\n",
    "baseline3 = \"/mnt/logs/baselines/baseline_3/checkpoint_0/checkpoint-0\"\n",
    "checkpoint_pc = \"/mnt/logs/train_agents_modified_rewards_3_vs_3_auto_GK_independent_50000000_fifo_fixed/PPO_3_vs_3_auto_GK_fc470_00000_0_2022-07-15_18-07-27/checkpoint_008100/checkpoint-8100\"\n",
    "# checkpoint_pc = \"/mnt/logs/train_agents_modified_rewards_3_vs_3_auto_GK_independent_50000000_fifo_fixed/PPO_3_vs_3_auto_GK_fc470_00000_0_2022-07-15_18-07-27/checkpoint_008951/checkpoint-8951\"\n",
    "# checkpoint_qmix = \"/mnt/logs/\n",
    "checkpoint_gcs = \"/mnt/logs/my_agent/checkpoint_008900/checkpoint-8900\"\n",
    "checkpoint_pc2 = \"/mnt/logs/my_agent_2/checkpoint_008800/checkpoint-8800-modified\"\n",
    "checkpoints = [baseline1,baseline2,baseline3,checkpoint_pc2]\n",
    "# checkpoints = [checkpoint_gcs]\n",
    "num_episodes = 10\n",
    "debug = False\n",
    "#algorithm = \"MADDPG\"\n",
    "algorithm = \"PPO\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647af080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:38308',\n",
       " 'object_store_address': '/tmp/ray/session_2022-07-23_19-24-26_780923_81841/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-07-23_19-24-26_780923_81841/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-07-23_19-24-26_780923_81841',\n",
       " 'metrics_export_port': 48403,\n",
       " 'node_id': '3802d9b381d6a60c263e11db6dff25d09eff70f6b35614a8ae5778d7'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True, log_to_driver=debug, include_dashboard=False,\n",
    "         local_mode=True, logging_level='DEBUG' if debug else 'ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b367048",
   "metadata": {},
   "source": [
    "## Matrices: Evalute all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653ec4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(agent,config,env,num_episodes=10):\n",
    "    players_idx = {'player_0':1,'player_1':2 }\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    eps_stats = {}\n",
    "    eps_stats['rewards_total'] = []\n",
    "    eps_stats['timesteps'] = []\n",
    "    eps_stats['score_reward'] = []\n",
    "    eps_stats['win_perc'] = []\n",
    "    eps_stats['undefeated_perc'] = []\n",
    "    eps_stats[\"num_passes_per_eps\"] = []\n",
    "    eps_stats[\"num_shots_per_eps\"] = []\n",
    "    eps_stats['possession_per_eps'] = []\n",
    "    eps_stats['regaining_possession_per_eps'] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "\n",
    "        done = False\n",
    "        eps_stats['rewards_total'].append({a:0.0 for a in obs})\n",
    "        eps_stats['timesteps'].append(-1)\n",
    "        eps_stats['score_reward'].append(0)\n",
    "        eps_stats['win_perc'].append(0)\n",
    "        eps_stats['undefeated_perc'].append(0)\n",
    "        possession = obs['player_0'][31]==1 #when obs[31]==1 it means the left team have the ball\n",
    "        passes = 0\n",
    "        shoots = 0\n",
    "        regains = 0\n",
    "        steps = 0 \n",
    "        possession_steps= 1 if possession==True else 0\n",
    "        infos = None\n",
    "        while not done:\n",
    "            steps +=1\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "                a_action_flatten = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action_flatten\n",
    "                prev_actions[player_id] = a_action_flatten\n",
    "                \n",
    "                \n",
    "                if infos==None:\n",
    "                    continue\n",
    "                    \n",
    "                infos_ = infos[player_id]['game_info']\n",
    "                if (infos_['ball_owned_team'] == 0 and infos_['ball_owned_player'] == players_idx[player_id]):\n",
    "#                     print(possession)\n",
    "                    if a_action in [9,10,11]:\n",
    "                        passes += 1\n",
    "                    elif a_action == 12: #shooting\n",
    "                        shoots +=1\n",
    "                \n",
    "            if infos!=None:\n",
    "                if possession:\n",
    "                    possession_steps +=1\n",
    "                if not possession_pre and possession: #if i didn't have the ball, then i regain it\n",
    "                    regains +=1 \n",
    "            \n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "            possession_pre = possession\n",
    "            possession = next_obs['player_0'][31]==1\n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "                eps_stats['rewards_total'][-1][player_id] += r\n",
    "            \n",
    "            obs = next_obs\n",
    "            eps_stats['timesteps'][-1] += 1\n",
    "\n",
    "        eps_stats[\"num_passes_per_eps\"].append(passes)\n",
    "        eps_stats[\"num_shots_per_eps\"].append(shoots)\n",
    "        eps_stats['possession_per_eps'].append(round(possession_steps/steps,2)*100)\n",
    "        eps_stats['regaining_possession_per_eps'].append(regains)\n",
    "        \n",
    "        eps_stats['score_reward'][-1] = infos['player_0']['score_reward']\n",
    "        game_result = \"loss\" if infos['player_0']['score_reward'] == -1 else \\\n",
    "            \"win\" if infos['player_0']['score_reward'] == 1 else \"tie\"\n",
    "        eps_stats['win_perc'][-1] = int(game_result == \"win\")\n",
    "        eps_stats['undefeated_perc'][-1] = int(game_result != \"loss\")\n",
    "        if debug:\n",
    "            print(f\"\\nEpisode #{eidx+1} ended with a {game_result}:\")\n",
    "            for p, r in eps_stats['rewards_total'][-1].items():\n",
    "                print(\"\\t{} got episode reward: {:.2f}\".format(p, r))\n",
    "            print(\"\\tTotal reward: {:.2f}\".format(sum(eps_stats['rewards_total'][-1].values())))\n",
    "\n",
    "    eps_stats['rewards_total'] = {k: [dic[k] for dic in eps_stats['rewards_total']] \\\n",
    "        for k in eps_stats['rewards_total'][0]}\n",
    "    print(\"\\n\\nAll trials completed:\")\n",
    "    if debug:\n",
    "        for stat_name, values in eps_stats.items():\n",
    "            print(f\"\\t{stat_name}:\")\n",
    "            if type(values) is dict:\n",
    "                for stat_name2, values2 in values.items():\n",
    "                    print(f\"\\t\\t{stat_name2}:\")\n",
    "                    for func_name, func in STAT_FUNC.items():\n",
    "                        print(\"\\t\\t\\t{}: {:.2f}\".format(func_name, func(values2)))\n",
    "            else:\n",
    "                for func_name, func in STAT_FUNC.items():\n",
    "                    print(\"\\t\\t{}: {:.2f}\".format(func_name, func(values)))\n",
    "\n",
    "    return eps_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08994281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(eval_fuc,checkpoint, num_episodes=10, algorithm=\"PPO\"):\n",
    "    assert checkpoint, \"A checkpoint string is required\"\n",
    "\n",
    "    # assert num_episodes <= 10, \"Must rollout a maximum of 10 episodes--rendering takes a very long time\"\n",
    "    # assert num_episodes >= 1, \"Must rollout at least 1 episode\"\n",
    "\n",
    "    n_cpus, _ = st.get_cpu_gpu_count()\n",
    "    assert n_cpus >= 2, \"Didn't find enough cpus\"\n",
    "\n",
    "    config = {}\n",
    "    config_dir = os.path.dirname(checkpoint)\n",
    "    config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "    assert os.path.exists(config_path), \"Could not find a proper config from checkpoint\"\n",
    "    with open(config_path, \"rb\") as f:\n",
    "        config = cloudpickle.load(f)\n",
    "\n",
    "    config[\"create_env_on_driver\"] = True\n",
    "    config[\"log_level\"] = 'DEBUG' if debug else 'ERROR' \n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"num_gpus\"] = 0\n",
    "    test_env_name=\"3_vs_3_auto_GK\" # this semester we're only doing 3v3 w auto GK\n",
    "    \n",
    "    \n",
    "    register_env(test_env_name, lambda _: ft.RllibGFootball(env_name=test_env_name))\n",
    "    cls = get_trainable_cls(algorithm)\n",
    "    agent = cls(env=test_env_name, config=config)\n",
    "    agent.restore(checkpoint)\n",
    "\n",
    "    try:\n",
    "        del env\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    env = ft.RllibGFootball(env_name=config['env'], write_video=False, render=False)\n",
    "    \n",
    "    est_state = eval_fuc(agent,config,env,num_episodes)\n",
    "\n",
    "\n",
    "    return est_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5342318",
   "metadata": {},
   "source": [
    "## The below code was needed to modified the saved checkpoint as it didn't contain 'train_exec_impl' which gave error when restoring the checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a4a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# checkpoint1 = \"/mnt/logs/my_agent_2/checkpoint_008800/checkpoint-8800-modified\"\n",
    "# extra_data1 = pickle.load(open(checkpoint1, \"rb\"))\n",
    "# extra_data2 = pickle.load(open(checkpoints[-2], \"rb\"))\n",
    "# #the training data for this checkpint was based on baselin 3\n",
    "# extra_data1['train_exec_impl'] = extra_data2['train_exec_impl'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26425342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"/mnt/logs/my_agent_2/checkpoint_008800/checkpoint-8800-modified\"\n",
    "# pickle.dump(extra_data1, open(checkpoint_path, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb00b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_states = pickle.load(open(checkpoint_path, \"rb\"))\n",
    "# extra_data1['train_exec_impl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ad14e",
   "metadata": {},
   "source": [
    "## evaluate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bdf177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [02:26<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  0\n",
      "wining per:  0.35\n",
      "player_0 average reward:  0.62\n",
      "player_1 average reward:  0.7439999897778035\n",
      "undefeated per:  0.92\n",
      "score average:  0.27\n",
      "epsidoe len average:  100.83\n",
      "passes average per eps:  0.02\n",
      "shooting average per eps:  1.35\n",
      "possession perc average per eps:  33.27\n",
      "regaining ball average per eps:  0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [03:29<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  1\n",
      "wining per:  0.33\n",
      "player_0 average reward:  0.6370000004768371\n",
      "player_1 average reward:  0.7630000211298466\n",
      "undefeated per:  0.96\n",
      "score average:  0.29\n",
      "epsidoe len average:  135.07\n",
      "passes average per eps:  0.06\n",
      "shooting average per eps:  1.11\n",
      "possession perc average per eps:  36.09\n",
      "regaining ball average per eps:  0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [03:10<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  2\n",
      "wining per:  0.42\n",
      "player_0 average reward:  0.79\n",
      "player_1 average reward:  0.8780000093579292\n",
      "undefeated per:  0.95\n",
      "score average:  0.37\n",
      "epsidoe len average:  111.71\n",
      "passes average per eps:  0.02\n",
      "shooting average per eps:  1.16\n",
      "possession perc average per eps:  43.6\n",
      "regaining ball average per eps:  0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [04:32<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  3\n",
      "wining per:  0.52\n",
      "player_0 average reward:  1.0150000000745059\n",
      "player_1 average reward:  1.0960000091046096\n",
      "undefeated per:  0.97\n",
      "score average:  0.49\n",
      "epsidoe len average:  138.62\n",
      "passes average per eps:  0.16\n",
      "shooting average per eps:  0.75\n",
      "possession perc average per eps:  48.81\n",
      "regaining ball average per eps:  1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "states_mean = {}\n",
    "states_mean['wining_rate'] = []\n",
    "states_mean['player_0_reward'] = []\n",
    "states_mean['player_1_reward'] = []\n",
    "states_mean['undefeated'] = []\n",
    "states_mean['len'] = []\n",
    "states_mean['score'] = []\n",
    "states_mean['passes'] = []\n",
    "states_mean['shooting'] = []\n",
    "states_mean[\"possession\"] = []\n",
    "states_mean[\"interception\"] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evaluate_all,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    stats.append(stat)\n",
    "    wining_rate = stat[\"win_perc\"].count(1)/ len(stat[\"win_perc\"])\n",
    "    player_0_reward_mean = np.mean(stat['rewards_total']['player_0'])\n",
    "    player_1_reward_mean = np.mean(stat['rewards_total']['player_1'])\n",
    "    undefeated_perc = stat[\"undefeated_perc\"].count(1)/ len(stat[\"undefeated_perc\"])\n",
    "    score_mean= np.mean(stat[\"score_reward\"])\n",
    "    len_mean = np.mean(stat[\"timesteps\"])\n",
    "    passes_mean = np.mean(stat[\"num_passes_per_eps\"])\n",
    "    shooting_mean =  np.mean(stat[\"num_shots_per_eps\"])\n",
    "    possession_mean = np.mean(stat[\"possession_per_eps\"])\n",
    "    interception_mean = np.mean(stat[\"regaining_possession_per_eps\"])\n",
    "    print(\"checkpint index: \",i)\n",
    "    print (\"wining per: \",wining_rate)\n",
    "    print(\"player_0 average reward: \",player_0_reward_mean)\n",
    "    print(\"player_1 average reward: \",player_1_reward_mean)\n",
    "    print (\"undefeated per: \",undefeated_perc)\n",
    "    print (\"score average: \",score_mean)\n",
    "    print(\"epsidoe len average: \", len_mean)\n",
    "    print (\"passes average per eps: \",passes_mean)\n",
    "    print(\"shooting average per eps: \",shooting_mean)\n",
    "    print (\"possession perc average per eps: \",possession_mean)\n",
    "    print(\"regaining ball average per eps: \", interception_mean)\n",
    "    #append data \n",
    "    states_mean['wining_rate'].append(wining_rate) \n",
    "    states_mean['player_0_reward'].append(player_0_reward_mean) \n",
    "    states_mean['player_1_reward'].append(player_1_reward_mean) \n",
    "    states_mean['undefeated'].append(undefeated_perc) \n",
    "    states_mean['len'].append(len_mean) \n",
    "    states_mean['score'].append(score_mean) \n",
    "    states_mean['passes'].append(passes_mean)\n",
    "    states_mean['shooting'].append(shooting_mean)\n",
    "    states_mean[\"possession\"].append(possession_mean)\n",
    "    states_mean[\"interception\"].append(interception_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i, checkpoint in enumerate(checkpoints):\n",
    "#     if i !=3:\n",
    "#         continue\n",
    "#     stat = evaluation(evaluate_all,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "#     stats.append(stat)\n",
    "#     wining_rate = stat[\"win_perc\"].count(1)/ len(stat[\"win_perc\"])\n",
    "#     player_0_reward_mean = np.mean(stat['rewards_total']['player_0'])\n",
    "#     player_1_reward_mean = np.mean(stat['rewards_total']['player_1'])\n",
    "#     undefeated_perc = stat[\"undefeated_perc\"].count(1)/ len(stat[\"undefeated_perc\"])\n",
    "#     score_mean= np.mean(stat[\"score_reward\"])\n",
    "#     len_mean = np.mean(stat[\"timesteps\"])\n",
    "#     passes_mean = np.mean(stat[\"num_passes_per_eps\"])\n",
    "#     shooting_mean =  np.mean(stat[\"num_shots_per_eps\"])\n",
    "#     possession_mean = np.mean(stat[\"possession_per_eps\"])\n",
    "#     interception_mean = np.mean(stat[\"regaining_possession_per_eps\"])\n",
    "#     print(\"checkpint index: \",i)\n",
    "#     print (\"wining per: \",wining_rate)\n",
    "#     print(\"player_0 average reward: \",player_0_reward_mean)\n",
    "#     print(\"player_1 average reward: \",player_1_reward_mean)\n",
    "#     print (\"undefeated per: \",undefeated_perc)\n",
    "#     print (\"score average: \",score_mean)\n",
    "#     print(\"epsidoe len average: \", len_mean)\n",
    "#     print (\"passes average per eps: \",passes_mean)\n",
    "#     print(\"shooting average per eps: \",shooting_mean)\n",
    "#     print (\"possession perc average per eps: \",possession_mean)\n",
    "#     print(\"regaining ball average per eps: \", interception_mean)\n",
    "#     #append data \n",
    "#     states_mean['wining_rate'].append(wining_rate) \n",
    "#     states_mean['player_0_reward'].append(player_0_reward_mean) \n",
    "#     states_mean['player_1_reward'].append(player_1_reward_mean) \n",
    "#     states_mean['undefeated'].append(undefeated_perc) \n",
    "#     states_mean['len'].append(len_mean) \n",
    "#     states_mean['score'].append(score_mean) \n",
    "#     states_mean['passes'].append(passes_mean)\n",
    "#     states_mean['shooting'].append(shooting_mean)\n",
    "#     states_mean[\"possession\"].append(possession_mean)\n",
    "#     states_mean[\"interception\"].append(interception_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766354cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(432x288)\n"
     ]
    }
   ],
   "source": [
    "for key in states_mean.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MR_PPO'], key:states_mean[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {} for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb5aa9",
   "metadata": {},
   "source": [
    "## Matrices: Total reward, episode len, score_reward, win_perc,undefeated_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926cf8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_wining(agent,config,env,num_episodes=10):\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    eps_stats = {}\n",
    "    eps_stats['rewards_total'] = []\n",
    "    eps_stats['timesteps'] = []\n",
    "    eps_stats['score_reward'] = []\n",
    "    eps_stats['win_perc'] = []\n",
    "    eps_stats['undefeated_perc'] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "\n",
    "        done = False\n",
    "        eps_stats['rewards_total'].append({a:0.0 for a in obs})\n",
    "        eps_stats['timesteps'].append(-1)\n",
    "        eps_stats['score_reward'].append(0)\n",
    "        eps_stats['win_perc'].append(0)\n",
    "        eps_stats['undefeated_perc'].append(0)\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "                a_action = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action\n",
    "                prev_actions[player_id] = a_action\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "                eps_stats['rewards_total'][-1][player_id] += r\n",
    "\n",
    "            obs = next_obs\n",
    "            eps_stats['timesteps'][-1] += 1\n",
    "\n",
    "        eps_stats['score_reward'][-1] = infos['player_0']['score_reward']\n",
    "        game_result = \"loss\" if infos['player_0']['score_reward'] == -1 else \\\n",
    "            \"win\" if infos['player_0']['score_reward'] == 1 else \"tie\"\n",
    "        eps_stats['win_perc'][-1] = int(game_result == \"win\")\n",
    "        eps_stats['undefeated_perc'][-1] = int(game_result != \"loss\")\n",
    "        if debug:\n",
    "            print(f\"\\nEpisode #{eidx+1} ended with a {game_result}:\")\n",
    "            for p, r in eps_stats['rewards_total'][-1].items():\n",
    "                print(\"\\t{} got episode reward: {:.2f}\".format(p, r))\n",
    "            print(\"\\tTotal reward: {:.2f}\".format(sum(eps_stats['rewards_total'][-1].values())))\n",
    "\n",
    "    eps_stats['rewards_total'] = {k: [dic[k] for dic in eps_stats['rewards_total']] \\\n",
    "        for k in eps_stats['rewards_total'][0]}\n",
    "    print(\"\\n\\nAll trials completed:\")\n",
    "    if debug:\n",
    "        for stat_name, values in eps_stats.items():\n",
    "            print(f\"\\t{stat_name}:\")\n",
    "            if type(values) is dict:\n",
    "                for stat_name2, values2 in values.items():\n",
    "                    print(f\"\\t\\t{stat_name2}:\")\n",
    "                    for func_name, func in STAT_FUNC.items():\n",
    "                        print(\"\\t\\t\\t{}: {:.2f}\".format(func_name, func(values2)))\n",
    "            else:\n",
    "                for func_name, func in STAT_FUNC.items():\n",
    "                    print(\"\\t\\t{}: {:.2f}\".format(func_name, func(values)))\n",
    "\n",
    "    return eps_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_1 = []\n",
    "states_mean = {}\n",
    "states_mean['wining_rate'] = []\n",
    "states_mean['player_0_reward'] = []\n",
    "states_mean['player_1_reward'] = []\n",
    "states_mean['undefeated'] = []\n",
    "states_mean['len'] = []\n",
    "states_mean['score'] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evaluate_wining,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    stats_1.append(stat)\n",
    "    wining_rate = stat[\"win_perc\"].count(1)/ len(stat[\"win_perc\"])\n",
    "    player_0_reward_mean = np.mean(stat['rewards_total']['player_0'])\n",
    "    player_1_reward_mean = np.mean(stat['rewards_total']['player_1'])\n",
    "    undefeated_perc = stat[\"undefeated_perc\"].count(1)/ len(stat[\"undefeated_perc\"])\n",
    "    score_mean= np.mean(stat[\"score_reward\"])\n",
    "    len_mean = np.mean(stat[\"timesteps\"])\n",
    "    print(\"checkpint index: \",i)\n",
    "    print (\"wining per: \",wining_rate)\n",
    "    print(\"player_0 average reward: \",player_0_reward_mean)\n",
    "    print(\"player_1 average reward: \",player_1_reward_mean)\n",
    "    print (\"undefeated per: \",undefeated_perc)\n",
    "    print (\"score average: \",score_mean)\n",
    "    print(\"epsidoe len average: \", len_mean)\n",
    "    #append data \n",
    "    states_mean['wining_rate'].append(wining_rate) \n",
    "    states_mean['player_0_reward'].append(player_0_reward_mean) \n",
    "    states_mean['player_1_reward'].append(player_1_reward_mean) \n",
    "    states_mean['undefeated'].append(undefeated_perc) \n",
    "    states_mean['len'].append(len_mean) \n",
    "    states_mean['score'].append(score_mean) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b6f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb754fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in states_mean.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MR_PPO'], key:states_mean[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {} for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d2683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b43b77",
   "metadata": {},
   "source": [
    "## num of passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ec666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actions(agent,config,env,num_episodes=10):\n",
    "    players_idx = {'player_0':1,'player_1':2 }\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "    eps_stats = {}\n",
    "    eps_stats[\"num_passes_per_eps\"] = []\n",
    "    eps_stats[\"num_shots_per_eps\"] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "        passes = 0\n",
    "        shoots = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "\n",
    "                a_action_flatten = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action_flatten\n",
    "                prev_actions[player_id] = a_action_flatten\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "            infos = infos[player_id]['game_info']\n",
    "    #         print(infos['ball_owned_player'])\n",
    "            if (infos['ball_owned_team'] == 0 and infos['ball_owned_player'] == players_idx[player_id]): \n",
    "                if a_action in [9,10,11]:\n",
    "                    passes += 1\n",
    "                elif a_action == 12: #shooting\n",
    "                    shoots +=1\n",
    "\n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "\n",
    "            obs = next_obs\n",
    "        eps_stats[\"num_passes_per_eps\"].append(passes)\n",
    "        eps_stats[\"num_shots_per_eps\"].append(shoots)\n",
    "    \n",
    "    return eps_stats\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_2 = []\n",
    "\n",
    "states_mean_2 = {}\n",
    "states_mean_2['passes'] = []\n",
    "states_mean_2['shooting'] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evaluate_actions,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    passes_mean = np.mean(stat[\"num_passes_per_eps\"])\n",
    "    shooting_mean =  np.mean(stat[\"num_shots_per_eps\"])\n",
    "    print (\"passes average per eps: \",passes_mean)\n",
    "    print(\"shooting average per eps: \",shooting_mean)\n",
    "    \n",
    "    states_mean_2['passes'].append(passes_mean)\n",
    "    states_mean_2['shooting'].append(shooting_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a001eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in states_mean_2.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MR_PPO'], key:states_mean_2[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {}  for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d202ae",
   "metadata": {},
   "source": [
    "## Regaining possession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute_possession(agent,config,env,num_episodes=10):\n",
    "    players_idx = {'player_0':1,'player_1':2 }\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    eps_stats = {}\n",
    "    eps_stats['possession_per_eps'] = []\n",
    "    eps_stats['regaining_possession_per_eps'] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "        possession = obs['player_0'][31]==1 #when obs[31]==1 it means the left team have the ball\n",
    "        regains = 0\n",
    "        steps = 0 \n",
    "        possession_steps= 1 if possession==True else 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps +=1\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "\n",
    "                a_action_flatten = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action_flatten\n",
    "                prev_actions[player_id] = a_action_flatten\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "            possession_next = next_obs['player_0'][31]==1\n",
    "            if possession_next:\n",
    "                possession_steps +=1\n",
    "            if not possession and possession_next: #if i didn't have the ball, then i regain it\n",
    "                regains +=1 \n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "\n",
    "            possession = possession_next\n",
    "            obs = next_obs\n",
    "        eps_stats['possession_per_eps'].append(round(possession_steps/steps,2)*100)\n",
    "        eps_stats['regaining_possession_per_eps'].append(regains)\n",
    "\n",
    "    return eps_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_3 = []\n",
    "states_mean_3 = {}\n",
    "states_mean_3[\"possession\"] = []\n",
    "states_mean_3[\"interception\"] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evalaute_possession,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    possession_mean = np.mean(stat[\"possession_per_eps\"])\n",
    "    interception_mean = np.mean(stat[\"regaining_possession_per_eps\"])\n",
    "    print (\"possession perc average per eps: \",possession_mean)\n",
    "    print(\"regaining ball average per eps: \", interception_mean)\n",
    "    states_mean_3[\"possession\"].append(possession_mean)\n",
    "    states_mean_3[\"interception\"].append(interception_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in states_mean_3.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MR_PPO'], key:states_mean_3[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {} % for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632580ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(scalars, weight=0.99):  # Weight between 0 and 1\n",
    "    \"\"\"\n",
    "\n",
    "    ref: https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
    "\n",
    "    :param scalars:\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed: list = []\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "labels = ['baseline_1','baseline_2','baseline_3',\"MR-PPO-trial 1\",\"MR-PPO-trial 2\",\"MR-QMIX\"]\n",
    "files_name = ['baseline_1','baseline_2','baseline_3','my_agent','my_agent_2','my_agent_qmix']\n",
    "# Set the figure size\n",
    "plt.rcParams[\"figure.figsize\"] = [6.00, 4.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "for i, file_name in enumerate(files_name):\n",
    "\n",
    "    # Read a CSV file\n",
    "    df = pd.read_csv(\"training_data/{}.csv\".format(file_name))\n",
    "    x = df[\"Step\"].values\n",
    "    y = df[\"Value\"].values\n",
    "    y = smooth(y,weight=0.98)\n",
    "    # # Plot the lines\n",
    "    if i >=3:\n",
    "        plt.plot(x,y,label=labels[i],linewidth='2.5')\n",
    "    else:\n",
    "        plt.plot(x,y,label=labels[i],linewidth='2.5',ls=\"--\")\n",
    "\n",
    "plt.title(\"Average scores during training\")\n",
    "plt.xlabel('time-steps in millions')\n",
    "plt.ylabel('average scores')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('average_scores_training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b25a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51091565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
