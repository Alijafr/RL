{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19665b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "from argparse import RawTextHelpFormatter\n",
    "\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.cloudpickle as cloudpickle\n",
    "import torch\n",
    "from gfootball import env as fe\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.spaces.space_utils import flatten_to_single_ndarray\n",
    "from ray.tune.registry import get_trainable_cls, register_env,  register_trainable\n",
    "from rldm.utils import football_tools as ft\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import system_tools as st\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bad7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gfootball.env.wrappers import CheckpointRewardWrapper\n",
    "\n",
    "# def reward_modified(self,reward):\n",
    "#     '''\n",
    "#     o['active'] => 1 or 2 \n",
    "#     '''\n",
    "#     #change checkpoints to only 4\n",
    "#     #self._num_checkpoints = 4\n",
    "    \n",
    "#     #assuming max of 500 steps \n",
    "#     passing_rewards = 0.002 \n",
    "#     shoting_rewards = 0.1\n",
    "#     not_owning_ball_team = -0.001\n",
    "#     owning_ball = 0.001\n",
    "#     observation = self.env.unwrapped.observation()\n",
    "#     actions = self.env.unwrapped._get_actions()\n",
    "#     #print(observation)\n",
    "#     if observation is None:\n",
    "#         return reward\n",
    "\n",
    "#     assert len(reward) == len(observation)\n",
    "\n",
    "#     for rew_index in range(len(reward)):\n",
    "#         o = observation[rew_index]\n",
    "#         a = actions[rew_index]\n",
    "#         if reward[rew_index] == 1: #scoring a goal\n",
    "#             reward[rew_index] += self._checkpoint_reward * (\n",
    "#                 self._num_checkpoints -\n",
    "#                 self._collected_checkpoints.get(rew_index, 0))\n",
    "#             self._collected_checkpoints[rew_index] = self._num_checkpoints\n",
    "#             #add extra points to compensate for the extra postive rewards\n",
    "#             reward[rew_index] += 0.5\n",
    "#             continue\n",
    "        \n",
    "#         #check if the right team does own the ball, this should be the first objective in the game\n",
    "#         if o['ball_owned_team'] != 0:\n",
    "#             #reward[rew_index] += not_owning_ball_team\n",
    "#             #player need to get closer to get the ball \n",
    "#             player_pos = o['right_team'][o['active']]\n",
    "#             player_ball_dist = ((o['ball'][0] - player_pos[0]) ** 2 + (o['ball'][1]-player_pos[1]) ** 2) ** 0.5\n",
    "#             reward[rew_index] += player_ball_dist*not_owning_ball_team\n",
    "#         else:\n",
    "#             #they should positive reward for owning the ball too\n",
    "#             reward[rew_index] += owning_ball\n",
    "            \n",
    "        \n",
    "        \n",
    "#         # Check if the active player has the ball.\n",
    "#         if ('ball_owned_team' not in o or\n",
    "#           o['ball_owned_team'] != 0 or\n",
    "#           'ball_owned_player' not in o or\n",
    "#           o['ball_owned_player'] != o['active']):\n",
    "#             continue\n",
    "#         #now, we know that the active player has the ball\n",
    "#         #reward passing if the second player is closer to goal\n",
    "#         #reward shooting if active player is near the goal\n",
    "#         passing_thre = 0.05\n",
    "#         position_second_player = o['right_team'][1 if o['active']==2 else 2]\n",
    "#         dist_second_player = ((position_second_player[0] - 1) ** 2 + position_second_player[1] ** 2) ** 0.5\n",
    "#         d = ((o['ball'][0] - 1) ** 2 + o['ball'][1] ** 2) ** 0.5 #ball dist to goal (the active player too)\n",
    "#         player_distances = ((o['ball'][0] - position_second_player[0]) ** 2 + (o['ball'][1]-position_second_player[1]) ** 2) ** 0.5\n",
    "#         #if the ball is very close, shooting would be better than passing\n",
    "#         if dist_second_player + passing_thre < d and d > 0.3: \n",
    "#             if player_distances < 0.2 and a == 11: #short pass when they are near each other\n",
    "#                 reward[rew_index] += passing_rewards\n",
    "#             elif player_distances > 0.2 and (a == 9 or a==10):\n",
    "#                 reward[rew_index] += passing_rewards\n",
    "#         #now check if the ball is near target, then rewarding shotting \n",
    "#         if d < 0.4 and a ==12:\n",
    "#             reward[rew_index] += shoting_rewards\n",
    "            \n",
    "\n",
    "#         # Collect the checkpoints.\n",
    "#         # We give reward for distance 1 to 0.2.\n",
    "#         while (self._collected_checkpoints.get(rew_index, 0) <\n",
    "#              self._num_checkpoints):\n",
    "#             if self._num_checkpoints == 1:\n",
    "#                 threshold = 0.99 - 0.8\n",
    "#             else:\n",
    "#                 threshold = (0.99 - 0.8 / (self._num_checkpoints - 1) *\n",
    "#                            self._collected_checkpoints.get(rew_index, 0))\n",
    "#             if d > threshold:\n",
    "#                 break\n",
    "#             reward[rew_index] += self._checkpoint_reward\n",
    "#             self._collected_checkpoints[rew_index] = (\n",
    "#                 self._collected_checkpoints.get(rew_index, 0) + 1)\n",
    "#     return reward\n",
    "\n",
    "# CheckpointRewardWrapper.reward = reward_modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51aa7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultMapping(collections.defaultdict):\n",
    "    \"\"\"default_factory now takes as an argument the missing key.\"\"\"\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        self[key] = value = self.default_factory(key)\n",
    "        return value\n",
    "\n",
    "STAT_FUNC = {\n",
    "    'min': np.min,\n",
    "    'mean': np.mean,\n",
    "    'median': np.median,\n",
    "    'max': np.max,\n",
    "    'std': np.std,\n",
    "    'var': np.var,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbe023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/football/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532c29b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 root root 2241 Jul  3 13:00 /mnt/logs/baselines/baseline_1/params.json\r\n",
      "-rwxrwxrwx 1 root root 3090 Jul  3 13:00 /mnt/logs/baselines/baseline_1/params.pkl\r\n",
      "-rwxrwxrwx 1 root root 2254 Jul  3 13:00 /mnt/logs/baselines/baseline_2/params.json\r\n",
      "-rwxrwxrwx 1 root root 3095 Jul  3 13:00 /mnt/logs/baselines/baseline_2/params.pkl\r\n",
      "-rwxrwxrwx 1 root root 2240 Jul  3 13:00 /mnt/logs/baselines/baseline_3/params.json\r\n",
      "-rwxrwxrwx 1 root root 3090 Jul  3 13:00 /mnt/logs/baselines/baseline_3/params.pkl\r\n",
      "\r\n",
      "/mnt/logs/baselines/baseline_1/checkpoint_0:\r\n",
      "total 14772\r\n",
      "-rwxrwxrwx 1 root root 15122984 Jul  3 13:00 checkpoint-0\r\n",
      "-rwxrwxrwx 1 root root      184 Jul  3 13:00 checkpoint-0.tune_metadata\r\n",
      "\r\n",
      "/mnt/logs/baselines/baseline_2/checkpoint_0:\r\n",
      "total 13872\r\n",
      "-rwxrwxrwx 1 root root 14201365 Jul  3 13:00 checkpoint-0\r\n",
      "-rwxrwxrwx 1 root root      184 Jul  3 13:00 checkpoint-0.tune_metadata\r\n",
      "\r\n",
      "/mnt/logs/baselines/baseline_3/checkpoint_0:\r\n",
      "total 10928\r\n",
      "-rwxrwxrwx 1 root root 11187733 Jul  3 13:00 checkpoint-0\r\n",
      "-rwxrwxrwx 1 root root      184 Jul  3 13:00 checkpoint-0.tune_metadata\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /mnt/logs/baselines/**/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdada25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT TO LOAD\n",
    "# here are some example checkpoints from the baseline teams...\n",
    "baseline1 = \"/mnt/logs/baselines/baseline_1/checkpoint_0/checkpoint-0\"\n",
    "baseline2 = \"/mnt/logs/baselines/baseline_2/checkpoint_0/checkpoint-0\"\n",
    "baseline3 = \"/mnt/logs/baselines/baseline_3/checkpoint_0/checkpoint-0\"\n",
    "checkpoint_pc = \"/mnt/logs/train_agents_modified_rewards_3_vs_3_auto_GK_independent_50000000_fifo_fixed/PPO_3_vs_3_auto_GK_fc470_00000_0_2022-07-15_18-07-27/checkpoint_008100/checkpoint-8100\"\n",
    "# checkpoint_pc = \"/mnt/logs/train_agents_modified_rewards_3_vs_3_auto_GK_independent_50000000_fifo_fixed/PPO_3_vs_3_auto_GK_fc470_00000_0_2022-07-15_18-07-27/checkpoint_008951/checkpoint-8951\"\n",
    "# checkpoint_qmix = \"/mnt/logs/\n",
    "checkpoints = [baseline1,baseline2,baseline3,checkpoint_pc]\n",
    "num_episodes = 10\n",
    "debug = False\n",
    "#algorithm = \"MADDPG\"\n",
    "algorithm = \"PPO\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba57096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:60568',\n",
       " 'object_store_address': '/tmp/ray/session_2022-07-19_17-12-34_908933_17070/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-07-19_17-12-34_908933_17070/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-07-19_17-12-34_908933_17070',\n",
       " 'metrics_export_port': 49988,\n",
       " 'node_id': 'cade77276da3a6c4a9f854a31b3f68508048c18098a65ccce58106b5'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True, log_to_driver=debug, include_dashboard=False,\n",
    "         local_mode=True, logging_level='DEBUG' if debug else 'ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62da3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad990a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de02eb58",
   "metadata": {},
   "source": [
    "## Matrices: Total reward, episode len, score_reward, win_perc,undefeated_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0464363",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_wining(agent,config,env,num_episodes=10):\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    eps_stats = {}\n",
    "    eps_stats['rewards_total'] = []\n",
    "    eps_stats['timesteps'] = []\n",
    "    eps_stats['score_reward'] = []\n",
    "    eps_stats['win_perc'] = []\n",
    "    eps_stats['undefeated_perc'] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "\n",
    "        done = False\n",
    "        eps_stats['rewards_total'].append({a:0.0 for a in obs})\n",
    "        eps_stats['timesteps'].append(-1)\n",
    "        eps_stats['score_reward'].append(0)\n",
    "        eps_stats['win_perc'].append(0)\n",
    "        eps_stats['undefeated_perc'].append(0)\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "                a_action = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action\n",
    "                prev_actions[player_id] = a_action\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "                eps_stats['rewards_total'][-1][player_id] += r\n",
    "\n",
    "            obs = next_obs\n",
    "            eps_stats['timesteps'][-1] += 1\n",
    "\n",
    "        eps_stats['score_reward'][-1] = infos['player_0']['score_reward']\n",
    "        game_result = \"loss\" if infos['player_0']['score_reward'] == -1 else \\\n",
    "            \"win\" if infos['player_0']['score_reward'] == 1 else \"tie\"\n",
    "        eps_stats['win_perc'][-1] = int(game_result == \"win\")\n",
    "        eps_stats['undefeated_perc'][-1] = int(game_result != \"loss\")\n",
    "        if debug:\n",
    "            print(f\"\\nEpisode #{eidx+1} ended with a {game_result}:\")\n",
    "            for p, r in eps_stats['rewards_total'][-1].items():\n",
    "                print(\"\\t{} got episode reward: {:.2f}\".format(p, r))\n",
    "            print(\"\\tTotal reward: {:.2f}\".format(sum(eps_stats['rewards_total'][-1].values())))\n",
    "\n",
    "    eps_stats['rewards_total'] = {k: [dic[k] for dic in eps_stats['rewards_total']] \\\n",
    "        for k in eps_stats['rewards_total'][0]}\n",
    "    print(\"\\n\\nAll trials completed:\")\n",
    "    if debug:\n",
    "        for stat_name, values in eps_stats.items():\n",
    "            print(f\"\\t{stat_name}:\")\n",
    "            if type(values) is dict:\n",
    "                for stat_name2, values2 in values.items():\n",
    "                    print(f\"\\t\\t{stat_name2}:\")\n",
    "                    for func_name, func in STAT_FUNC.items():\n",
    "                        print(\"\\t\\t\\t{}: {:.2f}\".format(func_name, func(values2)))\n",
    "            else:\n",
    "                for func_name, func in STAT_FUNC.items():\n",
    "                    print(\"\\t\\t{}: {:.2f}\".format(func_name, func(values)))\n",
    "\n",
    "    return eps_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6a0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(eval_fuc,checkpoint, num_episodes=10, algorithm=\"PPO\"):\n",
    "    assert checkpoint, \"A checkpoint string is required\"\n",
    "\n",
    "    # assert num_episodes <= 10, \"Must rollout a maximum of 10 episodes--rendering takes a very long time\"\n",
    "    # assert num_episodes >= 1, \"Must rollout at least 1 episode\"\n",
    "\n",
    "    n_cpus, _ = st.get_cpu_gpu_count()\n",
    "    assert n_cpus >= 2, \"Didn't find enough cpus\"\n",
    "\n",
    "    config = {}\n",
    "    config_dir = os.path.dirname(checkpoint)\n",
    "    config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "    assert os.path.exists(config_path), \"Could not find a proper config from checkpoint\"\n",
    "    with open(config_path, \"rb\") as f:\n",
    "        config = cloudpickle.load(f)\n",
    "\n",
    "    config[\"create_env_on_driver\"] = True\n",
    "    config[\"log_level\"] = 'DEBUG' if debug else 'ERROR' \n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"num_gpus\"] = 0\n",
    "    test_env_name=\"3_vs_3_auto_GK\" # this semester we're only doing 3v3 w auto GK\n",
    "    \n",
    "    \n",
    "    register_env(test_env_name, lambda _: ft.RllibGFootball(env_name=test_env_name))\n",
    "    cls = get_trainable_cls(algorithm)\n",
    "    agent = cls(env=test_env_name, config=config)\n",
    "    agent.restore(checkpoint)\n",
    "\n",
    "    try:\n",
    "        del env\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    env = ft.RllibGFootball(env_name=config['env'], write_video=False, render=False)\n",
    "    \n",
    "    est_state = eval_fuc(agent,config,env,num_episodes)\n",
    "\n",
    "\n",
    "    return est_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb6314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbfa403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  0\n",
      "wining per:  0.4\n",
      "player_0 average reward:  0.69\n",
      "player_1 average reward:  0.7659999978542328\n",
      "undefeated per:  0.89\n",
      "score average:  0.29\n",
      "epsidoe len average:  140.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  1\n",
      "wining per:  0.44\n",
      "player_0 average reward:  0.815\n",
      "player_1 average reward:  0.864000016450882\n",
      "undefeated per:  0.93\n",
      "score average:  0.37\n",
      "epsidoe len average:  134.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  2\n",
      "wining per:  0.47\n",
      "player_0 average reward:  0.945\n",
      "player_1 average reward:  0.9700000096857547\n",
      "undefeated per:  1.0\n",
      "score average:  0.47\n",
      "epsidoe len average:  97.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All trials completed:\n",
      "checkpint index:  3\n",
      "wining per:  0.35\n",
      "player_0 average reward:  0.62\n",
      "player_1 average reward:  0.7500000154972076\n",
      "undefeated per:  0.92\n",
      "score average:  0.27\n",
      "epsidoe len average:  141.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats_1 = []\n",
    "states_mean = {}\n",
    "states_mean['wining_rate'] = []\n",
    "states_mean['player_0_reward'] = []\n",
    "states_mean['player_1_reward'] = []\n",
    "states_mean['undefeated'] = []\n",
    "states_mean['len'] = []\n",
    "states_mean['score'] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evaluate_wining,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    stats_1.append(stat)\n",
    "    wining_rate = stat[\"win_perc\"].count(1)/ len(stat[\"win_perc\"])\n",
    "    player_0_reward_mean = np.mean(stat['rewards_total']['player_0'])\n",
    "    player_1_reward_mean = np.mean(stat['rewards_total']['player_1'])\n",
    "    undefeated_perc = stat[\"undefeated_perc\"].count(1)/ len(stat[\"undefeated_perc\"])\n",
    "    score_mean= np.mean(stat[\"score_reward\"])\n",
    "    len_mean = np.mean(stat[\"timesteps\"])\n",
    "    print(\"checkpint index: \",i)\n",
    "    print (\"wining per: \",wining_rate)\n",
    "    print(\"player_0 average reward: \",player_0_reward_mean)\n",
    "    print(\"player_1 average reward: \",player_1_reward_mean)\n",
    "    print (\"undefeated per: \",undefeated_perc)\n",
    "    print (\"score average: \",score_mean)\n",
    "    print(\"epsidoe len average: \", len_mean)\n",
    "    #append data \n",
    "    states_mean['wining_rate'].append(wining_rate) \n",
    "    states_mean['player_0_reward'].append(player_0_reward_mean) \n",
    "    states_mean['player_1_reward'].append(player_1_reward_mean) \n",
    "    states_mean['undefeated'].append(undefeated_perc) \n",
    "    states_mean['len'].append(len_mean) len_mean\n",
    "    states_mean['score'].append(score_mean) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "084c2118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(432x288)\n"
     ]
    }
   ],
   "source": [
    "for key in states_mean.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MRF_PPO'], key:states_mean[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {} % for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "258542c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(432x288)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176af3ee",
   "metadata": {},
   "source": [
    "## num of passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d5fadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actions(agent,config,env,num_episodes=10):\n",
    "    players_idx = {'player_0':1,'player_1':2 }\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "    eps_stats = {}\n",
    "    eps_stats[\"num_passes_per_eps\"] = []\n",
    "    eps_stats[\"num_shots_per_eps\"] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "        passes = 0\n",
    "        shoots = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "\n",
    "                a_action_flatten = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action_flatten\n",
    "                prev_actions[player_id] = a_action_flatten\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "            infos = infos[player_id]['game_info']\n",
    "    #         print(infos['ball_owned_player'])\n",
    "            if (infos['ball_owned_team'] == 0 and infos['ball_owned_player'] == players_idx[player_id]): \n",
    "                if a_action in [9,10,11]:\n",
    "                    passes += 1\n",
    "                elif a_action == 12: #shooting\n",
    "                    shoots +=1\n",
    "\n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "\n",
    "            obs = next_obs\n",
    "        eps_stats[\"num_passes_per_eps\"].append(passes)\n",
    "        eps_stats[\"num_shots_per_eps\"].append(shoots)\n",
    "    \n",
    "    return eps_stats\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e670f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes average per eps:  0.06\n",
      "shooting average per eps:  1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes average per eps:  0.05\n",
      "shooting average per eps:  1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes average per eps:  0.0\n",
      "shooting average per eps:  0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes average per eps:  0.08\n",
      "shooting average per eps:  1.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats_2 = []\n",
    "\n",
    "states_mean_2 = {}\n",
    "states_mean_2['passes'] = []\n",
    "states_mean_2['shooting'] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evaluate_actions,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    passes_mean = np.mean(stat[\"num_passes_per_eps\"])\n",
    "    shooting_mean =  np.mean(stat[\"num_shots_per_eps\"])\n",
    "    print (\"passes average per eps: \",passes_mean)\n",
    "    print(\"shooting average per eps: \",shooting_mean)\n",
    "    \n",
    "    states_mean_2['passes'].append(passes_mean)\n",
    "    states_mean_2['shooting'].append(shooting_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "798b3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(432x288)\n"
     ]
    }
   ],
   "source": [
    "for key in states_mean_2.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MRF_PPO'], key:states_mean_2[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {}  for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76070c91",
   "metadata": {},
   "source": [
    "## Regaining possession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bac23762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute_possession(agent,config,env,num_episodes=10):\n",
    "    players_idx = {'player_0':1,'player_1':2 }\n",
    "    policy_agent_mapping = agent.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "    policy_map = agent.workers.local_worker().policy_map\n",
    "    state_init = {p: m.get_initial_state() for p, m in policy_map.items()}\n",
    "    use_lstm = {p: len(s) > 0 for p, s in state_init.items()}\n",
    "\n",
    "    action_init = {\n",
    "        p: flatten_to_single_ndarray(m.action_space.sample())\n",
    "        for p, m in policy_map.items()\n",
    "    }\n",
    "\n",
    "    eps_stats = {}\n",
    "    eps_stats['possession_per_eps'] = []\n",
    "    eps_stats['regaining_possession_per_eps'] = []\n",
    "    for eidx in tqdm(range(num_episodes)):\n",
    "        mapping_cache = {}  # in case policy_agent_mapping is stochastic\n",
    "        obs = env.reset()\n",
    "        agent_states = DefaultMapping(\n",
    "            lambda player_id: state_init[mapping_cache[player_id]])\n",
    "        prev_actions = DefaultMapping(\n",
    "            lambda player_id: action_init[mapping_cache[player_id]])\n",
    "        prev_rewards = collections.defaultdict(lambda: 0.)\n",
    "        possession = obs['player_0'][31]==1 #when obs[31]==1 it means the left team have the ball\n",
    "        regains = 0\n",
    "        steps = 0 \n",
    "        possession_steps= 1 if possession==True else 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps +=1\n",
    "            actions = {}\n",
    "            for player_id, a_obs in obs.items():\n",
    "                policy_id = mapping_cache.setdefault(\n",
    "                    player_id, policy_agent_mapping(player_id, None))\n",
    "                p_use_lstm = use_lstm[policy_id]\n",
    "                if p_use_lstm:\n",
    "                    a_action, p_state, _ = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        state=agent_states[player_id],\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "                    agent_states[player_id] = p_state\n",
    "                else:\n",
    "                    a_action = agent.compute_single_action(\n",
    "                        a_obs,\n",
    "                        prev_action=prev_actions[player_id],\n",
    "                        prev_reward=prev_rewards[player_id],\n",
    "                        policy_id=policy_id)\n",
    "\n",
    "\n",
    "                a_action_flatten = flatten_to_single_ndarray(a_action)\n",
    "                actions[player_id] = a_action_flatten\n",
    "                prev_actions[player_id] = a_action_flatten\n",
    "\n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "            possession_next = next_obs['player_0'][31]==1\n",
    "            if possession_next:\n",
    "                possession_steps +=1\n",
    "            if not possession and possession_next: #if i didn't have the ball, then i regain it\n",
    "                regains +=1 \n",
    "            done = dones['__all__']\n",
    "            for player_id, r in rewards.items():\n",
    "                prev_rewards[player_id] = r\n",
    "\n",
    "            possession = possession_next\n",
    "            obs = next_obs\n",
    "        eps_stats['possession_per_eps'].append(round(possession_steps/steps,2)*100)\n",
    "        eps_stats['regaining_possession_per_eps'].append(regains)\n",
    "\n",
    "    return eps_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08254663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [02:44<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession perc average per eps:  36.81\n",
      "regaining ball average per eps:  0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [03:08<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession perc average per eps:  39.86\n",
      "regaining ball average per eps:  0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [03:52<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession perc average per eps:  43.79\n",
      "regaining ball average per eps:  0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [03:46<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession perc average per eps:  39.84\n",
      "regaining ball average per eps:  0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats_3 = []\n",
    "states_mean_3 = {}\n",
    "states_mean_3[\"possession\"] = []\n",
    "states_mean_3[\"interception\"] = []\n",
    "for i, checkpoint in enumerate(checkpoints):\n",
    "    stat = evaluation(evalaute_possession,checkpoint, num_episodes=100, algorithm=\"PPO\")\n",
    "    possession_mean = np.mean(stat[\"possession_per_eps\"])\n",
    "    interception_mean = np.mean(stat[\"regaining_possession_per_eps\"])\n",
    "    print (\"possession perc average per eps: \",possession_mean)\n",
    "    print(\"regaining ball average per eps: \", interception_mean)\n",
    "    states_mean_3[\"possession\"].append(possession_mean)\n",
    "    states_mean_3[\"interception\"].append(interception_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cfcd932",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'possession'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17070/902820109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates_mean_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Checkpoint'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'baseline_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'baseline_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'baseline_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MRF_PPO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstates_mean_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mNew_Colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'purple'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'possession'"
     ]
    }
   ],
   "source": [
    "for key in states_mean_3.keys():\n",
    "    data = {'Checkpoint': ['baseline_1','baseline_2','baseline_3','MRF_PPO'], key:states_mean_3[key]}\n",
    "    New_Colors = ['green','blue','purple','brown']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    sns.barplot(x='Checkpoint', y=key, capsize=0.2, data=df).set(title=\"Avarage {} % for each Checkpoint\".format(key))\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}.png'.format(key))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
